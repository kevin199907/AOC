{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from ptq.layers import QAct, QConv2d, QIntLayerNorm, QIntSoftmax, QLinear, QConvTranspose2d , QConv2d_BNFold ,QConvTranspose2d_BNFold , QConv2d_output\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "\n",
    "        QConv2d_BNFold(inp,\n",
    "                 oup,\n",
    "                 kernel_size=3,\n",
    "                 stride= stride,\n",
    "                 padding=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True,\n",
    "                 quant=True,\n",
    "                 calibrate=False,\n",
    "                 last_calibrate=False),\n",
    "\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        QConv2d_BNFold(inp,\n",
    "                 oup,\n",
    "                 kernel_size=1,\n",
    "                 stride= 1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True,\n",
    "                 quant=True,\n",
    "                 calibrate=False,\n",
    "                 last_calibrate=False),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                QConv2d_BNFold(hidden_dim,\n",
    "                        hidden_dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride= stride,\n",
    "                        padding=(kernel_size - 1) // 2,\n",
    "                        dilation=1,\n",
    "                        groups=hidden_dim,\n",
    "                        bias=True,\n",
    "                        quant=True,\n",
    "                        calibrate=False,\n",
    "                        last_calibrate=False),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                QConv2d_BNFold(hidden_dim,\n",
    "                        oup,\n",
    "                        kernel_size=1,\n",
    "                        stride= 1,\n",
    "                        padding=0,\n",
    "                        dilation=1,\n",
    "                        groups=1,\n",
    "                        bias=True,\n",
    "                        quant=True,\n",
    "                        calibrate=False,\n",
    "                        last_calibrate=False),                \n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                QConv2d_BNFold(inp,\n",
    "                        hidden_dim,\n",
    "                        kernel_size=1,\n",
    "                        stride= 1,\n",
    "                        padding=0,\n",
    "                        dilation=1,\n",
    "                        groups=1,\n",
    "                        bias=True,\n",
    "                        quant=True,\n",
    "                        calibrate=False,\n",
    "                        last_calibrate=False),    \n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                QConv2d_BNFold(hidden_dim,\n",
    "                        hidden_dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride= stride,\n",
    "                        padding=(kernel_size - 1) // 2,\n",
    "                        dilation=1,\n",
    "                        groups=hidden_dim,\n",
    "                        bias=True,\n",
    "                        quant=True,\n",
    "                        calibrate=False,\n",
    "                        last_calibrate=False),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                QConv2d_BNFold(hidden_dim,\n",
    "                        oup,\n",
    "                        kernel_size=1,\n",
    "                        stride= 1,\n",
    "                        padding=0,\n",
    "                        dilation=1,\n",
    "                        groups=1,\n",
    "                        bias=True,\n",
    "                        quant=True,\n",
    "                        calibrate=False,\n",
    "                        last_calibrate=False), \n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class  our_MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, num_classes=10, width_mult=1.):\n",
    "        super(our_MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        assert mode in ['large', 'small','Ours']\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = 1\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = InvertedResidual\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        self.conv = conv_1x1_bn(input_channel, exp_size)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024,'Ours':10}\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(exp_size, output_channel),\n",
    "            h_swish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(output_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def Ours_mobilenetv3(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Ours-MobileNetV3 model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # kernal_size, t, c, SE, HS, s \n",
    "        [3,    1,  16, 1, 1, 2],\n",
    "        [3,  4.5,  24, 1, 1, 2],\n",
    "        [3, 3.67,  24, 1, 1, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "\n",
    "\n",
    "\n",
    "    ]\n",
    "    return our_MobileNetV3(cfgs, mode='Ours', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  Ours_mobilenetv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our_MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): QConv2d_BNFold(\n",
      "        3, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (quantizer): UniformQuantizer()\n",
      "        (quantizer_act): UniformQuantizer()\n",
      "      )\n",
      "      (1): h_swish(\n",
      "        (sigmoid): h_sigmoid(\n",
      "          (relu): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          1, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          8, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          16, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=72, out_features=24, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=24, out_features=72, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          72, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          24, 88, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=88, out_features=24, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=24, out_features=88, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          88, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          24, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=96, out_features=24, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=24, out_features=96, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          96, 40, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          40, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=240, out_features=64, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=64, out_features=240, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          240, 40, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          40, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=240, out_features=64, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=64, out_features=240, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          240, 40, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          40, 120, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=120, out_features=32, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=120, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          120, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): QConv2d_BNFold(\n",
      "          48, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (1): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): QConv2d_BNFold(\n",
      "          144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "        (3): SELayer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=144, out_features=40, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=40, out_features=144, bias=True)\n",
      "            (3): h_sigmoid(\n",
      "              (relu): ReLU6(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): h_swish(\n",
      "          (sigmoid): h_sigmoid(\n",
      "            (relu): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): QConv2d_BNFold(\n",
      "          144, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (quantizer): UniformQuantizer()\n",
      "          (quantizer_act): UniformQuantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv): Sequential(\n",
      "    (0): QConv2d_BNFold(\n",
      "      48, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (quantizer): UniformQuantizer()\n",
      "      (quantizer_act): UniformQuantizer()\n",
      "    )\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=144, out_features=10, bias=True)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m grad_para \u001b[39m=\u001b[39m count_parameters(net)\n\u001b[1;32m      7\u001b[0m \u001b[39m# print(f'Total params: {grad_para / 1e6}M')\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m summary(net,(\u001b[39m1\u001b[39;49m,\u001b[39m28\u001b[39;49m,\u001b[39m28\u001b[39;49m))\n\u001b[1;32m     10\u001b[0m \u001b[39m#Compute MACs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthop\u001b[39;00m \u001b[39mimport\u001b[39;00m profile\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 243\u001b[0m, in \u001b[0;36mour_MobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 243\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m    244\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n\u001b[1;32m    245\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1126\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1130\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/ProjectV2/project_Mobilenet_q/ptq/layers.py:453\u001b[0m, in \u001b[0;36mQConv2d_BNFold.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    446\u001b[0m weight_fused \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m*\u001b[39m reshape_to_weight(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps))   \u001b[39m#fused 為何要self.running_var\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39m#print('the weight input type of conv2d  is : {} '.format(type(self.weight)))\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m buffer1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantizer(weight_fused)\n\u001b[1;32m    454\u001b[0m \u001b[39m#buffer2 = self.quantizer_bias(self.bias)\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_int \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39msize()))\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1126\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1130\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/ProjectV2/project_Mobilenet_q/ptq/quantizer/base.py:44\u001b[0m, in \u001b[0;36mBaseQuantizer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 44\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquant(inputs)\n\u001b[1;32m     45\u001b[0m     \u001b[39m#outputs = torch.zeros(inputs.size())\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     outputs, scale, int_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdequantize(outputs)\n",
      "File \u001b[0;32m~/ProjectV2/project_Mobilenet_q/ptq/quantizer/uniform.py:26\u001b[0m, in \u001b[0;36mUniformQuantizer.quant\u001b[0;34m(self, inputs, scale, zero_point)\u001b[0m\n\u001b[1;32m     24\u001b[0m     zero_point \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_point\n\u001b[1;32m     25\u001b[0m range_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_reshape_range(inputs)\n\u001b[0;32m---> 26\u001b[0m scale \u001b[39m=\u001b[39m scale\u001b[39m.\u001b[39;49mreshape(range_shape)\n\u001b[1;32m     27\u001b[0m zero_point \u001b[39m=\u001b[39m zero_point\u001b[39m.\u001b[39mreshape(range_shape)\n\u001b[1;32m     28\u001b[0m outputs \u001b[39m=\u001b[39m inputs \u001b[39m*\u001b[39m scale \u001b[39m+\u001b[39m zero_point\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# Create an instance of the DNN model\n",
    "net =  Ours_mobilenetv3().cuda()  # Move the model to GPU if available\n",
    "print(net)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "grad_para = count_parameters(net)\n",
    "# print(f'Total params: {grad_para / 1e6}M')\n",
    "summary(net,(1,28,28))\n",
    "\n",
    "#Compute MACs\n",
    "from thop import profile\n",
    "input1 = torch.randn(1,1,28,28).cuda()\n",
    "MACs, params = profile(net, inputs=(input1, ))\n",
    "print('MACs = ' + str(MACs/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
