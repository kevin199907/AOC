{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin199907/.conda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "from mobilenet_model.Q_layer import bn_folding_model, bn_folding, fold_conv_bn_eval\n",
    "from mobilenet_model.Q_layer import get_scale_and_zero_point, linear_quantize\n",
    "from mobilenet_model.Q_layer import quantized_linear, quantized_conv, do_requant, do_fake_quant,do_dequant\n",
    "from mobilenet_model.Q_layer import AVP_Fake_Quant,Q_SELayer_deq, Q_SELayer, QuantizedConv, QuantizedLinear, Preprocess, Quantizer\n",
    "\n",
    "from mobilenet_model.mobilenet_model import SELayer,h_swish,h_sigmoid\n",
    "from mobilenet_model.mobilenet_model import _make_divisible\n",
    "from mobilenet_model.mobilenet_model import Our_MobileNetV3,BN_fold_Our_MobileNetV3\n",
    "\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (block1): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block6): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block7): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=20, bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=20, out_features=10, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN_fold_model = BN_fold_Our_MobileNetV3()\n",
    "BN_fold_weight =torch.load(\"Mobilenet_ckpt/Mobilenet_BN_folded.ckpt\")\n",
    "BN_fold_model.load_state_dict(BN_fold_weight,strict=True)\n",
    "BN_fold_model = BN_fold_model.cuda()\n",
    "BN_fold_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n"
     ]
    }
   ],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "input_activation = {}\n",
    "output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        input_activation[module_name] = x.detach()\n",
    "        output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Linear, nn.ReLU,nn.Conv2d,h_swish,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "hooks = add_range_recoder_hook(BN_fold_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "BN_fold_model(sample_data) #Forward to use hook\n",
    "# print(output_activation['Conv.1'])\n",
    "# print(\"==\")\n",
    "# print(input_activation['Conv.2.avg_pool'])\n",
    "print(output_activation.keys())\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and first Conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = BN_fold_Our_MobileNetV3()\n",
    "quantized_backbone = []\n",
    "quantized_Conv = []\n",
    "i = 0\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"conv1\"])\n",
    "preprocess = Preprocess(input_scale, input_zero_point)\n",
    "quantized_Conv.append(preprocess)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['conv1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.conv1.weight.data)\n",
    "Conv_bias = BN_fold_model.conv1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_Conv.append(quantizedConv1)\n",
    "quantized_Conv.append(req1)\n",
    "\n",
    "quantized_model.conv1 = nn.Sequential(*quantized_Conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block1 = []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv1)\n",
    "quantized_block1.append(h_swish1)\n",
    "quantized_block1.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,8,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedConv2)\n",
    "quantized_block1.append(h_swish2)\n",
    "quantized_block1.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block1.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block1.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block1.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block1.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block1.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block1.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block1.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv3)\n",
    "quantized_block1.append(h_swish3)\n",
    "quantized_block1.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block1 = nn.Sequential(*quantized_block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block2= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv1)\n",
    "quantized_block2.append(h_swish1)\n",
    "quantized_block2.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedConv2)\n",
    "quantized_block2.append(h_swish2)\n",
    "quantized_block2.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block2.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block2.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block2.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block2.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block2.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block2.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block2.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv3)\n",
    "quantized_block2.append(h_swish3)\n",
    "quantized_block2.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block2 = nn.Sequential(*quantized_block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block3= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv1)\n",
    "quantized_block3.append(h_swish1)\n",
    "quantized_block3.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block3.append(quantizedConv2)\n",
    "quantized_block3.append(h_swish2)\n",
    "quantized_block3.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block3.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block3.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block3.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block3.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block3.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block3.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block3.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv3)\n",
    "quantized_block3.append(h_swish3)\n",
    "quantized_block3.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block3 = nn.Sequential(*quantized_block3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block4= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv1)\n",
    "quantized_block4.append(h_swish1)\n",
    "quantized_block4.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedConv2)\n",
    "quantized_block4.append(h_swish2)\n",
    "quantized_block4.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block4.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block4.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block4.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block4.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block4.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block4.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block4.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv3)\n",
    "quantized_block4.append(h_swish3)\n",
    "quantized_block4.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block4 = nn.Sequential(*quantized_block4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block5= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv1)\n",
    "quantized_block5.append(h_swish1)\n",
    "quantized_block5.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedConv2)\n",
    "quantized_block5.append(h_swish2)\n",
    "quantized_block5.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block5.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block5.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block5.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block5.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block5.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block5.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block5.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv3)\n",
    "quantized_block5.append(h_swish3)\n",
    "quantized_block5.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block5 = nn.Sequential(*quantized_block5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block6= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv1)\n",
    "quantized_block6.append(h_swish1)\n",
    "quantized_block6.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedConv2)\n",
    "quantized_block6.append(h_swish2)\n",
    "quantized_block6.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block6.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block6.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block6.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block6.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block6.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block6.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block6.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv3)\n",
    "quantized_block6.append(h_swish3)\n",
    "quantized_block6.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block6 = nn.Sequential(*quantized_block6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block7= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv1)\n",
    "quantized_block7.append(h_swish1)\n",
    "quantized_block7.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block7.append(quantizedConv2)\n",
    "quantized_block7.append(h_swish2)\n",
    "quantized_block7.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block7.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block7.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block7.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block7.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block7.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block7.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block7.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv3)\n",
    "quantized_block7.append(h_swish3)\n",
    "quantized_block7.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block7 = nn.Sequential(*quantized_block7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_avgpool= []\n",
    "quantized_classifier= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['avgpool'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['avgpool'])\n",
    "fake_q = AVP_Fake_Quant(input_scale,output_scale,input_zero_point,output_zero_point) \n",
    "quantized_avgpool.append(fake_q)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.0'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[0].weight.data)\n",
    "quantizedLinear1 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear1)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[2].weight.data)\n",
    "quantizedLinear2 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear2)\n",
    "\n",
    "quantized_model.avgpool = nn.Sequential(*quantized_avgpool)\n",
    "quantized_model.classifier = nn.Sequential(*quantized_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN_fold_Our_MobileNetV3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Preprocess()\n",
      "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (2): Quantizer()\n",
      "  )\n",
      "  (block1): Sequential(\n",
      "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
      "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block6): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block7): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (avgpool): Sequential(\n",
      "    (0): AVP_Fake_Quant(\n",
      "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
      "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Sequential(\n",
       "    (0): Preprocess()\n",
       "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (2): Quantizer()\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
       "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block6): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block7): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (avgpool): Sequential(\n",
       "    (0): AVP_Fake_Quant(\n",
       "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
       "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quantized_model)\n",
    "quantized_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "q_input_activation = {}\n",
    "q_output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        q_input_activation[module_name] = x.detach()\n",
    "        q_output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "\n",
    "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "hooks = add_range_recoder_hook(quantized_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "quantized_model(sample_data) #Forward to use hook\n",
    "\n",
    "\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  #Set the model to train mode\n",
    "  model.train()\n",
    "  for batch, (x, y) in enumerate(dataloader):\n",
    "    if use_gpu:\n",
    "      x, y = x.cuda(), y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    #forward\n",
    "    pred = model(x)\n",
    "\n",
    "    #loss\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    #backward\n",
    "    loss.backward()\n",
    "\n",
    "    #optimize\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      loss, current = loss.item(), (batch + 1) * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  #set model to evaluate mode\n",
    "  model.eval()\n",
    "  model.cuda()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "      if use_gpu:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      pred = model(x)\n",
    "      test_loss = loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
    "optimizer = torch.optim.Adam(BN_fold_model.parameters(), lr=learning_rate)  #define optimizer\n",
    "quantized_model.to(device)\n",
    "quantized_model.eval()\n",
    "torch.save(quantized_model,\"Mobilenet_ckpt\\Quantized_Mobilenet.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.000058 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, BN_fold_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.008795 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, quantized_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n",
      "dict_keys(['conv1.0', 'conv1.1', 'conv1.2', 'block1.0', 'block1.1', 'block1.2', 'block1.3', 'block1.4', 'block1.5', 'block1.6.avg_pool', 'block1.6.fc.0', 'block1.6.fc.1', 'block1.7', 'block1.8', 'block1.9', 'block2.0', 'block2.1', 'block2.2', 'block2.3', 'block2.4', 'block2.5', 'block2.6.avg_pool', 'block2.6.fc.0', 'block2.6.fc.1', 'block2.7', 'block2.8', 'block2.9', 'block3.0', 'block3.1', 'block3.2', 'block3.3', 'block3.4', 'block3.5', 'block3.6.avg_pool', 'block3.6.fc.0', 'block3.6.fc.1', 'block3.7', 'block3.8', 'block3.9', 'block4.0', 'block4.1', 'block4.2', 'block4.3', 'block4.4', 'block4.5', 'block4.6.avg_pool', 'block4.6.fc.0', 'block4.6.fc.1', 'block4.7', 'block4.8', 'block4.9', 'block5.0', 'block5.1', 'block5.2', 'block5.3', 'block5.4', 'block5.5', 'block5.6.avg_pool', 'block5.6.fc.0', 'block5.6.fc.1', 'block5.7', 'block5.8', 'block5.9', 'block6.0', 'block6.1', 'block6.2', 'block6.3', 'block6.4', 'block6.5', 'block6.6.avg_pool', 'block6.6.fc.0', 'block6.6.fc.1', 'block6.7', 'block6.8', 'block6.9', 'block7.0', 'block7.1', 'block7.2', 'block7.3', 'block7.4', 'block7.5', 'block7.6.avg_pool', 'block7.6.fc.0', 'block7.6.fc.1', 'block7.7', 'block7.8', 'block7.9', 'avgpool.0.avp', 'classifier.0', 'classifier.1'])\n"
     ]
    }
   ],
   "source": [
    "print(output_activation.keys())\n",
    "print(q_output_activation.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100352,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        2.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
       "        0.000e+00, 0.000e+00, 3.000e+00, 2.000e+00, 1.000e+00, 1.000e+00,\n",
       "        1.000e+00, 2.000e+00, 0.000e+00, 2.000e+00, 2.000e+00, 1.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 2.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        3.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 3.000e+00,\n",
       "        0.000e+00, 1.000e+00, 8.000e+00, 4.000e+00, 3.000e+00, 1.000e+00,\n",
       "        4.000e+00, 3.000e+00, 3.000e+00, 8.000e+00, 3.000e+00, 7.000e+00,\n",
       "        4.000e+00, 3.000e+00, 3.000e+00, 3.000e+00, 2.000e+00, 2.000e+00,\n",
       "        3.000e+00, 3.000e+00, 5.000e+00, 7.000e+00, 5.000e+00, 6.000e+00,\n",
       "        8.000e+00, 6.000e+00, 1.300e+01, 1.200e+01, 1.900e+01, 3.000e+01,\n",
       "        1.400e+02, 3.000e+01, 5.500e+01, 6.300e+01, 2.480e+02, 2.310e+02,\n",
       "        2.490e+02, 1.770e+02, 9.600e+01, 2.610e+02, 2.960e+02, 3.360e+02,\n",
       "        5.380e+02, 4.120e+02, 6.850e+02, 9.300e+02, 1.148e+03, 1.355e+03,\n",
       "        1.349e+03, 1.505e+03, 1.944e+03, 1.655e+03, 2.172e+03, 2.470e+03,\n",
       "        2.653e+03, 2.328e+03, 2.396e+03, 2.583e+03, 2.262e+03, 2.344e+03,\n",
       "        2.593e+03, 2.505e+03, 2.080e+03, 1.923e+03, 1.904e+03, 1.972e+03,\n",
       "        1.865e+03, 1.995e+03, 1.814e+03, 1.757e+03, 1.777e+03, 1.963e+03,\n",
       "        1.778e+03, 2.047e+03, 1.922e+03, 2.333e+03, 2.351e+03, 2.235e+03,\n",
       "        2.528e+03, 2.991e+03, 2.138e+03, 2.157e+03, 2.018e+03, 2.516e+03,\n",
       "        2.107e+03, 1.811e+03, 1.210e+03, 1.225e+03, 8.870e+02, 8.200e+02,\n",
       "        9.070e+02, 1.138e+03, 6.830e+02, 1.364e+03, 5.370e+02, 5.140e+02,\n",
       "        4.100e+02, 5.880e+02, 8.430e+02, 8.300e+01, 1.720e+02, 5.000e+01,\n",
       "        1.510e+02, 1.580e+02, 1.600e+01, 1.300e+01, 2.400e+01, 2.200e+01,\n",
       "        1.500e+01, 2.400e+01, 2.200e+01, 2.600e+01, 2.500e+01, 2.000e+01,\n",
       "        1.300e+01, 1.000e+01, 1.100e+01, 1.000e+01, 1.000e+01, 1.000e+01,\n",
       "        1.400e+01, 1.000e+01, 9.000e+00, 6.000e+00, 9.000e+00, 2.000e+00,\n",
       "        3.000e+00, 4.000e+00, 7.000e+00, 1.000e+00, 6.000e+00, 4.000e+00,\n",
       "        4.000e+00, 4.000e+00, 7.000e+00, 7.000e+00, 4.000e+00, 1.000e+00,\n",
       "        1.000e+00, 1.000e+00, 1.000e+00, 3.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 2.000e+00, 1.000e+00, 2.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
       "        1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 2.000e+00,\n",
       "        2.000e+00, 2.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        2.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([-7.31194544e+00, -7.24885178e+00, -7.18575811e+00, -7.12266445e+00,\n",
       "        -7.05957079e+00, -6.99647713e+00, -6.93338394e+00, -6.87029028e+00,\n",
       "        -6.80719662e+00, -6.74410295e+00, -6.68100929e+00, -6.61791563e+00,\n",
       "        -6.55482197e+00, -6.49172831e+00, -6.42863464e+00, -6.36554098e+00,\n",
       "        -6.30244732e+00, -6.23935366e+00, -6.17625999e+00, -6.11316681e+00,\n",
       "        -6.05007315e+00, -5.98697948e+00, -5.92388582e+00, -5.86079216e+00,\n",
       "        -5.79769850e+00, -5.73460484e+00, -5.67151117e+00, -5.60841751e+00,\n",
       "        -5.54532385e+00, -5.48223019e+00, -5.41913700e+00, -5.35604334e+00,\n",
       "        -5.29294968e+00, -5.22985601e+00, -5.16676235e+00, -5.10366869e+00,\n",
       "        -5.04057503e+00, -4.97748137e+00, -4.91438770e+00, -4.85129404e+00,\n",
       "        -4.78820038e+00, -4.72510672e+00, -4.66201305e+00, -4.59891987e+00,\n",
       "        -4.53582621e+00, -4.47273254e+00, -4.40963888e+00, -4.34654522e+00,\n",
       "        -4.28345156e+00, -4.22035789e+00, -4.15726423e+00, -4.09417057e+00,\n",
       "        -4.03107691e+00, -3.96798348e+00, -3.90488982e+00, -3.84179616e+00,\n",
       "        -3.77870250e+00, -3.71560907e+00, -3.65251541e+00, -3.58942175e+00,\n",
       "        -3.52632809e+00, -3.46323442e+00, -3.40014076e+00, -3.33704734e+00,\n",
       "        -3.27395368e+00, -3.21086001e+00, -3.14776635e+00, -3.08467269e+00,\n",
       "        -3.02157903e+00, -2.95848560e+00, -2.89539194e+00, -2.83229828e+00,\n",
       "        -2.76920462e+00, -2.70611095e+00, -2.64301729e+00, -2.57992363e+00,\n",
       "        -2.51683021e+00, -2.45373654e+00, -2.39064288e+00, -2.32754922e+00,\n",
       "        -2.26445556e+00, -2.20136213e+00, -2.13826847e+00, -2.07517481e+00,\n",
       "        -2.01208115e+00, -1.94898748e+00, -1.88589394e+00, -1.82280028e+00,\n",
       "        -1.75970662e+00, -1.69661307e+00, -1.63351941e+00, -1.57042575e+00,\n",
       "        -1.50733221e+00, -1.44423854e+00, -1.38114488e+00, -1.31805134e+00,\n",
       "        -1.25495768e+00, -1.19186401e+00, -1.12877047e+00, -1.06567681e+00,\n",
       "        -1.00258315e+00, -9.39489543e-01, -8.76395941e-01, -8.13302338e-01,\n",
       "        -7.50208676e-01, -6.87115073e-01, -6.24021471e-01, -5.60927808e-01,\n",
       "        -4.97834206e-01, -4.34740573e-01, -3.71646971e-01, -3.08553338e-01,\n",
       "        -2.45459720e-01, -1.82366088e-01, -1.19272470e-01, -5.61788492e-02,\n",
       "         6.91477442e-03,  7.00083971e-02,  1.33102015e-01,  1.96195647e-01,\n",
       "         2.59289265e-01,  3.22382897e-01,  3.85476500e-01,  4.48570132e-01,\n",
       "         5.11663735e-01,  5.74757397e-01,  6.37851000e-01,  7.00944602e-01,\n",
       "         7.64038265e-01,  8.27131867e-01,  8.90225470e-01,  9.53319132e-01,\n",
       "         1.01641273e+00,  1.07950640e+00,  1.14259994e+00,  1.20569360e+00,\n",
       "         1.26878726e+00,  1.33188081e+00,  1.39497447e+00,  1.45806813e+00,\n",
       "         1.52116168e+00,  1.58425534e+00,  1.64734900e+00,  1.71044254e+00,\n",
       "         1.77353621e+00,  1.83662987e+00,  1.89972341e+00,  1.96281707e+00,\n",
       "         2.02591062e+00,  2.08900428e+00,  2.15209794e+00,  2.21519160e+00,\n",
       "         2.27828526e+00,  2.34137869e+00,  2.40447235e+00,  2.46756601e+00,\n",
       "         2.53065968e+00,  2.59375334e+00,  2.65684700e+00,  2.71994066e+00,\n",
       "         2.78303409e+00,  2.84612775e+00,  2.90922141e+00,  2.97231507e+00,\n",
       "         3.03540874e+00,  3.09850216e+00,  3.16159582e+00,  3.22468948e+00,\n",
       "         3.28778315e+00,  3.35087681e+00,  3.41397047e+00,  3.47706413e+00,\n",
       "         3.54015756e+00,  3.60325122e+00,  3.66634488e+00,  3.72943854e+00,\n",
       "         3.79253221e+00,  3.85562563e+00,  3.91871929e+00,  3.98181295e+00,\n",
       "         4.04490662e+00,  4.10800028e+00,  4.17109394e+00,  4.23418760e+00,\n",
       "         4.29728127e+00,  4.36037493e+00,  4.42346859e+00,  4.48656178e+00,\n",
       "         4.54965544e+00,  4.61274910e+00,  4.67584276e+00,  4.73893642e+00,\n",
       "         4.80203009e+00,  4.86512375e+00,  4.92821741e+00,  4.99131107e+00,\n",
       "         5.05440474e+00,  5.11749840e+00,  5.18059158e+00,  5.24368525e+00,\n",
       "         5.30677891e+00,  5.36987257e+00,  5.43296623e+00,  5.49605989e+00,\n",
       "         5.55915356e+00,  5.62224722e+00,  5.68534088e+00,  5.74843454e+00,\n",
       "         5.81152821e+00,  5.87462187e+00,  5.93771505e+00,  6.00080872e+00,\n",
       "         6.06390238e+00,  6.12699604e+00,  6.19008970e+00,  6.25318336e+00,\n",
       "         6.31627703e+00,  6.37937069e+00,  6.44246435e+00,  6.50555801e+00,\n",
       "         6.56865168e+00,  6.63174534e+00,  6.69483852e+00,  6.75793219e+00,\n",
       "         6.82102585e+00,  6.88411951e+00,  6.94721317e+00,  7.01030684e+00,\n",
       "         7.07340050e+00,  7.13649416e+00,  7.19958782e+00,  7.26268148e+00,\n",
       "         7.32577515e+00,  7.38886881e+00,  7.45196199e+00,  7.51505566e+00,\n",
       "         7.57814932e+00,  7.64124298e+00,  7.70433664e+00,  7.76743031e+00,\n",
       "         7.83052397e+00,  7.89361763e+00,  7.95671129e+00,  8.01980495e+00,\n",
       "         8.08289814e+00,  8.14599228e+00,  8.20908546e+00,  8.27217960e+00,\n",
       "         8.33527279e+00,  8.39836693e+00,  8.46146011e+00,  8.52455425e+00,\n",
       "         8.58764744e+00,  8.65074062e+00,  8.71383476e+00,  8.77692795e+00,\n",
       "         8.84002209e+00,  8.90311527e+00,  8.96620941e+00,  9.02930260e+00,\n",
       "         9.09239674e+00,  9.15548992e+00,  9.21858406e+00,  9.28167725e+00,\n",
       "         9.34477043e+00,  9.40786457e+00,  9.47095776e+00,  9.53405190e+00,\n",
       "         9.59714508e+00,  9.66023922e+00,  9.72333241e+00,  9.78642654e+00,\n",
       "         9.84951973e+00,  9.91261387e+00,  9.97570705e+00,  1.00388012e+01,\n",
       "         1.01018944e+01,  1.01649876e+01,  1.02280817e+01,  1.02911749e+01,\n",
       "         1.03542690e+01,  1.04173622e+01,  1.04804564e+01,  1.05435495e+01,\n",
       "         1.06066437e+01,  1.06697369e+01,  1.07328310e+01,  1.07959242e+01,\n",
       "         1.08590183e+01,  1.09221115e+01,  1.09852047e+01,  1.10482988e+01,\n",
       "         1.11113920e+01,  1.11744862e+01,  1.12375793e+01,  1.13006735e+01,\n",
       "         1.13637667e+01,  1.14268608e+01,  1.14899540e+01,  1.15530481e+01,\n",
       "         1.16161413e+01,  1.16792345e+01,  1.17423286e+01,  1.18054218e+01,\n",
       "         1.18685160e+01,  1.19316092e+01,  1.19947033e+01,  1.20577965e+01,\n",
       "         1.21208906e+01,  1.21839838e+01,  1.22470779e+01,  1.23101711e+01,\n",
       "         1.23732643e+01,  1.24363585e+01,  1.24994516e+01,  1.25625458e+01,\n",
       "         1.26256390e+01,  1.26887331e+01,  1.27518263e+01,  1.28149204e+01,\n",
       "         1.28780136e+01,  1.29411077e+01,  1.30042009e+01,  1.30672951e+01,\n",
       "         1.31303883e+01]),\n",
       " <BarContainer object of 324 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y = torch.flatten(FP32_model.Conv[0].weight)\n",
    "y = torch.flatten(output_activation['block1.pw2'])\n",
    "y = y.cpu()\n",
    "y = torch.flatten(y)\n",
    "y = y.detach()\n",
    "y = y.numpy()\n",
    "print(y.shape)\n",
    "\n",
    "plt.title(\"Original Mobilenet distribution.block 1 \")\n",
    "plt.hist(y, bins='auto',density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173056,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.5099e+04, 6.4990e+03, 2.1247e+04, 4.6250e+03, 3.3200e+03,\n",
       "        3.1100e+03, 1.0456e+04, 3.1340e+03, 2.4880e+03, 2.9890e+03,\n",
       "        1.0057e+04, 2.1770e+03, 2.1690e+03, 2.0470e+03, 1.9640e+03,\n",
       "        1.8020e+03, 1.9490e+03, 1.7970e+03, 1.7710e+03, 1.7770e+03,\n",
       "        1.7190e+03, 1.6240e+03, 1.5660e+03, 1.4750e+03, 1.4940e+03,\n",
       "        1.6540e+03, 9.5370e+03, 1.5310e+03, 1.5570e+03, 1.6240e+03,\n",
       "        9.2970e+03, 1.0174e+04, 1.4450e+03, 1.4050e+03, 8.8040e+03,\n",
       "        1.2170e+03, 8.2200e+02, 7.4300e+02, 6.5500e+02, 5.7300e+02,\n",
       "        6.3000e+02, 5.3100e+02, 5.1900e+02, 5.4200e+02, 6.5300e+02,\n",
       "        5.6200e+02, 5.9400e+02, 5.1100e+02, 5.3500e+02, 5.1300e+02,\n",
       "        4.5200e+02, 4.5900e+02, 4.3600e+02, 3.9800e+02, 3.4100e+02,\n",
       "        3.1900e+02, 2.9600e+02, 2.7700e+02, 3.1700e+02, 3.8100e+02,\n",
       "        4.1200e+02, 3.4900e+02, 2.4900e+02, 2.0600e+02, 1.8300e+02,\n",
       "        1.7400e+02, 2.4200e+02, 2.4600e+02, 2.0800e+02, 1.7800e+02,\n",
       "        1.3500e+02, 1.2900e+02, 1.1000e+02, 1.0300e+02, 9.6000e+01,\n",
       "        7.3000e+01, 5.9000e+01, 5.6000e+01, 4.5000e+01, 3.8000e+01,\n",
       "        5.3000e+01, 4.6000e+01, 3.2000e+01, 4.1000e+01, 3.1000e+01,\n",
       "        2.7000e+01, 2.9000e+01, 3.0000e+01, 3.2000e+01, 2.4000e+01,\n",
       "        3.4000e+01, 3.1000e+01, 3.1000e+01, 4.0000e+01, 2.8000e+01,\n",
       "        2.4000e+01, 2.5000e+01, 2.2000e+01, 3.1000e+01, 1.6000e+01,\n",
       "        2.3000e+01, 1.7000e+01, 2.4000e+01, 1.6000e+01, 2.1000e+01,\n",
       "        1.2000e+01, 1.3000e+01, 1.7000e+01, 1.8000e+01, 1.2000e+01,\n",
       "        1.9000e+01, 1.8000e+01, 1.4000e+01, 1.2000e+01, 2.3000e+01,\n",
       "        1.5000e+01, 2.5000e+01, 1.6000e+01, 1.0000e+01, 2.0000e+01,\n",
       "        1.2000e+01, 1.1000e+01, 1.8000e+01, 1.6000e+01, 1.0000e+01,\n",
       "        1.5000e+01, 9.0000e+00, 6.0000e+00, 6.0000e+00, 2.0000e+00,\n",
       "        3.0000e+00, 2.0000e+00, 4.0000e+00, 2.0000e+00, 3.0000e+00,\n",
       "        3.0000e+00, 0.0000e+00, 3.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([-0.37500003, -0.34536228, -0.31572452, -0.28608674, -0.25644898,\n",
       "        -0.22681123, -0.19717346, -0.16753571, -0.13789794, -0.10826018,\n",
       "        -0.07862242, -0.04898466, -0.0193469 ,  0.01029086,  0.03992863,\n",
       "         0.06956638,  0.09920415,  0.12884191,  0.15847968,  0.18811743,\n",
       "         0.2177552 ,  0.24739295,  0.27703071,  0.30666849,  0.33630624,\n",
       "         0.365944  ,  0.39558175,  0.42521951,  0.45485729,  0.48449504,\n",
       "         0.5141328 ,  0.54377055,  0.57340831,  0.60304606,  0.63268387,\n",
       "         0.66232163,  0.69195938,  0.72159714,  0.75123489,  0.78087264,\n",
       "         0.8105104 ,  0.84014815,  0.86978596,  0.89942372,  0.92906147,\n",
       "         0.95869923,  0.98833698,  1.01797473,  1.04761255,  1.07725024,\n",
       "         1.10688806,  1.13652575,  1.16616356,  1.19580126,  1.22543907,\n",
       "         1.25507689,  1.28471458,  1.31435239,  1.34399009,  1.3736279 ,\n",
       "         1.4032656 ,  1.43290341,  1.4625411 ,  1.49217892,  1.52181673,\n",
       "         1.55145442,  1.58109224,  1.61072993,  1.64036775,  1.67000544,\n",
       "         1.69964325,  1.72928095,  1.75891876,  1.78855658,  1.81819427,\n",
       "         1.84783208,  1.87746978,  1.90710759,  1.93674529,  1.9663831 ,\n",
       "         1.99602091,  2.02565861,  2.05529642,  2.08493423,  2.11457181,\n",
       "         2.14420962,  2.17384744,  2.20348525,  2.23312306,  2.26276064,\n",
       "         2.29239845,  2.32203627,  2.35167408,  2.38131166,  2.41094947,\n",
       "         2.44058728,  2.4702251 ,  2.49986291,  2.52950048,  2.5591383 ,\n",
       "         2.58877611,  2.61841393,  2.6480515 ,  2.67768931,  2.70732713,\n",
       "         2.73696494,  2.76660275,  2.79624033,  2.82587814,  2.85551596,\n",
       "         2.88515377,  2.91479135,  2.94442916,  2.97406697,  3.00370479,\n",
       "         3.0333426 ,  3.06298018,  3.09261799,  3.1222558 ,  3.15189362,\n",
       "         3.18153119,  3.211169  ,  3.24080682,  3.27044463,  3.30008245,\n",
       "         3.32972002,  3.35935783,  3.38899565,  3.41863346,  3.44827104,\n",
       "         3.47790885,  3.50754666,  3.53718448,  3.56682229,  3.59645987,\n",
       "         3.62609768,  3.65573549,  3.68537331,  3.71501112,  3.74464869,\n",
       "         3.77428651,  3.80392432,  3.83356214,  3.86319971,  3.89283752,\n",
       "         3.92247534,  3.95211315,  3.98175097,  4.01138878,  4.04102659,\n",
       "         4.07066393,  4.10030174,  4.12993956,  4.15957737,  4.18921518,\n",
       "         4.218853  ,  4.24849081,  4.27812862,  4.30776644,  4.33740377,\n",
       "         4.36704159,  4.3966794 ,  4.42631721,  4.45595503]),\n",
       " <BarContainer object of 163 artists>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = torch.flatten(quantized_weights12)\n",
    "#x = torch.flatten(quantized_model.Conv[1].weights)\n",
    "#print(q_output_activation['block7.9'])\n",
    "x = torch.flatten(q_output_activation['block1.1'])\n",
    "x = x.cpu()\n",
    "x = torch.flatten(x)\n",
    "x = x.detach()\n",
    "x = x.numpy()\n",
    "print(x.shape)\n",
    "\n",
    "plt.title(\"Quantized Mobilenet distribution.block1\")\n",
    "plt.hist(x, bins='auto',density=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
