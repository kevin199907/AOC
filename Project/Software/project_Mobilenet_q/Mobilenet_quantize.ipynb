{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\胡家豪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "from mobilenet_model.Q_layer import bn_folding_model, bn_folding, fold_conv_bn_eval\n",
    "from mobilenet_model.Q_layer import get_scale_and_zero_point, linear_quantize\n",
    "from mobilenet_model.Q_layer import quantized_linear, quantized_conv, do_requant, do_fake_quant,do_dequant\n",
    "from mobilenet_model.Q_layer import AVP_Fake_Quant,Q_SELayer_deq, Q_SELayer, QuantizedConv, QuantizedLinear, Preprocess, Quantizer\n",
    "\n",
    "from mobilenet_model.mobilenet_model import SELayer,h_swish,h_sigmoid\n",
    "from mobilenet_model.mobilenet_model import _make_divisible\n",
    "from mobilenet_model.mobilenet_model import Our_MobileNetV3,BN_fold_Our_MobileNetV3\n",
    "\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (block1): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block6): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block7): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=20, bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=20, out_features=10, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN_fold_model = BN_fold_Our_MobileNetV3()\n",
    "BN_fold_weight =torch.load(\"Mobilenet_ckpt\\Mobilenet_BN_folded.ckpt\")\n",
    "BN_fold_model.load_state_dict(BN_fold_weight,strict=True)\n",
    "BN_fold_model = BN_fold_model.cuda()\n",
    "BN_fold_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n"
     ]
    }
   ],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "input_activation = {}\n",
    "output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        input_activation[module_name] = x.detach()\n",
    "        output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Linear, nn.ReLU,nn.Conv2d,h_swish,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "hooks = add_range_recoder_hook(BN_fold_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "BN_fold_model(sample_data) #Forward to use hook\n",
    "# print(output_activation['Conv.1'])\n",
    "# print(\"==\")\n",
    "# print(input_activation['Conv.2.avg_pool'])\n",
    "print(output_activation.keys())\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and first Conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = BN_fold_Our_MobileNetV3()\n",
    "quantized_backbone = []\n",
    "quantized_Conv = []\n",
    "i = 0\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"conv1\"])\n",
    "preprocess = Preprocess(input_scale, input_zero_point)\n",
    "quantized_Conv.append(preprocess)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['conv1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.conv1.weight.data)\n",
    "Conv_bias = BN_fold_model.conv1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_Conv.append(quantizedConv1)\n",
    "quantized_Conv.append(req1)\n",
    "\n",
    "quantized_model.conv1 = nn.Sequential(*quantized_Conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block1 = []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv1)\n",
    "quantized_block1.append(h_swish1)\n",
    "quantized_block1.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,8,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedConv2)\n",
    "quantized_block1.append(h_swish2)\n",
    "quantized_block1.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block1.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block1.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block1.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block1.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block1.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block1.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block1.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv3)\n",
    "quantized_block1.append(h_swish3)\n",
    "quantized_block1.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block1 = nn.Sequential(*quantized_block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block2= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv1)\n",
    "quantized_block2.append(h_swish1)\n",
    "quantized_block2.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedConv2)\n",
    "quantized_block2.append(h_swish2)\n",
    "quantized_block2.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block2.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block2.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block2.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block2.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block2.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block2.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block2.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv3)\n",
    "quantized_block2.append(h_swish3)\n",
    "quantized_block2.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block2 = nn.Sequential(*quantized_block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block3= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv1)\n",
    "quantized_block3.append(h_swish1)\n",
    "quantized_block3.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block3.append(quantizedConv2)\n",
    "quantized_block3.append(h_swish2)\n",
    "quantized_block3.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block3.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block3.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block3.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block3.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block3.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block3.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block3.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv3)\n",
    "quantized_block3.append(h_swish3)\n",
    "quantized_block3.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block3 = nn.Sequential(*quantized_block3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block4= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv1)\n",
    "quantized_block4.append(h_swish1)\n",
    "quantized_block4.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedConv2)\n",
    "quantized_block4.append(h_swish2)\n",
    "quantized_block4.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block4.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block4.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block4.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block4.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block4.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block4.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block4.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv3)\n",
    "quantized_block4.append(h_swish3)\n",
    "quantized_block4.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block4 = nn.Sequential(*quantized_block4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block5= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv1)\n",
    "quantized_block5.append(h_swish1)\n",
    "quantized_block5.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedConv2)\n",
    "quantized_block5.append(h_swish2)\n",
    "quantized_block5.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block5.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block5.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block5.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block5.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block5.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block5.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block5.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv3)\n",
    "quantized_block5.append(h_swish3)\n",
    "quantized_block5.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block5 = nn.Sequential(*quantized_block5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block6= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv1)\n",
    "quantized_block6.append(h_swish1)\n",
    "quantized_block6.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedConv2)\n",
    "quantized_block6.append(h_swish2)\n",
    "quantized_block6.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block6.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block6.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block6.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block6.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block6.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block6.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block6.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv3)\n",
    "quantized_block6.append(h_swish3)\n",
    "quantized_block6.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block6 = nn.Sequential(*quantized_block6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block7= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv1)\n",
    "quantized_block7.append(h_swish1)\n",
    "quantized_block7.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block7.append(quantizedConv2)\n",
    "quantized_block7.append(h_swish2)\n",
    "quantized_block7.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block7.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block7.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block7.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block7.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block7.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block7.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block7.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv3)\n",
    "quantized_block7.append(h_swish3)\n",
    "quantized_block7.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block7 = nn.Sequential(*quantized_block7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_avgpool= []\n",
    "quantized_classifier= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['avgpool'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['avgpool'])\n",
    "fake_q = AVP_Fake_Quant(input_scale,output_scale,input_zero_point,output_zero_point) \n",
    "quantized_avgpool.append(fake_q)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.0'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[0].weight.data)\n",
    "quantizedLinear1 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear1)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[2].weight.data)\n",
    "quantizedLinear2 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear2)\n",
    "\n",
    "quantized_model.avgpool = nn.Sequential(*quantized_avgpool)\n",
    "quantized_model.classifier = nn.Sequential(*quantized_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN_fold_Our_MobileNetV3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Preprocess()\n",
      "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (2): Quantizer()\n",
      "  )\n",
      "  (block1): Sequential(\n",
      "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
      "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block6): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block7): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (avgpool): Sequential(\n",
      "    (0): AVP_Fake_Quant(\n",
      "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
      "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Sequential(\n",
       "    (0): Preprocess()\n",
       "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (2): Quantizer()\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
       "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block6): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block7): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (avgpool): Sequential(\n",
       "    (0): AVP_Fake_Quant(\n",
       "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
       "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quantized_model)\n",
    "quantized_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "q_input_activation = {}\n",
    "q_output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        q_input_activation[module_name] = x.detach()\n",
    "        q_output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "\n",
    "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "hooks = add_range_recoder_hook(quantized_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "quantized_model(sample_data) #Forward to use hook\n",
    "\n",
    "\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  #Set the model to train mode\n",
    "  model.train()\n",
    "  for batch, (x, y) in enumerate(dataloader):\n",
    "    if use_gpu:\n",
    "      x, y = x.cuda(), y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    #forward\n",
    "    pred = model(x)\n",
    "\n",
    "    #loss\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    #backward\n",
    "    loss.backward()\n",
    "\n",
    "    #optimize\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      loss, current = loss.item(), (batch + 1) * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  #set model to evaluate mode\n",
    "  model.eval()\n",
    "  model.cuda()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "      if use_gpu:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      pred = model(x)\n",
    "      test_loss = loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
    "optimizer = torch.optim.Adam(BN_fold_model.parameters(), lr=learning_rate)  #define optimizer\n",
    "quantized_model.to(device)\n",
    "quantized_model.eval()\n",
    "torch.save(quantized_model,\"Mobilenet_ckpt\\Quantized_Mobilenet.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.000058 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, BN_fold_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.009868 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, quantized_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n",
      "dict_keys(['conv1.0', 'conv1.1', 'conv1.2', 'block1.0', 'block1.1', 'block1.2', 'block1.3', 'block1.4', 'block1.5', 'block1.6.avg_pool', 'block1.6.fc.0', 'block1.6.fc.1', 'block1.7', 'block1.8', 'block1.9', 'block2.0', 'block2.1', 'block2.2', 'block2.3', 'block2.4', 'block2.5', 'block2.6.avg_pool', 'block2.6.fc.0', 'block2.6.fc.1', 'block2.7', 'block2.8', 'block2.9', 'block3.0', 'block3.1', 'block3.2', 'block3.3', 'block3.4', 'block3.5', 'block3.6.avg_pool', 'block3.6.fc.0', 'block3.6.fc.1', 'block3.7', 'block3.8', 'block3.9', 'block4.0', 'block4.1', 'block4.2', 'block4.3', 'block4.4', 'block4.5', 'block4.6.avg_pool', 'block4.6.fc.0', 'block4.6.fc.1', 'block4.7', 'block4.8', 'block4.9', 'block5.0', 'block5.1', 'block5.2', 'block5.3', 'block5.4', 'block5.5', 'block5.6.avg_pool', 'block5.6.fc.0', 'block5.6.fc.1', 'block5.7', 'block5.8', 'block5.9', 'block6.0', 'block6.1', 'block6.2', 'block6.3', 'block6.4', 'block6.5', 'block6.6.avg_pool', 'block6.6.fc.0', 'block6.6.fc.1', 'block6.7', 'block6.8', 'block6.9', 'block7.0', 'block7.1', 'block7.2', 'block7.3', 'block7.4', 'block7.5', 'block7.6.avg_pool', 'block7.6.fc.0', 'block7.6.fc.1', 'block7.7', 'block7.8', 'block7.9', 'avgpool.0.avp', 'classifier.0', 'classifier.1'])\n"
     ]
    }
   ],
   "source": [
    "print(output_activation.keys())\n",
    "print(q_output_activation.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200704,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.5430e+04, 6.9640e+03, 1.6338e+04, 1.5067e+04, 3.5370e+03,\n",
       "        3.2190e+03, 2.9930e+03, 1.3409e+04, 2.6510e+03, 3.0720e+03,\n",
       "        1.2831e+04, 2.3750e+03, 2.2470e+03, 2.1490e+03, 2.1300e+03,\n",
       "        1.9820e+03, 1.9470e+03, 1.9560e+03, 1.8080e+03, 1.8860e+03,\n",
       "        1.8560e+03, 1.7350e+03, 1.7120e+03, 1.6170e+03, 1.5520e+03,\n",
       "        1.5460e+03, 2.2540e+03, 1.1694e+04, 1.5550e+03, 1.6770e+03,\n",
       "        1.7580e+03, 1.9900e+03, 2.2811e+04, 1.4830e+03, 1.4220e+03,\n",
       "        1.1675e+04, 1.0590e+03, 8.6800e+02, 7.8500e+02, 6.6100e+02,\n",
       "        5.8300e+02, 5.7400e+02, 6.0500e+02, 5.3500e+02, 5.2100e+02,\n",
       "        6.0400e+02, 6.1000e+02, 5.9200e+02, 5.4100e+02, 5.5700e+02,\n",
       "        5.3600e+02, 4.8800e+02, 4.6600e+02, 4.6500e+02, 4.1800e+02,\n",
       "        3.8300e+02, 3.5600e+02, 3.2300e+02, 2.7600e+02, 3.1500e+02,\n",
       "        2.7900e+02, 3.7100e+02, 3.7300e+02, 3.8100e+02, 2.8900e+02,\n",
       "        2.1300e+02, 1.8800e+02, 2.0900e+02, 2.7600e+02, 2.4200e+02,\n",
       "        2.2600e+02, 1.4800e+02, 1.5800e+02, 1.1300e+02, 1.1800e+02,\n",
       "        9.2000e+01, 1.0000e+02, 9.2000e+01, 6.3000e+01, 6.8000e+01,\n",
       "        4.9000e+01, 4.9000e+01, 4.3000e+01, 4.4000e+01, 4.1000e+01,\n",
       "        4.0000e+01, 3.1000e+01, 4.3000e+01, 3.5000e+01, 3.6000e+01,\n",
       "        3.1000e+01, 2.0000e+01, 3.7000e+01, 2.7000e+01, 3.9000e+01,\n",
       "        3.0000e+01, 3.3000e+01, 2.8000e+01, 2.0000e+01, 3.0000e+01,\n",
       "        2.3000e+01, 3.0000e+01, 2.4000e+01, 1.9000e+01, 2.2000e+01,\n",
       "        2.0000e+01, 1.9000e+01, 1.3000e+01, 1.4000e+01, 1.7000e+01,\n",
       "        1.1000e+01, 1.5000e+01, 2.1000e+01, 1.4000e+01, 1.5000e+01,\n",
       "        1.1000e+01, 1.6000e+01, 2.0000e+01, 2.2000e+01, 1.7000e+01,\n",
       "        2.1000e+01, 1.5000e+01, 1.4000e+01, 1.1000e+01, 1.1000e+01,\n",
       "        2.5000e+01, 1.0000e+01, 9.0000e+00, 1.4000e+01, 1.1000e+01,\n",
       "        9.0000e+00, 1.0000e+00, 5.0000e+00, 4.0000e+00, 1.0000e+00,\n",
       "        3.0000e+00, 4.0000e+00, 3.0000e+00, 3.0000e+00, 2.0000e+00,\n",
       "        2.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00]),\n",
       " array([-3.75000030e-01, -3.46166939e-01, -3.17333817e-01, -2.88500726e-01,\n",
       "        -2.59667635e-01, -2.30834529e-01, -2.02001438e-01, -1.73168331e-01,\n",
       "        -1.44335240e-01, -1.15502134e-01, -8.66690353e-02, -5.78359365e-02,\n",
       "        -2.90028378e-02, -1.69738414e-04,  2.86633614e-02,  5.74964620e-02,\n",
       "         8.63295570e-02,  1.15162656e-01,  1.43995762e-01,  1.72828853e-01,\n",
       "         2.01661959e-01,  2.30495051e-01,  2.59328157e-01,  2.88161248e-01,\n",
       "         3.16994369e-01,  3.45827460e-01,  3.74660552e-01,  4.03493643e-01,\n",
       "         4.32326764e-01,  4.61159855e-01,  4.89992946e-01,  5.18826067e-01,\n",
       "         5.47659159e-01,  5.76492250e-01,  6.05325341e-01,  6.34158432e-01,\n",
       "         6.62991524e-01,  6.91824675e-01,  7.20657766e-01,  7.49490857e-01,\n",
       "         7.78323948e-01,  8.07157040e-01,  8.35990131e-01,  8.64823222e-01,\n",
       "         8.93656313e-01,  9.22489464e-01,  9.51322556e-01,  9.80155647e-01,\n",
       "         1.00898874e+00,  1.03782189e+00,  1.06665492e+00,  1.09548807e+00,\n",
       "         1.12432110e+00,  1.15315425e+00,  1.18198729e+00,  1.21082044e+00,\n",
       "         1.23965359e+00,  1.26848662e+00,  1.29731977e+00,  1.32615280e+00,\n",
       "         1.35498595e+00,  1.38381898e+00,  1.41265213e+00,  1.44148529e+00,\n",
       "         1.47031832e+00,  1.49915147e+00,  1.52798450e+00,  1.55681765e+00,\n",
       "         1.58565068e+00,  1.61448383e+00,  1.64331686e+00,  1.67215002e+00,\n",
       "         1.70098317e+00,  1.72981620e+00,  1.75864935e+00,  1.78748238e+00,\n",
       "         1.81631553e+00,  1.84514856e+00,  1.87398171e+00,  1.90281487e+00,\n",
       "         1.93164790e+00,  1.96048105e+00,  1.98931408e+00,  2.01814723e+00,\n",
       "         2.04698038e+00,  2.07581353e+00,  2.10464644e+00,  2.13347960e+00,\n",
       "         2.16231275e+00,  2.19114590e+00,  2.21997881e+00,  2.24881196e+00,\n",
       "         2.27764511e+00,  2.30647826e+00,  2.33531141e+00,  2.36414433e+00,\n",
       "         2.39297748e+00,  2.42181063e+00,  2.45064378e+00,  2.47947669e+00,\n",
       "         2.50830984e+00,  2.53714299e+00,  2.56597614e+00,  2.59480929e+00,\n",
       "         2.62364221e+00,  2.65247536e+00,  2.68130851e+00,  2.71014166e+00,\n",
       "         2.73897481e+00,  2.76780772e+00,  2.79664087e+00,  2.82547402e+00,\n",
       "         2.85430717e+00,  2.88314009e+00,  2.91197324e+00,  2.94080639e+00,\n",
       "         2.96963954e+00,  2.99847269e+00,  3.02730560e+00,  3.05613875e+00,\n",
       "         3.08497190e+00,  3.11380506e+00,  3.14263821e+00,  3.17147112e+00,\n",
       "         3.20030427e+00,  3.22913742e+00,  3.25797057e+00,  3.28680348e+00,\n",
       "         3.31563663e+00,  3.34446979e+00,  3.37330294e+00,  3.40213609e+00,\n",
       "         3.43096900e+00,  3.45980215e+00,  3.48863530e+00,  3.51746845e+00,\n",
       "         3.54630136e+00,  3.57513452e+00,  3.60396767e+00,  3.63280082e+00,\n",
       "         3.66163397e+00,  3.69046688e+00,  3.71930003e+00,  3.74813318e+00,\n",
       "         3.77696633e+00,  3.80579948e+00,  3.83463240e+00,  3.86346555e+00,\n",
       "         3.89229870e+00,  3.92113185e+00,  3.94996476e+00,  3.97879791e+00,\n",
       "         4.00763130e+00,  4.03646421e+00,  4.06529713e+00,  4.09413052e+00,\n",
       "         4.12296343e+00,  4.15179634e+00,  4.18062973e+00,  4.20946264e+00,\n",
       "         4.23829603e+00,  4.26712894e+00,  4.29596186e+00,  4.32479525e+00,\n",
       "         4.35362816e+00,  4.38246155e+00,  4.41129446e+00]),\n",
       " <BarContainer object of 166 artists>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y = torch.flatten(FP32_model.Conv[0].weight)\n",
    "y = torch.flatten(output_activation['block1.hs1'])\n",
    "y = y.cpu()\n",
    "y = torch.flatten(y)\n",
    "y = y.detach()\n",
    "y = y.numpy()\n",
    "print(y.shape)\n",
    "\n",
    "plt.title(\"Original Mobilenet distribution.block 1 \")\n",
    "plt.hist(y, bins='auto',density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173056,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.4189e+04, 1.6405e+04, 3.6400e+03, 1.3467e+04, 2.1100e+03,\n",
       "        4.0130e+03, 1.1300e+04, 2.0650e+03, 3.2230e+03, 1.9270e+03,\n",
       "        1.0833e+04, 1.4260e+03, 2.7050e+03, 2.5920e+03, 1.2490e+03,\n",
       "        2.3320e+03, 1.2460e+03, 2.3000e+03, 1.1240e+03, 2.2590e+03,\n",
       "        2.1560e+03, 1.0460e+03, 2.0170e+03, 9.4300e+02, 1.8790e+03,\n",
       "        9.8640e+03, 1.1700e+03, 1.9770e+03, 9.7100e+02, 2.0900e+03,\n",
       "        1.1640e+03, 1.8114e+04, 1.8480e+03, 8.8000e+02, 9.1280e+03,\n",
       "        8.0500e+02, 1.0780e+03, 4.7600e+02, 8.3600e+02, 7.4900e+02,\n",
       "        3.7800e+02, 6.8400e+02, 3.2900e+02, 7.1500e+02, 4.1600e+02,\n",
       "        7.3700e+02, 7.2100e+02, 3.3200e+02, 6.6100e+02, 3.2400e+02,\n",
       "        6.0600e+02, 5.7000e+02, 2.7000e+02, 5.1300e+02, 1.8800e+02,\n",
       "        4.1700e+02, 1.8600e+02, 3.5700e+02, 4.1100e+02, 2.6400e+02,\n",
       "        4.9500e+02, 2.3300e+02, 3.1500e+02, 1.2200e+02, 2.3800e+02,\n",
       "        2.2600e+02, 1.7300e+02, 2.9300e+02, 1.3600e+02, 2.2500e+02,\n",
       "        1.7400e+02, 7.6000e+01, 1.4100e+02, 6.2000e+01, 1.1200e+02,\n",
       "        5.1000e+01, 7.2000e+01, 7.0000e+01, 3.0000e+01, 4.6000e+01,\n",
       "        3.8000e+01, 5.8000e+01, 1.9000e+01, 4.6000e+01, 4.0000e+01,\n",
       "        1.6000e+01, 4.0000e+01, 1.6000e+01, 4.1000e+01, 1.5000e+01,\n",
       "        3.5000e+01, 5.0000e+01, 1.5000e+01, 4.4000e+01, 2.0000e+01,\n",
       "        3.1000e+01, 2.9000e+01, 1.9000e+01, 2.9000e+01, 1.3000e+01,\n",
       "        2.5000e+01, 1.3000e+01, 3.0000e+01, 2.3000e+01, 1.1000e+01,\n",
       "        1.5000e+01, 8.0000e+00, 2.1000e+01, 1.2000e+01, 2.1000e+01,\n",
       "        2.1000e+01, 1.1000e+01, 1.8000e+01, 5.0000e+00, 3.4000e+01,\n",
       "        1.7000e+01, 1.4000e+01, 2.0000e+01, 7.0000e+00, 2.6000e+01,\n",
       "        3.0000e+00, 1.7000e+01, 2.3000e+01, 7.0000e+00, 1.4000e+01,\n",
       "        9.0000e+00, 1.2000e+01, 3.0000e+00, 6.0000e+00, 3.0000e+00,\n",
       "        2.0000e+00, 5.0000e+00, 1.0000e+00, 4.0000e+00, 1.0000e+00,\n",
       "        3.0000e+00, 2.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00]),\n",
       " array([-127.        , -125.42236328, -123.84471893, -122.26708221,\n",
       "        -120.68943787, -119.11180115, -117.53416443, -115.95652008,\n",
       "        -114.37888336, -112.80123901, -111.22360229, -109.64596558,\n",
       "        -108.06832123, -106.49068451, -104.91304016, -103.33540344,\n",
       "        -101.75776672, -100.18012238,  -98.60248566,  -97.02484131,\n",
       "         -95.44720459,  -93.86956787,  -92.29192352,  -90.7142868 ,\n",
       "         -89.13664246,  -87.55900574,  -85.98136902,  -84.40372467,\n",
       "         -82.82608795,  -81.2484436 ,  -79.67080688,  -78.09317017,\n",
       "         -76.51552582,  -74.9378891 ,  -73.36024475,  -71.78260803,\n",
       "         -70.20497131,  -68.62732697,  -67.04969025,  -65.4720459 ,\n",
       "         -63.89440918,  -62.31676865,  -60.73913193,  -59.16149139,\n",
       "         -57.58385086,  -56.00621033,  -54.42856979,  -52.85093307,\n",
       "         -51.27329254,  -49.69565201,  -48.11801147,  -46.54037094,\n",
       "         -44.96273422,  -43.38509369,  -41.80745316,  -40.22981262,\n",
       "         -38.65217209,  -37.07453537,  -35.49689484,  -33.9192543 ,\n",
       "         -32.34161377,  -30.76397514,  -29.18633461,  -27.60869598,\n",
       "         -26.03105545,  -24.45341682,  -22.87577629,  -21.29813576,\n",
       "         -19.72049713,  -18.1428566 ,  -16.56521797,  -14.98757744,\n",
       "         -13.40993786,  -11.83229828,  -10.2546587 ,   -8.67701817,\n",
       "          -7.09937906,   -5.52173901,   -3.94409943,   -2.36645961,\n",
       "          -0.78881985,    0.78881985,    2.36645961,    3.94409943,\n",
       "           5.52173901,    7.09937906,    8.67701817,   10.2546587 ,\n",
       "          11.83229828,   13.40993786,   14.98757744,   16.56521797,\n",
       "          18.1428566 ,   19.72049713,   21.29813576,   22.87577629,\n",
       "          24.45341682,   26.03105545,   27.60869598,   29.18633461,\n",
       "          30.76397514,   32.34161377,   33.9192543 ,   35.49689484,\n",
       "          37.07453537,   38.65217209,   40.22981262,   41.80745316,\n",
       "          43.38509369,   44.96273422,   46.54037094,   48.11801147,\n",
       "          49.69565201,   51.27329254,   52.85093307,   54.42856979,\n",
       "          56.00621033,   57.58385086,   59.16149139,   60.73913193,\n",
       "          62.31676865,   63.89440918,   65.4720459 ,   67.04969025,\n",
       "          68.62732697,   70.20497131,   71.78260803,   73.36024475,\n",
       "          74.9378891 ,   76.51552582,   78.09317017,   79.67080688,\n",
       "          81.2484436 ,   82.82608795,   84.40372467,   85.98136902,\n",
       "          87.55900574,   89.13664246,   90.7142868 ,   92.29192352,\n",
       "          93.86956787,   95.44720459,   97.02484131,   98.60248566,\n",
       "         100.18012238,  101.75776672,  103.33540344,  104.91304016,\n",
       "         106.49068451,  108.06832123,  109.64596558,  111.22360229,\n",
       "         112.80123901,  114.37888336,  115.95652008,  117.53416443,\n",
       "         119.11180115,  120.68943787,  122.26708221,  123.84471893,\n",
       "         125.42236328,  127.        ]),\n",
       " <BarContainer object of 161 artists>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = torch.flatten(quantized_weights12)\n",
    "#x = torch.flatten(quantized_model.Conv[1].weights)\n",
    "#print(q_output_activation['block7.9'])\n",
    "x = torch.flatten(q_output_activation['block1.2'])\n",
    "x = x.cpu()\n",
    "x = torch.flatten(x)\n",
    "x = x.detach()\n",
    "x = x.numpy()\n",
    "print(x.shape)\n",
    "\n",
    "plt.title(\"Quantized Mobilenet distribution.block1\")\n",
    "plt.hist(x, bins='auto',density=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
