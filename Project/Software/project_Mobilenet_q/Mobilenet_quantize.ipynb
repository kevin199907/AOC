{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\胡家豪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "from mobilenet_model.Q_layer import bn_folding_model, bn_folding, fold_conv_bn_eval\n",
    "from mobilenet_model.Q_layer import get_scale_and_zero_point, linear_quantize\n",
    "from mobilenet_model.Q_layer import quantized_linear, quantized_conv, do_requant, do_fake_quant,do_dequant\n",
    "from mobilenet_model.Q_layer import AVP_Fake_Quant,Q_SELayer_deq, Q_SELayer, QuantizedConv, QuantizedLinear, Preprocess, Quantizer\n",
    "\n",
    "from mobilenet_model.mobilenet_model import SELayer,h_swish,h_sigmoid\n",
    "from mobilenet_model.mobilenet_model import _make_divisible\n",
    "from mobilenet_model.mobilenet_model import Our_MobileNetV3,BN_fold_Our_MobileNetV3\n",
    "\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (block1): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block6): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block7): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=20, bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=20, out_features=10, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN_fold_model = BN_fold_Our_MobileNetV3()\n",
    "BN_fold_weight =torch.load(\"Mobilenet_ckpt\\Mobilenet_BN_folded.ckpt\")\n",
    "BN_fold_model.load_state_dict(BN_fold_weight,strict=True)\n",
    "BN_fold_model = BN_fold_model.cuda()\n",
    "BN_fold_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n"
     ]
    }
   ],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "input_activation = {}\n",
    "output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        input_activation[module_name] = x.detach()\n",
    "        output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Linear, nn.ReLU,nn.Conv2d,h_swish,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "hooks = add_range_recoder_hook(BN_fold_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "BN_fold_model(sample_data) #Forward to use hook\n",
    "# print(output_activation['Conv.1'])\n",
    "# print(\"==\")\n",
    "# print(input_activation['Conv.2.avg_pool'])\n",
    "print(output_activation.keys())\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and first Conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = BN_fold_Our_MobileNetV3()\n",
    "quantized_backbone = []\n",
    "quantized_Conv = []\n",
    "i = 0\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"conv1\"])\n",
    "preprocess = Preprocess(input_scale, input_zero_point)\n",
    "quantized_Conv.append(preprocess)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['conv1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.conv1.weight.data)\n",
    "Conv_bias = BN_fold_model.conv1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_Conv.append(quantizedConv1)\n",
    "quantized_Conv.append(req1)\n",
    "\n",
    "quantized_model.conv1 = nn.Sequential(*quantized_Conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block1 = []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv1)\n",
    "quantized_block1.append(h_swish1)\n",
    "quantized_block1.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,8,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedConv2)\n",
    "quantized_block1.append(h_swish2)\n",
    "quantized_block1.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block1.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block1.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block1.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block1.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block1.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block1.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block1.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv3)\n",
    "quantized_block1.append(h_swish3)\n",
    "quantized_block1.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block1 = nn.Sequential(*quantized_block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block2= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv1)\n",
    "quantized_block2.append(h_swish1)\n",
    "quantized_block2.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedConv2)\n",
    "quantized_block2.append(h_swish2)\n",
    "quantized_block2.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block2.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block2.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block2.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block2.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block2.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block2.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block2.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv3)\n",
    "quantized_block2.append(h_swish3)\n",
    "quantized_block2.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block2 = nn.Sequential(*quantized_block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block3= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv1)\n",
    "quantized_block3.append(h_swish1)\n",
    "quantized_block3.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block3.append(quantizedConv2)\n",
    "quantized_block3.append(h_swish2)\n",
    "quantized_block3.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block3.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block3.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block3.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block3.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block3.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block3.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block3.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv3)\n",
    "quantized_block3.append(h_swish3)\n",
    "quantized_block3.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block3 = nn.Sequential(*quantized_block3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block4= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv1)\n",
    "quantized_block4.append(h_swish1)\n",
    "quantized_block4.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedConv2)\n",
    "quantized_block4.append(h_swish2)\n",
    "quantized_block4.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block4.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block4.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block4.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block4.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block4.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block4.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block4.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv3)\n",
    "quantized_block4.append(h_swish3)\n",
    "quantized_block4.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block4 = nn.Sequential(*quantized_block4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block5= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv1)\n",
    "quantized_block5.append(h_swish1)\n",
    "quantized_block5.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedConv2)\n",
    "quantized_block5.append(h_swish2)\n",
    "quantized_block5.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block5.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block5.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block5.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block5.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block5.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block5.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block5.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv3)\n",
    "quantized_block5.append(h_swish3)\n",
    "quantized_block5.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block5 = nn.Sequential(*quantized_block5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block6= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv1)\n",
    "quantized_block6.append(h_swish1)\n",
    "quantized_block6.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedConv2)\n",
    "quantized_block6.append(h_swish2)\n",
    "quantized_block6.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block6.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block6.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block6.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block6.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block6.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block6.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block6.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv3)\n",
    "quantized_block6.append(h_swish3)\n",
    "quantized_block6.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block6 = nn.Sequential(*quantized_block6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block7= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv1)\n",
    "quantized_block7.append(h_swish1)\n",
    "quantized_block7.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block7.append(quantizedConv2)\n",
    "quantized_block7.append(h_swish2)\n",
    "quantized_block7.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block7.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block7.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block7.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block7.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block7.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block7.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block7.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv3)\n",
    "quantized_block7.append(h_swish3)\n",
    "quantized_block7.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block7 = nn.Sequential(*quantized_block7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_avgpool= []\n",
    "quantized_classifier= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['avgpool'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['avgpool'])\n",
    "fake_q = AVP_Fake_Quant(input_scale,output_scale,input_zero_point,output_zero_point) \n",
    "quantized_avgpool.append(fake_q)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.0'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[0].weight.data)\n",
    "quantizedLinear1 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear1)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[2].weight.data)\n",
    "quantizedLinear2 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear2)\n",
    "\n",
    "quantized_model.avgpool = nn.Sequential(*quantized_avgpool)\n",
    "quantized_model.classifier = nn.Sequential(*quantized_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN_fold_Our_MobileNetV3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Preprocess()\n",
      "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (2): Quantizer()\n",
      "  )\n",
      "  (block1): Sequential(\n",
      "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
      "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block6): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block7): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (avgpool): Sequential(\n",
      "    (0): AVP_Fake_Quant(\n",
      "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
      "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Sequential(\n",
       "    (0): Preprocess()\n",
       "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (2): Quantizer()\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
       "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block6): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block7): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (avgpool): Sequential(\n",
       "    (0): AVP_Fake_Quant(\n",
       "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
       "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quantized_model)\n",
    "quantized_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "q_input_activation = {}\n",
    "q_output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        q_input_activation[module_name] = x.detach()\n",
    "        q_output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "\n",
    "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "hooks = add_range_recoder_hook(quantized_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "quantized_model(sample_data) #Forward to use hook\n",
    "\n",
    "\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  #Set the model to train mode\n",
    "  model.train()\n",
    "  for batch, (x, y) in enumerate(dataloader):\n",
    "    if use_gpu:\n",
    "      x, y = x.cuda(), y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    #forward\n",
    "    pred = model(x)\n",
    "\n",
    "    #loss\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    #backward\n",
    "    loss.backward()\n",
    "\n",
    "    #optimize\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      loss, current = loss.item(), (batch + 1) * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  #set model to evaluate mode\n",
    "  model.eval()\n",
    "  model.cuda()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "      if use_gpu:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      pred = model(x)\n",
    "      test_loss = loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
    "optimizer = torch.optim.Adam(BN_fold_model.parameters(), lr=learning_rate)  #define optimizer\n",
    "quantized_model.to(device)\n",
    "quantized_model.eval()\n",
    "torch.save(quantized_model,\"Mobilenet_ckpt\\Quantized_Mobilenet.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.000058 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, BN_fold_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.009868 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, quantized_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n",
      "dict_keys(['conv1.0', 'conv1.1', 'conv1.2', 'block1.0', 'block1.1', 'block1.2', 'block1.3', 'block1.4', 'block1.5', 'block1.6.avg_pool', 'block1.6.fc.0', 'block1.6.fc.1', 'block1.7', 'block1.8', 'block1.9', 'block2.0', 'block2.1', 'block2.2', 'block2.3', 'block2.4', 'block2.5', 'block2.6.avg_pool', 'block2.6.fc.0', 'block2.6.fc.1', 'block2.7', 'block2.8', 'block2.9', 'block3.0', 'block3.1', 'block3.2', 'block3.3', 'block3.4', 'block3.5', 'block3.6.avg_pool', 'block3.6.fc.0', 'block3.6.fc.1', 'block3.7', 'block3.8', 'block3.9', 'block4.0', 'block4.1', 'block4.2', 'block4.3', 'block4.4', 'block4.5', 'block4.6.avg_pool', 'block4.6.fc.0', 'block4.6.fc.1', 'block4.7', 'block4.8', 'block4.9', 'block5.0', 'block5.1', 'block5.2', 'block5.3', 'block5.4', 'block5.5', 'block5.6.avg_pool', 'block5.6.fc.0', 'block5.6.fc.1', 'block5.7', 'block5.8', 'block5.9', 'block6.0', 'block6.1', 'block6.2', 'block6.3', 'block6.4', 'block6.5', 'block6.6.avg_pool', 'block6.6.fc.0', 'block6.6.fc.1', 'block6.7', 'block6.8', 'block6.9', 'block7.0', 'block7.1', 'block7.2', 'block7.3', 'block7.4', 'block7.5', 'block7.6.avg_pool', 'block7.6.fc.0', 'block7.6.fc.1', 'block7.7', 'block7.8', 'block7.9', 'avgpool.0.avp', 'classifier.0', 'classifier.1'])\n"
     ]
    }
   ],
   "source": [
    "print(output_activation.keys())\n",
    "print(q_output_activation.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([390., 169.,  80.,  59.,  48.,  38.,  33.,  16.,  15.,  13.,  11.,\n",
       "         16.,  16.,   8.,  11.,   7.,   7.,  12.,  12.,  10.,  11.,   7.,\n",
       "          9.,   4.,   3.,   7.,   2.,   1.,   5.,   1.,   2.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([-0.37499753, -0.18090647,  0.01318456,  0.20727561,  0.40136665,\n",
       "         0.59545767,  0.78954875,  0.98363978,  1.1777308 ,  1.37182188,\n",
       "         1.56591296,  1.76000392,  1.95409501,  2.14818597,  2.34227705,\n",
       "         2.53636813,  2.73045921,  2.92455029,  3.11864138,  3.31273222,\n",
       "         3.5068233 ,  3.70091438,  3.89500546,  4.08909655,  4.28318739,\n",
       "         4.47727871,  4.67136955,  4.86546087,  5.05955172,  5.25364256,\n",
       "         5.44773388,  5.64182472,  5.83591604,  6.03000689,  6.22409821,\n",
       "         6.41818905,  6.61227989,  6.80637121,  7.00046206,  7.19455338,\n",
       "         7.38864422,  7.58273554,  7.77682638,  7.97091722,  8.16500854,\n",
       "         8.35909939,  8.55319023,  8.74728203,  8.94137287,  9.13546371,\n",
       "         9.32955456,  9.5236454 ,  9.7177372 ,  9.91182804, 10.10591888]),\n",
       " <BarContainer object of 54 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y = torch.flatten(FP32_model.Conv[0].weight)\n",
    "y = torch.flatten(output_activation['block7.hs4'])\n",
    "y = y.cpu()\n",
    "y = torch.flatten(y)\n",
    "y = y.detach()\n",
    "y = y.numpy()\n",
    "print(y.shape)\n",
    "\n",
    "plt.hist(y, bins='auto',density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([360., 163., 117.,  47.,  50.,  33.,  45.,  24.,  20.,  18.,  21.,\n",
       "         18.,  13.,  14.,  10.,   9.,   8.,   5.,  10.,   8.,   8.,   4.,\n",
       "         10.,   1.,   2.,   2.,   1.,   0.,   0.,   0.,   1.,   1.,   0.,\n",
       "          0.,   0.,   1.]),\n",
       " array([-127. , -122.5, -118. , -113.5, -109. , -104.5, -100. ,  -95.5,\n",
       "         -91. ,  -86.5,  -82. ,  -77.5,  -73. ,  -68.5,  -64. ,  -59.5,\n",
       "         -55. ,  -50.5,  -46. ,  -41.5,  -37. ,  -32.5,  -28. ,  -23.5,\n",
       "         -19. ,  -14.5,  -10. ,   -5.5,   -1. ,    3.5,    8. ,   12.5,\n",
       "          17. ,   21.5,   26. ,   30.5,   35. ]),\n",
       " <BarContainer object of 36 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = torch.flatten(quantized_weights12)\n",
    "#x = torch.flatten(quantized_model.Conv[1].weights)\n",
    "#print(q_output_activation['block7.9'])\n",
    "x = torch.flatten(q_output_activation['block7.9'])\n",
    "x = x.cpu()\n",
    "x = torch.flatten(x)\n",
    "x = x.detach()\n",
    "x = x.numpy()\n",
    "print(x.shape)\n",
    "\n",
    "plt.hist(x, bins='auto',density=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
