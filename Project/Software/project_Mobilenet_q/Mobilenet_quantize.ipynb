{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\胡家豪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "from mobilenet_model.Q_layer import bn_folding_model, bn_folding, fold_conv_bn_eval\n",
    "from mobilenet_model.Q_layer import get_scale_and_zero_point, linear_quantize\n",
    "from mobilenet_model.Q_layer import quantized_linear, quantized_conv, do_requant, do_fake_quant,do_dequant\n",
    "from mobilenet_model.Q_layer import AVP_Fake_Quant,Q_SELayer_deq, Q_SELayer, QuantizedConv, QuantizedLinear, Preprocess, Quantizer\n",
    "\n",
    "from mobilenet_model.mobilenet_model import SELayer,h_swish,h_sigmoid\n",
    "from mobilenet_model.mobilenet_model import _make_divisible\n",
    "from mobilenet_model.mobilenet_model import Our_MobileNetV3,BN_fold_Our_MobileNetV3\n",
    "\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (block1): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=64, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block6): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block7): BN_fold_MobileNetV3_block(\n",
       "    (pw1): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
       "    (hs2): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (se1): SELayer(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=16, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=16, out_features=48, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (hs4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=20, bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=20, out_features=10, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN_fold_model = BN_fold_Our_MobileNetV3()\n",
    "BN_fold_weight =torch.load(\"Mobilenet_ckpt\\Mobilenet_BN_folded.ckpt\")\n",
    "BN_fold_model.load_state_dict(BN_fold_weight,strict=True)\n",
    "BN_fold_model = BN_fold_model.cuda()\n",
    "BN_fold_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n"
     ]
    }
   ],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "input_activation = {}\n",
    "output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        input_activation[module_name] = x.detach()\n",
    "        output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Linear, nn.ReLU,nn.Conv2d,h_swish,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "hooks = add_range_recoder_hook(BN_fold_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "BN_fold_model(sample_data) #Forward to use hook\n",
    "# print(output_activation['Conv.1'])\n",
    "# print(\"==\")\n",
    "# print(input_activation['Conv.2.avg_pool'])\n",
    "print(output_activation.keys())\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and first Conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = BN_fold_Our_MobileNetV3()\n",
    "quantized_backbone = []\n",
    "quantized_Conv = []\n",
    "i = 0\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"conv1\"])\n",
    "preprocess = Preprocess(input_scale, input_zero_point)\n",
    "quantized_Conv.append(preprocess)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['conv1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.conv1.weight.data)\n",
    "Conv_bias = BN_fold_model.conv1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['conv1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_Conv.append(quantizedConv1)\n",
    "quantized_Conv.append(req1)\n",
    "\n",
    "quantized_model.conv1 = nn.Sequential(*quantized_Conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block1 = []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv1)\n",
    "quantized_block1.append(h_swish1)\n",
    "quantized_block1.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,8,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedConv2)\n",
    "quantized_block1.append(h_swish2)\n",
    "quantized_block1.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block1.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block1.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block1.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block1.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block1.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block1.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block1.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block1.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block1.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block1.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block1.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block1.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block1.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block1.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block1.append(quantizedConv3)\n",
    "quantized_block1.append(h_swish3)\n",
    "quantized_block1.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block1 = nn.Sequential(*quantized_block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block2= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv1)\n",
    "quantized_block2.append(h_swish1)\n",
    "quantized_block2.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedConv2)\n",
    "quantized_block2.append(h_swish2)\n",
    "quantized_block2.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block2.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block2.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block2.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block2.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block2.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block2.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block2.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block2.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block2.append(quantizedSE_linear1)\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block2.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block2.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block2.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block2.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block2.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block2.append(quantizedConv3)\n",
    "quantized_block2.append(h_swish3)\n",
    "quantized_block2.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block2 = nn.Sequential(*quantized_block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block3= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv1)\n",
    "quantized_block3.append(h_swish1)\n",
    "quantized_block3.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block3.append(quantizedConv2)\n",
    "quantized_block3.append(h_swish2)\n",
    "quantized_block3.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block3.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block3.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block3.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block3.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block3.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block3.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block3.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block3.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block3.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block3.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block3.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block3.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block3.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block3.append(quantizedConv3)\n",
    "quantized_block3.append(h_swish3)\n",
    "quantized_block3.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block3 = nn.Sequential(*quantized_block3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block4= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv1)\n",
    "quantized_block4.append(h_swish1)\n",
    "quantized_block4.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,64,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedConv2)\n",
    "quantized_block4.append(h_swish2)\n",
    "quantized_block4.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block4.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block4.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block4.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block4.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block4.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block4.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block4.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block4.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block4.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block4.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block4.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block4.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block4.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block4.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block4.append(quantizedConv3)\n",
    "quantized_block4.append(h_swish3)\n",
    "quantized_block4.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block4 = nn.Sequential(*quantized_block4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block5= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv1)\n",
    "quantized_block5.append(h_swish1)\n",
    "quantized_block5.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedConv2)\n",
    "quantized_block5.append(h_swish2)\n",
    "quantized_block5.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block5.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block5.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block5.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block5.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block5.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block5.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block5.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block5.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "quantized_block5.append(quantizedSE_linear1)\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block5.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block5.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block5.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block5.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block5.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block5.append(quantizedConv3)\n",
    "quantized_block5.append(h_swish3)\n",
    "quantized_block5.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block5 = nn.Sequential(*quantized_block5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block6= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv1)\n",
    "quantized_block6.append(h_swish1)\n",
    "quantized_block6.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,96,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedConv2)\n",
    "quantized_block6.append(h_swish2)\n",
    "quantized_block6.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block6.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block6.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block6.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block6.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block6.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block6.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block6.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block6.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "\n",
    "\n",
    "quantized_block6.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block6.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block6.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block6.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block6.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block6.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block6.append(quantizedConv3)\n",
    "quantized_block6.append(h_swish3)\n",
    "quantized_block6.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block6 = nn.Sequential(*quantized_block6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride, padding ,group\n",
    "########## pw###############\n",
    "quantized_block7= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw1.bias.data\n",
    "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish1 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs1'])\n",
    "req1 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv1)\n",
    "quantized_block7.append(h_swish1)\n",
    "quantized_block7.append(req1)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# dw ##############\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.dw1'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.dw1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.dw1.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.dw1.bias.data\n",
    "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 2,1,48,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish2 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "req2 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "\n",
    "quantized_block7.append(quantizedConv2)\n",
    "quantized_block7.append(h_swish2)\n",
    "quantized_block7.append(req2)\n",
    "\n",
    "\n",
    "###############################\n",
    "############# SE #############\n",
    "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['block7.se1.fc.0'])\n",
    "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['block7.se1.fc.1'])\n",
    "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(BN_fold_model.block7.se1.fc[0].weight.data)\n",
    "\n",
    "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['block7.se1.fc.2'])\n",
    "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['block7.se1.fc.3'])\n",
    "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(BN_fold_model.block7.se1.fc[2].weight.data)\n",
    "\n",
    "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['block7.hs2'])\n",
    "\n",
    "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "\n",
    "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['block7.se1.avg_pool'])\n",
    "\n",
    "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
    "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
    "                               SE_in_scale, SE_in_zero_point,\n",
    "                               SE_out_scale, SE_out_zero_point,\n",
    "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedSE_linear1)\n",
    "\n",
    "\n",
    "###############################\n",
    "######## pw ###################\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['block7.pw2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['block7.pw2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.block7.pw2.weight.data)\n",
    "Conv_bias = BN_fold_model.block7.pw2.bias.data\n",
    "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "h_swish3 = h_swish()\n",
    "#quantized_model.Conv[0] = quantizedConv1 \n",
    "\n",
    "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['block7.hs4'])\n",
    "req3 = Quantizer(req_scale,output_zero_point)\n",
    "\n",
    "quantized_block7.append(quantizedConv3)\n",
    "quantized_block7.append(h_swish3)\n",
    "quantized_block7.append(req3)\n",
    "###############################\n",
    "#print(quantized_block1)\n",
    "\n",
    "quantized_model.block7 = nn.Sequential(*quantized_block7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_avgpool= []\n",
    "quantized_classifier= []\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['avgpool'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['avgpool'])\n",
    "fake_q = AVP_Fake_Quant(input_scale,output_scale,input_zero_point,output_zero_point) \n",
    "quantized_avgpool.append(fake_q)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.0'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.1'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[0].weight.data)\n",
    "quantizedLinear1 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear1)\n",
    "\n",
    "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['classifier.2'])\n",
    "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['classifier.2'])\n",
    "quantized_weights, weight_scale, weight_zero_point = linear_quantize(BN_fold_model.classifier[2].weight.data)\n",
    "quantizedLinear2 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
    "quantized_classifier.append(quantizedLinear2)\n",
    "\n",
    "quantized_model.avgpool = nn.Sequential(*quantized_avgpool)\n",
    "quantized_model.classifier = nn.Sequential(*quantized_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN_fold_Our_MobileNetV3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Preprocess()\n",
      "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (2): Quantizer()\n",
      "  )\n",
      "  (block1): Sequential(\n",
      "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
      "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block6): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block7): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (avgpool): Sequential(\n",
      "    (0): AVP_Fake_Quant(\n",
      "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
      "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BN_fold_Our_MobileNetV3(\n",
       "  (conv1): Sequential(\n",
       "    (0): Preprocess()\n",
       "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (2): Quantizer()\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
       "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
       "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block6): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
       "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (block7): Sequential(\n",
       "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
       "    (1): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Quantizer()\n",
       "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
       "    (4): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Quantizer()\n",
       "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
       "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
       "    (8): h_swish(\n",
       "      (sigmoid): h_sigmoid(\n",
       "        (relu): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Quantizer()\n",
       "  )\n",
       "  (avgpool): Sequential(\n",
       "    (0): AVP_Fake_Quant(\n",
       "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
       "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quantized_model)\n",
    "quantized_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "q_input_activation = {}\n",
    "q_output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        q_input_activation[module_name] = x.detach()\n",
    "        q_output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "\n",
    "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "hooks = add_range_recoder_hook(quantized_model)\n",
    "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "quantized_model(sample_data) #Forward to use hook\n",
    "\n",
    "\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  #Set the model to train mode\n",
    "  model.train()\n",
    "  for batch, (x, y) in enumerate(dataloader):\n",
    "    if use_gpu:\n",
    "      x, y = x.cuda(), y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    #forward\n",
    "    pred = model(x)\n",
    "\n",
    "    #loss\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    #backward\n",
    "    loss.backward()\n",
    "\n",
    "    #optimize\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      loss, current = loss.item(), (batch + 1) * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  #set model to evaluate mode\n",
    "  model.eval()\n",
    "  model.cuda()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "      if use_gpu:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      pred = model(x)\n",
    "      test_loss = loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
    "optimizer = torch.optim.Adam(BN_fold_model.parameters(), lr=learning_rate)  #define optimizer\n",
    "quantized_model.to(device)\n",
    "quantized_model.eval()\n",
    "torch.save(quantized_model,\"Mobilenet_ckpt\\Quantized_Mobilenet.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.000058 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, BN_fold_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.009868 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader, quantized_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'block1.pw1', 'block1.hs1', 'block1.dw1', 'block1.hs2', 'block1.se1.avg_pool', 'block1.se1.fc.0', 'block1.se1.fc.1', 'block1.se1.fc.2', 'block1.se1.fc.3', 'block1.pw2', 'block1.hs4', 'block2.pw1', 'block2.hs1', 'block2.dw1', 'block2.hs2', 'block2.se1.avg_pool', 'block2.se1.fc.0', 'block2.se1.fc.1', 'block2.se1.fc.2', 'block2.se1.fc.3', 'block2.pw2', 'block2.hs4', 'block3.pw1', 'block3.hs1', 'block3.dw1', 'block3.hs2', 'block3.se1.avg_pool', 'block3.se1.fc.0', 'block3.se1.fc.1', 'block3.se1.fc.2', 'block3.se1.fc.3', 'block3.pw2', 'block3.hs4', 'block4.pw1', 'block4.hs1', 'block4.dw1', 'block4.hs2', 'block4.se1.avg_pool', 'block4.se1.fc.0', 'block4.se1.fc.1', 'block4.se1.fc.2', 'block4.se1.fc.3', 'block4.pw2', 'block4.hs4', 'block5.pw1', 'block5.hs1', 'block5.dw1', 'block5.hs2', 'block5.se1.avg_pool', 'block5.se1.fc.0', 'block5.se1.fc.1', 'block5.se1.fc.2', 'block5.se1.fc.3', 'block5.pw2', 'block5.hs4', 'block6.pw1', 'block6.hs1', 'block6.dw1', 'block6.hs2', 'block6.se1.avg_pool', 'block6.se1.fc.0', 'block6.se1.fc.1', 'block6.se1.fc.2', 'block6.se1.fc.3', 'block6.pw2', 'block6.hs4', 'block7.pw1', 'block7.hs1', 'block7.dw1', 'block7.hs2', 'block7.se1.avg_pool', 'block7.se1.fc.0', 'block7.se1.fc.1', 'block7.se1.fc.2', 'block7.se1.fc.3', 'block7.pw2', 'block7.hs4', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2'])\n",
      "dict_keys(['conv1.0', 'conv1.1', 'conv1.2', 'block1.0', 'block1.1', 'block1.2', 'block1.3', 'block1.4', 'block1.5', 'block1.6.avg_pool', 'block1.6.fc.0', 'block1.6.fc.1', 'block1.7', 'block1.8', 'block1.9', 'block2.0', 'block2.1', 'block2.2', 'block2.3', 'block2.4', 'block2.5', 'block2.6.avg_pool', 'block2.6.fc.0', 'block2.6.fc.1', 'block2.7', 'block2.8', 'block2.9', 'block3.0', 'block3.1', 'block3.2', 'block3.3', 'block3.4', 'block3.5', 'block3.6.avg_pool', 'block3.6.fc.0', 'block3.6.fc.1', 'block3.7', 'block3.8', 'block3.9', 'block4.0', 'block4.1', 'block4.2', 'block4.3', 'block4.4', 'block4.5', 'block4.6.avg_pool', 'block4.6.fc.0', 'block4.6.fc.1', 'block4.7', 'block4.8', 'block4.9', 'block5.0', 'block5.1', 'block5.2', 'block5.3', 'block5.4', 'block5.5', 'block5.6.avg_pool', 'block5.6.fc.0', 'block5.6.fc.1', 'block5.7', 'block5.8', 'block5.9', 'block6.0', 'block6.1', 'block6.2', 'block6.3', 'block6.4', 'block6.5', 'block6.6.avg_pool', 'block6.6.fc.0', 'block6.6.fc.1', 'block6.7', 'block6.8', 'block6.9', 'block7.0', 'block7.1', 'block7.2', 'block7.3', 'block7.4', 'block7.5', 'block7.6.avg_pool', 'block7.6.fc.0', 'block7.6.fc.1', 'block7.7', 'block7.8', 'block7.9', 'avgpool.0.avp', 'classifier.0', 'classifier.1'])\n"
     ]
    }
   ],
   "source": [
    "print(output_activation.keys())\n",
    "print(q_output_activation.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([438., 245., 231., 175., 163., 128.,  98.,  77.,  71.,  45.,  44.,\n",
       "         53.,  46.,  35.,  43.,  19.,  21.,  17.,  17.,  16.,   8.,  13.,\n",
       "         13.,   5.,   2.,   8.,   0.,   4.,   1.,   2.,   0.,   0.,   2.,\n",
       "          0.,   0.,   0.,   1.,   2.,   2.,   1.,   0.,   0.,   1.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   1.]),\n",
       " array([-0.37499961, -0.27995762, -0.1849156 , -0.0898736 ,  0.0051684 ,\n",
       "         0.10021041,  0.19525242,  0.29029441,  0.38533643,  0.48037842,\n",
       "         0.57542044,  0.67046243,  0.76550442,  0.86054647,  0.95558846,\n",
       "         1.05063045,  1.14567244,  1.24071443,  1.33575642,  1.43079853,\n",
       "         1.52584052,  1.62088251,  1.7159245 ,  1.81096649,  1.90600848,\n",
       "         2.00105047,  2.09609246,  2.19113445,  2.28617644,  2.38121843,\n",
       "         2.47626042,  2.57130241,  2.66634464,  2.76138663,  2.85642862,\n",
       "         2.95147061,  3.0465126 ,  3.14155459,  3.23659658,  3.33163857,\n",
       "         3.42668056,  3.52172256,  3.61676455,  3.71180654,  3.80684853,\n",
       "         3.90189052,  3.99693251,  4.09197474,  4.18701649,  4.28205872,\n",
       "         4.37710047,  4.4721427 ,  4.56718445,  4.66222668,  4.75726843,\n",
       "         4.85231066,  4.94735241,  5.04239464]),\n",
       " <BarContainer object of 57 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y = torch.flatten(FP32_model.Conv[0].weight)\n",
    "y = torch.flatten(output_activation['block6.hs4'])\n",
    "y = y.cpu()\n",
    "y = torch.flatten(y)\n",
    "y = y.detach()\n",
    "y = y.numpy()\n",
    "print(y.shape)\n",
    "\n",
    "plt.title(\"Original Mobilenet distribution.block 6 \")\n",
    "plt.hist(y, bins='auto',density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([341., 231., 230., 222., 171., 133., 114.,  86.,  98.,  60.,  45.,\n",
       "         43.,  55.,  35.,  28.,  26.,  20.,  18.,  10.,  12.,  12.,  11.,\n",
       "          7.,   8.,   7.,   3.,   5.,   4.,   1.,   2.,   1.,   1.,   2.,\n",
       "          1.,   2.,   0.,   1.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   1.]),\n",
       " array([-1.27000000e+02, -1.22765961e+02, -1.18531914e+02, -1.14297874e+02,\n",
       "        -1.10063828e+02, -1.05829788e+02, -1.01595741e+02, -9.73617020e+01,\n",
       "        -9.31276627e+01, -8.88936157e+01, -8.46595764e+01, -8.04255295e+01,\n",
       "        -7.61914902e+01, -7.19574432e+01, -6.77234039e+01, -6.34893608e+01,\n",
       "        -5.92553177e+01, -5.50212784e+01, -5.07872353e+01, -4.65531921e+01,\n",
       "        -4.23191490e+01, -3.80851059e+01, -3.38510628e+01, -2.96170216e+01,\n",
       "        -2.53829784e+01, -2.11489353e+01, -1.69148941e+01, -1.26808510e+01,\n",
       "        -8.44680882e+00, -4.21276617e+00,  2.12765951e-02,  4.25531912e+00,\n",
       "         8.48936176e+00,  1.27234039e+01,  1.69574471e+01,  2.11914902e+01,\n",
       "         2.54255314e+01,  2.96595745e+01,  3.38936157e+01,  3.81276588e+01,\n",
       "         4.23617020e+01,  4.65957451e+01,  5.08297882e+01,  5.50638313e+01,\n",
       "         5.92978706e+01,  6.35319138e+01,  6.77659607e+01,  7.20000000e+01]),\n",
       " <BarContainer object of 47 artists>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = torch.flatten(quantized_weights12)\n",
    "#x = torch.flatten(quantized_model.Conv[1].weights)\n",
    "#print(q_output_activation['block7.9'])\n",
    "x = torch.flatten(q_output_activation['block6.9'])\n",
    "x = x.cpu()\n",
    "x = torch.flatten(x)\n",
    "x = x.detach()\n",
    "x = x.numpy()\n",
    "print(x.shape)\n",
    "\n",
    "plt.title(\"Quantized Mobilenet distribution.block6\")\n",
    "plt.hist(x, bins='auto',density=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
