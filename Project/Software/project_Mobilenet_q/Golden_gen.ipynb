{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\胡家豪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "from mobilenet_model.Q_layer import bn_folding_model, bn_folding, fold_conv_bn_eval\n",
    "from mobilenet_model.Q_layer import get_scale_and_zero_point, linear_quantize\n",
    "from mobilenet_model.Q_layer import quantized_linear, quantized_conv, do_requant, do_fake_quant,do_dequant\n",
    "from mobilenet_model.Q_layer import AVP_Fake_Quant,Q_SELayer_deq, Q_SELayer, QuantizedConv, QuantizedLinear, Preprocess, Quantizer\n",
    "\n",
    "from mobilenet_model.mobilenet_model import SELayer,h_swish,h_sigmoid\n",
    "from mobilenet_model.mobilenet_model import _make_divisible\n",
    "from mobilenet_model.mobilenet_model import Our_MobileNetV3,BN_fold_Our_MobileNetV3\n",
    "\n",
    "from mobilenet_model.golden_gen import bias_gen, signed_dec2hex_matrix, signed_dec2hex, golden_gen, input_or_weight_gen, DecToBin_machine\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Mobilenet_model = torch.load('Mobilenet_ckpt\\Quantized_Mobilenet.pt',map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "q_input_activation = {}\n",
    "q_output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        q_input_activation[module_name] = x.detach()\n",
    "        q_output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "\n",
    "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "hooks = add_range_recoder_hook(Q_Mobilenet_model)\n",
    "sample_data = iter(q_test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "Q_Mobilenet_model(sample_data) #Forward to use hook\n",
    "\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.009868 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  #set model to evaluate mode\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "      if use_gpu:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      pred = model(x)\n",
    "      test_loss = loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "test_loop(test_loader, Q_Mobilenet_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1.0', 'conv1.1', 'conv1.2', 'block1.0', 'block1.1', 'block1.2', 'block1.3', 'block1.4', 'block1.5', 'block1.6.fc.0', 'block1.6.fc.1', 'block1.7', 'block1.8', 'block1.9', 'block2.0', 'block2.1', 'block2.2', 'block2.3', 'block2.4', 'block2.5', 'block2.6.fc.0', 'block2.6.fc.1', 'block2.7', 'block2.8', 'block2.9', 'block3.0', 'block3.1', 'block3.2', 'block3.3', 'block3.4', 'block3.5', 'block3.6.fc.0', 'block3.6.fc.1', 'block3.7', 'block3.8', 'block3.9', 'block4.0', 'block4.1', 'block4.2', 'block4.3', 'block4.4', 'block4.5', 'block4.6.fc.0', 'block4.6.fc.1', 'block4.7', 'block4.8', 'block4.9', 'block5.0', 'block5.1', 'block5.2', 'block5.3', 'block5.4', 'block5.5', 'block5.6.fc.0', 'block5.6.fc.1', 'block5.7', 'block5.8', 'block5.9', 'block6.0', 'block6.1', 'block6.2', 'block6.3', 'block6.4', 'block6.5', 'block6.6.fc.0', 'block6.6.fc.1', 'block6.7', 'block6.8', 'block6.9', 'block7.0', 'block7.1', 'block7.2', 'block7.3', 'block7.4', 'block7.5', 'block7.6.fc.0', 'block7.6.fc.1', 'block7.7', 'block7.8', 'block7.9', 'classifier.0', 'classifier.1'])\n"
     ]
    }
   ],
   "source": [
    "print(q_input_activation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 3, 3])\n",
      "tensor([[[[ -88.,  -21.,   53.],\n",
      "          [ -20.,  -68.,  127.],\n",
      "          [ 105., -127.,   77.]]],\n",
      "\n",
      "\n",
      "        [[[  63.,   34.,   13.],\n",
      "          [  61.,   25.,  -41.],\n",
      "          [ -60.,  -67.,   13.]]],\n",
      "\n",
      "\n",
      "        [[[  20.,   32.,   12.],\n",
      "          [ -12.,   17.,   22.],\n",
      "          [  21.,   13.,   21.]]],\n",
      "\n",
      "\n",
      "        [[[  18.,   54.,  -25.],\n",
      "          [  14.,   37.,  -39.],\n",
      "          [  18.,  -41.,   13.]]],\n",
      "\n",
      "\n",
      "        [[[  35.,    3.,   31.],\n",
      "          [  19.,   43.,   29.],\n",
      "          [ -22.,  -76.,  -54.]]],\n",
      "\n",
      "\n",
      "        [[[   3.,    2.,  -13.],\n",
      "          [   4.,  -14.,  -17.],\n",
      "          [  14.,   10.,  -13.]]],\n",
      "\n",
      "\n",
      "        [[[  52.,   -8.,   22.],\n",
      "          [  20.,   56.,  -50.],\n",
      "          [ -35.,   66.,  -47.]]],\n",
      "\n",
      "\n",
      "        [[[ -50.,   46.,   28.],\n",
      "          [  35.,  -11.,    0.],\n",
      "          [  22.,  -74.,  -43.]]]], device='cuda:0')\n",
      "byte0: a8 bc 4d 3d bd 0c 15 36 d9 23 2b ca 04 0a 16 dd 2e 00\n",
      "=======\n",
      "byte1: eb 7f 3f 19 0d f4 0d e7 12 03 1d 03 f2 f3 14 42 1c 16\n",
      "=======\n",
      "byte2: 35 69 22 d7 14 11 15 0e d7 1f ea 02 ef 34 38 d1 23 b6\n",
      "=======\n",
      "byte3: ec 81 0d c4 20 16 12 25 0d 13 b4 f3 0e f8 ce ce f5 d5\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['a8',\n",
       "  'bc',\n",
       "  '4d',\n",
       "  '3d',\n",
       "  'bd',\n",
       "  '0c',\n",
       "  '15',\n",
       "  '36',\n",
       "  'd9',\n",
       "  '23',\n",
       "  '2b',\n",
       "  'ca',\n",
       "  '04',\n",
       "  '0a',\n",
       "  '16',\n",
       "  'dd',\n",
       "  '2e',\n",
       "  '00'],\n",
       " ['eb',\n",
       "  '7f',\n",
       "  '3f',\n",
       "  '19',\n",
       "  '0d',\n",
       "  'f4',\n",
       "  '0d',\n",
       "  'e7',\n",
       "  '12',\n",
       "  '03',\n",
       "  '1d',\n",
       "  '03',\n",
       "  'f2',\n",
       "  'f3',\n",
       "  '14',\n",
       "  '42',\n",
       "  '1c',\n",
       "  '16'],\n",
       " ['35',\n",
       "  '69',\n",
       "  '22',\n",
       "  'd7',\n",
       "  '14',\n",
       "  '11',\n",
       "  '15',\n",
       "  '0e',\n",
       "  'd7',\n",
       "  '1f',\n",
       "  'ea',\n",
       "  '02',\n",
       "  'ef',\n",
       "  '34',\n",
       "  '38',\n",
       "  'd1',\n",
       "  '23',\n",
       "  'b6'],\n",
       " ['ec',\n",
       "  '81',\n",
       "  '0d',\n",
       "  'c4',\n",
       "  '20',\n",
       "  '16',\n",
       "  '12',\n",
       "  '25',\n",
       "  '0d',\n",
       "  '13',\n",
       "  'b4',\n",
       "  'f3',\n",
       "  '0e',\n",
       "  'f8',\n",
       "  'ce',\n",
       "  'ce',\n",
       "  'f5',\n",
       "  'd5'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Q_Mobilenet_model.block1[3].weights.shape)\n",
    "print(Q_Mobilenet_model.block1[3].weights)\n",
    "#input_or_weight_gen(q_input_activation['block1.0'])\n",
    "#bias_gen(Q_Mobilenet_model.block1[3].q_bias)\n",
    "input_or_weight_gen(Q_Mobilenet_model.block1[3].weights)\n",
    "#golden_gen(q_output_activation['conv1.0'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
