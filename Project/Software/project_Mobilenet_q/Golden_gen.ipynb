{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin199907/.conda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "from mobilenet_model.Q_layer import bn_folding_model, bn_folding, fold_conv_bn_eval\n",
    "from mobilenet_model.Q_layer import get_scale_and_zero_point, linear_quantize\n",
    "from mobilenet_model.Q_layer import quantized_linear, quantized_conv, do_requant, do_fake_quant,do_dequant\n",
    "from mobilenet_model.Q_layer import AVP_Fake_Quant,Q_SELayer_deq, Q_SELayer, QuantizedConv, QuantizedLinear, Preprocess, Quantizer\n",
    "\n",
    "from mobilenet_model.mobilenet_model import SELayer,h_swish,h_sigmoid\n",
    "from mobilenet_model.mobilenet_model import _make_divisible\n",
    "from mobilenet_model.mobilenet_model import Our_MobileNetV3,BN_fold_Our_MobileNetV3\n",
    "\n",
    "from mobilenet_model.golden_gen import bias_gen, signed_dec2hex_matrix, signed_dec2hex, golden_gen, input_or_weight_gen, DecToBin_machine, input_weight_gen_with_0x, \\\n",
    "                                        input_gen_with_0x_dw_ifmap, input_gen_with_0x_pw_ifmap, customize_dw_input_gen, pw_ip_test_weight_bias_file_gen, pw_ip_test_golden_file_gen\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN_fold_Our_MobileNetV3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Preprocess()\n",
      "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (2): Quantizer()\n",
      "  )\n",
      "  (block1): Sequential(\n",
      "    (0): QuantizedConv(in_channels=8, out_channels=8)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=8, out_channels=8)\n",
      "    (7): QuantizedConv(in_channels=8, out_channels=16)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): QuantizedConv(in_channels=16, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): QuantizedConv(in_channels=32, out_channels=64)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=64)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=64, out_channels=64)\n",
      "    (7): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): QuantizedConv(in_channels=48, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block6): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=96)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=96)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=96, out_channels=96)\n",
      "    (7): QuantizedConv(in_channels=96, out_channels=64)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (block7): Sequential(\n",
      "    (0): QuantizedConv(in_channels=64, out_channels=48)\n",
      "    (1): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Quantizer()\n",
      "    (3): QuantizedConv(in_channels=1, out_channels=48)\n",
      "    (4): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Quantizer()\n",
      "    (6): Quantized_SE(in_channels=48, out_channels=48)\n",
      "    (7): QuantizedConv(in_channels=48, out_channels=32)\n",
      "    (8): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Quantizer()\n",
      "  )\n",
      "  (avgpool): Sequential(\n",
      "    (0): AVP_Fake_Quant(\n",
      "      (avp): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_channels=32, out_channels=20)\n",
      "    (1): QuantizedLinear(in_channels=20, out_channels=10)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Q_Mobilenet_model = torch.load('Mobilenet_ckpt/Quantized_Mobilenet.pt',map_location=device)\n",
    "print(Q_Mobilenet_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "q_input_activation = {}\n",
    "q_output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        q_input_activation[module_name] = x.detach()\n",
    "        q_output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess,nn.AdaptiveAvgPool2d)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "\n",
    "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "hooks = add_range_recoder_hook(Q_Mobilenet_model)\n",
    "sample_data = iter(q_test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "Q_Mobilenet_model(sample_data) #Forward to use hook\n",
    "\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.009868 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  #set model to evaluate mode\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "      if use_gpu:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      pred = model(x)\n",
    "      test_loss = loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "test_loop(test_loader, Q_Mobilenet_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1.0', 'conv1.1', 'conv1.2', 'block1.0', 'block1.1', 'block1.2', 'block1.3', 'block1.4', 'block1.5', 'block1.6.avg_pool', 'block1.6.fc.0', 'block1.6.fc.1', 'block1.7', 'block1.8', 'block1.9', 'block2.0', 'block2.1', 'block2.2', 'block2.3', 'block2.4', 'block2.5', 'block2.6.avg_pool', 'block2.6.fc.0', 'block2.6.fc.1', 'block2.7', 'block2.8', 'block2.9', 'block3.0', 'block3.1', 'block3.2', 'block3.3', 'block3.4', 'block3.5', 'block3.6.avg_pool', 'block3.6.fc.0', 'block3.6.fc.1', 'block3.7', 'block3.8', 'block3.9', 'block4.0', 'block4.1', 'block4.2', 'block4.3', 'block4.4', 'block4.5', 'block4.6.avg_pool', 'block4.6.fc.0', 'block4.6.fc.1', 'block4.7', 'block4.8', 'block4.9', 'block5.0', 'block5.1', 'block5.2', 'block5.3', 'block5.4', 'block5.5', 'block5.6.avg_pool', 'block5.6.fc.0', 'block5.6.fc.1', 'block5.7', 'block5.8', 'block5.9', 'block6.0', 'block6.1', 'block6.2', 'block6.3', 'block6.4', 'block6.5', 'block6.6.avg_pool', 'block6.6.fc.0', 'block6.6.fc.1', 'block6.7', 'block6.8', 'block6.9', 'block7.0', 'block7.1', 'block7.2', 'block7.3', 'block7.4', 'block7.5', 'block7.6.avg_pool', 'block7.6.fc.0', 'block7.6.fc.1', 'block7.7', 'block7.8', 'block7.9', 'avgpool.0.avp', 'classifier.0', 'classifier.1'])\n"
     ]
    }
   ],
   "source": [
    "print(q_input_activation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte0: 0c 21 fe a2 17 f9 3c b4 f1 2b ab 65 2a 0a 01 ec\n",
      "=======\n",
      "byte1: ed 2a cc ea e5 fa 25 ad 17 ee 00 41 2d d8 04 c9\n",
      "=======\n",
      "byte2: 0e fd 7f 7a 27 dd c7 6c de 22 58 6a ff e6 51 ad\n",
      "=======\n",
      "byte3: 24 23 ae 9d 18 ea 58 16 23 1e ca 81 0a 10 1b 1d\n",
      "=======\n",
      "byte0: 00 00 00 00\n",
      "=======\n",
      "byte1: 00 00 00 00\n",
      "=======\n",
      "byte2: 01 00 01 00\n",
      "=======\n",
      "byte3: 00 00 00 00\n",
      "=======\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pw_ip_test_golden_file_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(Q_Mobilenet_model.block1[3].weights.shape)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#print(Q_Mobilenet_model.block1[0].weights)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(q_input_activation['block1.0'].shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# golden = golden_gen(q_output_activation['block1.2'])\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(len(byte0))\u001b[39;00m\n\u001b[1;32m     31\u001b[0m pw_ip_test_weight_bias_file_gen(Q_Mobilenet_model\u001b[38;5;241m.\u001b[39mblock1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweights,Q_Mobilenet_model\u001b[38;5;241m.\u001b[39mblock1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mpw_ip_test_golden_file_gen\u001b[49m(q_output_activation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock1.2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pw_ip_test_golden_file_gen' is not defined"
     ]
    }
   ],
   "source": [
    "# print(Q_Mobilenet_model.block1[3].weights.shape)\n",
    "#print(Q_Mobilenet_model.block1[0].weights)\n",
    "# print(q_input_activation['block1.0'].shape)\n",
    "# print(q_input_activation['block1.0'])\n",
    "#input_gen_with_0x_dw_ifmap(q_input_activation['block1.3'])\n",
    "##############\n",
    "#p2d = (1,1,1,1)\n",
    "#test = golden_layer_decimal = torch.nn.functional.pad(q_output_activation['conv1.0'],p2d,\"constant\",0)\n",
    "#print(test.shape)\n",
    "#print(test)\n",
    "\n",
    "#customize_dw_input_gen(test)\n",
    "####################\n",
    "#patches = np.array(patches)\n",
    "#print(patches.flatten())\n",
    "#byte0,byte1,byte2,byte3 = input_or_weight_gen(q_input_activation['block1.0'])\n",
    "#bias_gen(Q_Mobilenet_model.block1[3].q_bias)\n",
    "#input_with_0x = input_weight_gen_with_0x(q_input_activation['block1.0'])\n",
    "#input_with_0x = input_weight_gen_with_0x(Q_Mobilenet_model.block1[0].weights)\n",
    "# weight_byte0, weight_byte1, weight_byte2, weight_byte3 = input_or_weight_gen(Q_Mobilenet_model.block1[0].weights)\n",
    "# golden = golden_gen(q_output_activation['block1.2'])\n",
    "\n",
    "#print(q_input_activation['block1.0'].shape)\n",
    "#print(q_output_activation['block1.2'].shape)\n",
    "#input_pw = input_or_weight_gen(q_input_activation['block1.0'])\n",
    "#weight_pw = input_or_weight_gen(Q_Mobilenet_model.block1[0].weights)\n",
    "#print(Q_Mobilenet_model.block1[0].input_scale*Q_Mobilenet_model.block1[0].weight_scale*4096)\n",
    "#print(Q_Mobilenet_model.block1[0].weight_scale)\n",
    "# golden = golden_gen(q_output_activation['block1.2'])\n",
    "# print(len(byte0))\n",
    "pw_ip_test_weight_bias_file_gen(Q_Mobilenet_model.block1[0].weights,Q_Mobilenet_model.block1[0].bias)\n",
    "pw_ip_test_golden_file_gen(q_output_activation['block1.2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.flatten(q_output_activation['block4.0'])\n",
    "x = x.cpu()\n",
    "x = torch.flatten(x)\n",
    "x = x.detach()\n",
    "x = x.numpy()\n",
    "print(x.shape)\n",
    "\n",
    "plt.title(\"Quantized Mobilenet distribution.block1\")\n",
    "plt.hist(x, bins='auto',density=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ifmap ##\n",
    "f = open('data_and_golden\\main0.hex', 'w')\n",
    "for i,data in enumerate(byte0):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "##\n",
    "f = open('data_and_golden\\main1.hex', 'w')\n",
    "for i,data in enumerate(byte1):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "##\n",
    "f = open('data_and_golden\\main2.hex', 'w')\n",
    "for i,data in enumerate(byte2):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "##\n",
    "f = open('data_and_golden\\main3.hex', 'w')\n",
    "for i,data in enumerate(byte3):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "## weight ##\n",
    "\n",
    "f = open('data_and_golden\\weight0.hex', 'w')\n",
    "for i,data in enumerate(weight_byte0):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "##\n",
    "f = open('data_and_golden\\weight1.hex', 'w')\n",
    "for i,data in enumerate(weight_byte1):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "##\n",
    "f = open('data_and_golden\\weight2.hex', 'w')\n",
    "for i,data in enumerate(weight_byte2):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "##\n",
    "f = open('data_and_golden\\weight3.hex', 'w')\n",
    "for i,data in enumerate(weight_byte3):\n",
    "    f.write(data+\"\\n\")\n",
    "f.close()\n",
    "############\n",
    "\n",
    "############\n",
    "## Golden ##\n",
    "f = open('data_and_golden\\golden.hex', 'w')\n",
    "for i,data in enumerate(golden):\n",
    "    print(*data,sep='',file=f)\n",
    "f.close()\n",
    "\n",
    "## input gen with 0x 32bits ##\n",
    "f = open('data_and_golden\\data.hex', 'w')\n",
    "for i,data in enumerate(input_with_0x ):\n",
    "    print(*data ,sep='',file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
