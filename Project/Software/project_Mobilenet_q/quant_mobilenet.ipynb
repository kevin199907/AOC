{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sHXk_L8g3fDh"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tVH9mlCdXrkw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kevin199907/.conda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "\n",
        "\n",
        "no_cuda = False\n",
        "use_gpu = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "def bn_folding_model(model):\n",
        "\n",
        "    new_model = copy.deepcopy(model)\n",
        "\n",
        "    module_names = list(new_model._modules)\n",
        "\n",
        "    for k, name in enumerate(module_names):\n",
        "        print(k,name)\n",
        "        if len(list(new_model._modules[name]._modules)) > 0:\n",
        "            new_model._modules[name] = bn_folding_model(new_model._modules[name])\n",
        "            # print(k)\n",
        "            # print(\"-\"*5)\n",
        "            # print(name)\n",
        "            # print(\"-\"*5)\n",
        "            # print(new_model._modules[name]._modules)\n",
        "            \n",
        "        else:\n",
        "            if isinstance(new_model._modules[name], nn.BatchNorm2d):\n",
        "                if isinstance(new_model._modules[module_names[k-1]], nn.Conv2d):\n",
        "\n",
        "                    # Folded BN\n",
        "                    folded_conv = fold_conv_bn_eval(new_model._modules[module_names[k-1]], new_model._modules[name])\n",
        "\n",
        "                    # Replace old weight values\n",
        "                    new_model._modules.pop(name) # Remove the BN layer\n",
        "                    new_model._modules[module_names[k-1]] = folded_conv # Replace the Convolutional Layer by the folded version\n",
        "                    print(\"yo\")\n",
        "    return new_model\n",
        "\n",
        "\n",
        "\n",
        "def bn_folding(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n",
        "    if conv_b is None:\n",
        "        conv_b = bn_rm.new_zeros(bn_rm.shape)\n",
        "    bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps)\n",
        "    \n",
        "    w_fold = conv_w * (bn_w * bn_var_rsqrt).view(-1, 1, 1, 1)\n",
        "    b_fold = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n",
        "    \n",
        "    return torch.nn.Parameter(w_fold), torch.nn.Parameter(b_fold)\n",
        "\n",
        "\n",
        "def fold_conv_bn_eval(conv, bn):\n",
        "    assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n",
        "    fused_conv = copy.deepcopy(conv)\n",
        "\n",
        "    fused_conv.weight, fused_conv.bias = bn_folding(fused_conv.weight, fused_conv.bias,\n",
        "                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n",
        "\n",
        "    return fused_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) / 6\n",
        "\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
        "                h_sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        " \n",
        "\n",
        "def pw_conv(infp,outfp):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels=infp, out_channels=outfp, kernel_size=1, stride=1,padding= 0, bias=False)\n",
        "    )\n",
        "\n",
        "def dw_conv(infp,outfp,kernel_size,stride,padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(infp,outfp,kernel_size,stride=stride,padding=padding,groups=infp)\n",
        "    )\n",
        "\n",
        "class MobileNetV3_block(nn.Module):\n",
        "    def __init__(self, infp, outfp, middle_feature ,kernel_size,stride,padding):\n",
        "        super(MobileNetV3_block, self).__init__()\n",
        "\n",
        "        ############ My Design #########\n",
        "        #self.pw1 = pw_conv(infp,middle_feature)\n",
        "        self.pw1 = nn.Conv2d(in_channels=infp, out_channels=middle_feature, kernel_size=1, stride=1,padding= 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(middle_feature)\n",
        "        self.hs1 = h_swish()\n",
        "\n",
        "        self.dw1 = nn.Conv2d(middle_feature,middle_feature,kernel_size,stride=stride,padding=padding,groups=middle_feature)\n",
        "        #self.dw1 = dw_conv(middle_feature,middle_feature,kernel_size,stride,padding)\n",
        "        self.bn2 = nn.BatchNorm2d(middle_feature)\n",
        "        self.hs2 = h_swish()\n",
        "        self.se1 = SELayer(middle_feature)\n",
        "        self.hs3 = h_swish()\n",
        "        self.pw2 = nn.Conv2d(in_channels=middle_feature, out_channels=outfp, kernel_size=1, stride=1,padding= 0, bias=False)\n",
        "        #self.pw2 = pw_conv(middle_feature, outfp)\n",
        "        self.bn3 = nn.BatchNorm2d( outfp)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        out = self.pw1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.hs1(out)\n",
        "        out = self.dw1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.hs2(out)\n",
        "        out = self.se1(out)\n",
        "        out = self.hs3(out)\n",
        "        out = self.pw2(out)\n",
        "        out = self.bn3(out)\n",
        "        return out\n",
        "\n",
        "class Our_MobileNetV3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Our_MobileNetV3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1,padding=1)\n",
        "        self.block1 = MobileNetV3_block(infp = 8, outfp=16 , middle_feature=8 ,kernel_size=3,stride=2,padding=1)\n",
        "        self.block2 = MobileNetV3_block(infp = 16, outfp=32 , middle_feature=48 ,kernel_size=3,stride=2,padding=1)\n",
        "        self.block3 = MobileNetV3_block(infp = 32, outfp=32 , middle_feature=64 ,kernel_size=3,stride=2,padding=1)\n",
        "        self.block4 = MobileNetV3_block(infp = 32, outfp=48 , middle_feature=64 ,kernel_size=3,stride=2,padding=1)\n",
        "        self.block5 = MobileNetV3_block(infp = 48, outfp=64 , middle_feature=96 ,kernel_size=3,stride=2,padding=1)\n",
        "        self.block6 = MobileNetV3_block(infp = 64, outfp=64 , middle_feature=96 ,kernel_size=3,stride=2,padding=1)\n",
        "        self.block7 = MobileNetV3_block(infp = 64, outfp=32 , middle_feature=48 ,kernel_size=3,stride=2,padding=1)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.linear1 = nn.Linear(32,20)\n",
        "        self.swish = h_swish()\n",
        "        self.linear2 = nn.Linear(20,10)\n",
        "\n",
        "    \n",
        "    def forward(self,x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "        out = self.block5(out)\n",
        "        out = self.block6(out)\n",
        "        out = self.block7(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out = self.linear1(out)\n",
        "        out = self.swish(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk1U6PcMXtDB",
        "outputId": "58b165e2-e66b-41c5-987d-4fcc1b6a26fa"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#Dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "empty_net = []\n",
        "\n",
        "empty_net.append(nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1,padding=1))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JzMtMm9n3hsZ"
      },
      "source": [
        "Create NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-pKE7xJbOc7",
        "outputId": "fce122fd-5aa8-4d0c-fe2c-94bb60cc64ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MobileNetV3_block(\n",
            "  (pw1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (hs1): h_swish(\n",
            "    (sigmoid): h_sigmoid(\n",
            "      (relu): ReLU6(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (dw1): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
            "  (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (hs2): h_swish(\n",
            "    (sigmoid): h_sigmoid(\n",
            "      (relu): ReLU6(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (se1): SELayer(\n",
            "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=8, out_features=8, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=8, out_features=8, bias=True)\n",
            "      (3): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (hs3): h_swish(\n",
            "    (sigmoid): h_sigmoid(\n",
            "      (relu): ReLU6(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (pw2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "-----\n",
            "0 conv1\n",
            "1 block1\n",
            "0 pw1\n",
            "1 bn1\n",
            "yo\n",
            "2 hs1\n",
            "0 sigmoid\n",
            "0 relu\n",
            "3 dw1\n",
            "4 bn2\n",
            "yo\n",
            "5 hs2\n",
            "0 sigmoid\n",
            "0 relu\n",
            "6 se1\n",
            "0 avg_pool\n",
            "1 fc\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "0 relu\n",
            "7 hs3\n",
            "0 sigmoid\n",
            "0 relu\n",
            "8 pw2\n",
            "9 bn3\n",
            "yo\n",
            "2 block2\n",
            "0 pw1\n",
            "1 bn1\n",
            "yo\n",
            "2 hs1\n",
            "0 sigmoid\n",
            "0 relu\n",
            "3 dw1\n",
            "4 bn2\n",
            "yo\n",
            "5 hs2\n",
            "0 sigmoid\n",
            "0 relu\n",
            "6 se1\n",
            "0 avg_pool\n",
            "1 fc\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "0 relu\n",
            "7 hs3\n",
            "0 sigmoid\n",
            "0 relu\n",
            "8 pw2\n",
            "9 bn3\n",
            "yo\n",
            "3 block3\n",
            "0 pw1\n",
            "1 bn1\n",
            "yo\n",
            "2 hs1\n",
            "0 sigmoid\n",
            "0 relu\n",
            "3 dw1\n",
            "4 bn2\n",
            "yo\n",
            "5 hs2\n",
            "0 sigmoid\n",
            "0 relu\n",
            "6 se1\n",
            "0 avg_pool\n",
            "1 fc\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "0 relu\n",
            "7 hs3\n",
            "0 sigmoid\n",
            "0 relu\n",
            "8 pw2\n",
            "9 bn3\n",
            "yo\n",
            "4 block4\n",
            "0 pw1\n",
            "1 bn1\n",
            "yo\n",
            "2 hs1\n",
            "0 sigmoid\n",
            "0 relu\n",
            "3 dw1\n",
            "4 bn2\n",
            "yo\n",
            "5 hs2\n",
            "0 sigmoid\n",
            "0 relu\n",
            "6 se1\n",
            "0 avg_pool\n",
            "1 fc\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "0 relu\n",
            "7 hs3\n",
            "0 sigmoid\n",
            "0 relu\n",
            "8 pw2\n",
            "9 bn3\n",
            "yo\n",
            "5 block5\n",
            "0 pw1\n",
            "1 bn1\n",
            "yo\n",
            "2 hs1\n",
            "0 sigmoid\n",
            "0 relu\n",
            "3 dw1\n",
            "4 bn2\n",
            "yo\n",
            "5 hs2\n",
            "0 sigmoid\n",
            "0 relu\n",
            "6 se1\n",
            "0 avg_pool\n",
            "1 fc\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "0 relu\n",
            "7 hs3\n",
            "0 sigmoid\n",
            "0 relu\n",
            "8 pw2\n",
            "9 bn3\n",
            "yo\n",
            "6 block6\n",
            "0 pw1\n",
            "1 bn1\n",
            "yo\n",
            "2 hs1\n",
            "0 sigmoid\n",
            "0 relu\n",
            "3 dw1\n",
            "4 bn2\n",
            "yo\n",
            "5 hs2\n",
            "0 sigmoid\n",
            "0 relu\n",
            "6 se1\n",
            "0 avg_pool\n",
            "1 fc\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "0 relu\n",
            "7 hs3\n",
            "0 sigmoid\n",
            "0 relu\n",
            "8 pw2\n",
            "9 bn3\n",
            "yo\n",
            "7 block7\n",
            "0 pw1\n",
            "1 bn1\n",
            "yo\n",
            "2 hs1\n",
            "0 sigmoid\n",
            "0 relu\n",
            "3 dw1\n",
            "4 bn2\n",
            "yo\n",
            "5 hs2\n",
            "0 sigmoid\n",
            "0 relu\n",
            "6 se1\n",
            "0 avg_pool\n",
            "1 fc\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "0 relu\n",
            "7 hs3\n",
            "0 sigmoid\n",
            "0 relu\n",
            "8 pw2\n",
            "9 bn3\n",
            "yo\n",
            "8 avgpool\n",
            "9 linear1\n",
            "10 swish\n",
            "0 sigmoid\n",
            "0 relu\n",
            "11 linear2\n",
            "Our_MobileNetV3(\n",
            "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (block1): MobileNetV3_block(\n",
            "    (pw1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (hs1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (dw1): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
            "    (hs2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (3): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (hs3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pw2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (block2): MobileNetV3_block(\n",
            "    (pw1): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (hs1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
            "    (hs2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=48, out_features=16, bias=True)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=16, out_features=48, bias=True)\n",
            "        (3): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (hs3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (block3): MobileNetV3_block(\n",
            "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (hs1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
            "    (hs2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=16, bias=True)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=16, out_features=64, bias=True)\n",
            "        (3): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (hs3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pw2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (block4): MobileNetV3_block(\n",
            "    (pw1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (hs1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (dw1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
            "    (hs2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=16, bias=True)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=16, out_features=64, bias=True)\n",
            "        (3): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (hs3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pw2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (block5): MobileNetV3_block(\n",
            "    (pw1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (hs1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
            "    (hs2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=96, out_features=24, bias=True)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=24, out_features=96, bias=True)\n",
            "        (3): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (hs3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (block6): MobileNetV3_block(\n",
            "    (pw1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (hs1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
            "    (hs2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=96, out_features=24, bias=True)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=24, out_features=96, bias=True)\n",
            "        (3): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (hs3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pw2): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (block7): MobileNetV3_block(\n",
            "    (pw1): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (hs1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (dw1): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
            "    (hs2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=48, out_features=16, bias=True)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=16, out_features=48, bias=True)\n",
            "        (3): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (hs3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pw2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (linear1): Linear(in_features=32, out_features=20, bias=True)\n",
            "  (swish): h_swish(\n",
            "    (sigmoid): h_sigmoid(\n",
            "      (relu): ReLU6(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (linear2): Linear(in_features=20, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "trained_net = Our_MobileNetV3().cuda()\n",
        "\n",
        "input_shape = torch.randn(1,1,28,28).cuda()\n",
        "trained_net_weight = torch.load(\"N26122246_minist_best.ckpt\")\n",
        "trained_net.load_state_dict(trained_net_weight,strict=False)\n",
        "trained_net.eval()\n",
        "\n",
        "# print(trained_net.block1)\n",
        "# cur_block = getattr(train_net, 'block\"+str(i))\n",
        "#print(trained_net.block1)\n",
        "print(trained_net.block1)\n",
        "print(\"-\"*5)\n",
        "BN_fold_trained_net=   bn_folding_model(trained_net)\n",
        "print(BN_fold_block1)\n",
        "# print(trained_net)\n",
        "# print(\"-\"*5)\n",
        "#print(BN_fold_trained_net)\n",
        "# print(trained_net_weight.keys())\n",
        "\n",
        "# print(trained_net_weight['block1.bn1.weight'])\n",
        "\n",
        "FP32_model = Our_MobileNetV3()\n",
        "#print(FP32_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MOyeqSPDbvr5"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  #Set the model to train mode\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    if use_gpu:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    #forward\n",
        "    pred = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #backward\n",
        "    loss.backward()\n",
        "\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  #set model to evaluate mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      if use_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "      pred = model(x)\n",
        "      test_loss = loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LsfzIw4b1AU",
        "outputId": "03aeb700-d0fe-4e88-a24e-d000b63c4184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Our_MobileNetV3(\n",
              "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (block1): MobileNetV3_block(\n",
              "    (pw1): Sequential(\n",
              "      (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (dw1): Sequential(\n",
              "      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs2): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (se1): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=8, out_features=8, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
              "        (3): h_sigmoid(\n",
              "          (relu): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (hs3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (pw2): Sequential(\n",
              "      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block2): MobileNetV3_block(\n",
              "    (pw1): Sequential(\n",
              "      (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (dw1): Sequential(\n",
              "      (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs2): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (se1): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=48, out_features=16, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=16, out_features=48, bias=True)\n",
              "        (3): h_sigmoid(\n",
              "          (relu): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (hs3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (pw2): Sequential(\n",
              "      (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block3): MobileNetV3_block(\n",
              "    (pw1): Sequential(\n",
              "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (dw1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs2): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (se1): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=16, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=16, out_features=64, bias=True)\n",
              "        (3): h_sigmoid(\n",
              "          (relu): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (hs3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (pw2): Sequential(\n",
              "      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block4): MobileNetV3_block(\n",
              "    (pw1): Sequential(\n",
              "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (dw1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs2): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (se1): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=16, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=16, out_features=64, bias=True)\n",
              "        (3): h_sigmoid(\n",
              "          (relu): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (hs3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (pw2): Sequential(\n",
              "      (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block5): MobileNetV3_block(\n",
              "    (pw1): Sequential(\n",
              "      (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (dw1): Sequential(\n",
              "      (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs2): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (se1): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=96, out_features=24, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=24, out_features=96, bias=True)\n",
              "        (3): h_sigmoid(\n",
              "          (relu): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (hs3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (pw2): Sequential(\n",
              "      (0): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block6): MobileNetV3_block(\n",
              "    (pw1): Sequential(\n",
              "      (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (dw1): Sequential(\n",
              "      (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs2): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (se1): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=96, out_features=24, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=24, out_features=96, bias=True)\n",
              "        (3): h_sigmoid(\n",
              "          (relu): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (hs3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (pw2): Sequential(\n",
              "      (0): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (block7): MobileNetV3_block(\n",
              "    (pw1): Sequential(\n",
              "      (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (dw1): Sequential(\n",
              "      (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (hs2): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (se1): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=48, out_features=16, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=16, out_features=48, bias=True)\n",
              "        (3): h_sigmoid(\n",
              "          (relu): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (hs3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (pw2): Sequential(\n",
              "      (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (linear1): Linear(in_features=32, out_features=20, bias=True)\n",
              "  (swish): h_swish(\n",
              "    (sigmoid): h_sigmoid(\n",
              "      (relu): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (linear2): Linear(in_features=20, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 3\n",
        "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
        "optimizer = torch.optim.SGD(FP32_model.parameters(), lr=learning_rate, momentum=0.9)  #define optimizer\n",
        "\n",
        "FP32_model.to(device) #let model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LH6kt0eqb9tl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.360337  [   32/60000]\n",
            "loss: 2.295292  [ 3232/60000]\n",
            "loss: 2.213784  [ 6432/60000]\n",
            "loss: 2.118740  [ 9632/60000]\n",
            "loss: 2.020132  [12832/60000]\n",
            "loss: 1.875932  [16032/60000]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m   train_loop(train_loader, FP32_model, loss_fn, optimizer)\n\u001b[1;32m      5\u001b[0m   test_loop(test_loader, FP32_model, loss_fn)\n",
            "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m \u001b[39m#optimize\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m   loss, current \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem(), (batch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(x)\n",
            "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/optim/sgd.py:144\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m             momentum_buffer_list\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmomentum_buffer\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 144\u001b[0m F\u001b[39m.\u001b[39;49msgd(params_with_grad,\n\u001b[1;32m    145\u001b[0m       d_p_list,\n\u001b[1;32m    146\u001b[0m       momentum_buffer_list,\n\u001b[1;32m    147\u001b[0m       weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    148\u001b[0m       momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    149\u001b[0m       lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    150\u001b[0m       dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[1;32m    151\u001b[0m       nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[1;32m    152\u001b[0m       maximize\u001b[39m=\u001b[39;49mmaximize,)\n\u001b[1;32m    154\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
            "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/optim/_functional.py:186\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    184\u001b[0m     momentum_buffer_list[i] \u001b[39m=\u001b[39m buf\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     buf\u001b[39m.\u001b[39;49mmul_(momentum)\u001b[39m.\u001b[39;49madd_(d_p, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m dampening)\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m nesterov:\n\u001b[1;32m    189\u001b[0m     d_p \u001b[39m=\u001b[39m d_p\u001b[39m.\u001b[39madd(buf, alpha\u001b[39m=\u001b[39mmomentum)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Training\n",
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, FP32_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAK3-fH3qGh"
      },
      "source": [
        "# Quantization definition"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0lWzxrde4G5i"
      },
      "source": [
        "####Question 1.####\n",
        "\n",
        "Use\n",
        ">$S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})$\n",
        "\n",
        ">$Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S$\n",
        "\n",
        "to calculate scale factor and zero point of a tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJIr-5SpcgQr"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero_point(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -2**(bitwidth-1), 2**(bitwidth-1) - 1\n",
        "  fp_min = fp32_tensor.min().item()\n",
        "  fp_max = fp32_tensor.max().item()\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  scale = (fp_max-fp_min) / (q_max-q_min)\n",
        "  zero_point = q_min-fp_min /scale\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "\n",
        "  zero_point = round(zero_point)          #round\n",
        "  zero_point = max(q_min, min(zero_point, q_max)) #clip\n",
        "\n",
        "  return scale, int(zero_point)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YA7ano5nS1"
      },
      "source": [
        "####Question 2.####\n",
        "\n",
        "Use $q=r/S + Z$ to quantize a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBMKB5Le54wr"
      },
      "outputs": [],
      "source": [
        "def linear_quantize(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -2**(bitwidth-1), 2**(bitwidth-1) - 1\n",
        "\n",
        "  scale, zero_point = get_scale_and_zero_point(fp32_tensor)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  q_tensor = torch.round( fp32_tensor/scale ) +zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp\n",
        "  q_tensor = torch.clamp(q_tensor, q_min, q_max)\n",
        "  return q_tensor, scale, zero_point"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0taJXSmz6KDS"
      },
      "source": [
        "####Question 3.####\n",
        "\n",
        "Use\n",
        "> $q_{\\mathrm{output}} = M * \\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Z_{\\mathrm{output}}$\n",
        "\n",
        "> $M = S_{\\mathrm{input}} * S_{\\mathrm{weight}} / S_{\\mathrm{output}}$\n",
        "\n",
        "to compute quantized linear operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbXY0vaCcn7l"
      },
      "outputs": [],
      "source": [
        "def quantized_conv(input, weights, stride, padding,groups,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=8):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale / output_scale\n",
        "  output = torch.nn.functional.conv2d((input - input_zero_point ), (weights - weight_zero_point),stride=stride,padding=padding,groups=groups)\n",
        "  output *= M\n",
        "  output += output_zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.round().clamp(-2**(activation_bitwidth-1), 2**(activation_bitwidth-1)-1)\n",
        "\n",
        "  return output\n",
        "\n",
        "def quantized_linear(input, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=8):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale / output_scale\n",
        "  output = torch.nn.functional.linear((input - input_zero_point ), (weights - weight_zero_point))\n",
        "  output *= M\n",
        "  output += output_zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.round().clamp(-2**(activation_bitwidth-1), 2**(activation_bitwidth-1)-1)\n",
        "\n",
        "  return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vK10k10R7II7"
      },
      "source": [
        "# Design quantized linear layer and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwrXNVKadKfG"
      },
      "outputs": [],
      "source": [
        "class QuantizedConv(nn.Module):\n",
        "  def __init__(self, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "\n",
        "  def forward(self, x):\n",
        "    return quantized_linear(x, self.weights, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedConv(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "  def __init__(self, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "\n",
        "  def forward(self, x):\n",
        "    return quantized_linear(x, self.weights, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedLinear(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "#Transform input data to correct integer range\n",
        "class Preprocess(nn.Module):\n",
        "  def __init__(self, input_scale, input_zero_point, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "  def forward(self, x):\n",
        "    x = x / self.input_scale + self.input_zero_point\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EUpiPDiu7RCH"
      },
      "source": [
        "# Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBdXFnr5dZqT"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'FP32_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     16\u001b[0m             all_hooks\u001b[39m.\u001b[39mappend(m\u001b[39m.\u001b[39mregister_forward_hook(\n\u001b[0;32m     17\u001b[0m                 functools\u001b[39m.\u001b[39mpartial(_record_range, module_name\u001b[39m=\u001b[39mname)))\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m all_hooks\n\u001b[1;32m---> 21\u001b[0m hooks \u001b[39m=\u001b[39m add_range_recoder_hook(FP32_model)\n\u001b[0;32m     22\u001b[0m sample_data \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\u001b[39m.\u001b[39m\u001b[39m__next__\u001b[39m()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device) \u001b[39m#Use a batch of training data to calibrate\u001b[39;00m\n\u001b[0;32m     23\u001b[0m FP32_model(sample_data) \u001b[39m#Forward to use hook\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'FP32_model' is not defined"
          ]
        }
      ],
      "source": [
        "# add hook to record the min max value of the activation\n",
        "input_activation = {}\n",
        "output_activation = {}\n",
        "\n",
        "#Define a hook to record the feature map of each layer\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        input_activation[module_name] = x.detach()\n",
        "        output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (nn.Linear, nn.ReLU)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "\n",
        "    return all_hooks\n",
        "\n",
        "hooks = add_range_recoder_hook(FP32_model)\n",
        "sample_data = iter(train_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
        "FP32_model(sample_data) #Forward to use hook\n",
        "print(input_activation.values())\n",
        "print(\"-\"*5)\n",
        "print(output_activation.values())\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YVf8vpiVTsDa"
      },
      "source": [
        "# Quantize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVh-SRj8eOrs"
      },
      "outputs": [],
      "source": [
        "#copy original model\n",
        "quantized_model = copy.deepcopy(FP32_model)\n",
        "\n",
        "#Record each layer in original model\n",
        "quantized_backbone = []\n",
        "i = 0\n",
        "\n",
        "#Record input scale and zero point\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"backbone.0\"])\n",
        "preprocess = Preprocess(input_scale, input_zero_point)\n",
        "quantized_backbone.append(preprocess)\n",
        "\n",
        "#Record Linear + ReLU of the model (except the last Linear)\n",
        "while i < len(quantized_model.backbone) - 1:\n",
        "  if isinstance(quantized_model.backbone[i], nn.Linear) and isinstance(quantized_model.backbone[i+1], nn.ReLU):\n",
        "    linear = quantized_model.backbone[i]\n",
        "    linear_name = f\"backbone.{i}\"\n",
        "    relu = quantized_model.backbone[i + 1]\n",
        "    relu_name = f\"backbone.{i + 1}\"\n",
        "\n",
        "    #Use the calibration data to calculate scale and zero point of each layer\n",
        "    input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "    output_scale, output_zero_point = get_scale_and_zero_point(output_activation[relu_name])\n",
        "    quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "\n",
        "    quantizedLinear = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "\n",
        "    quantized_backbone.append(quantizedLinear)\n",
        "    i += 2\n",
        "\n",
        "#Record the last Linear layer\n",
        "linear = quantized_model.backbone[4]\n",
        "linear_name = f\"backbone.4\"\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation[linear_name])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "quantizedLinear = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "quantized_backbone.append(quantizedLinear)\n",
        "\n",
        "\n",
        "quantized_model.backbone = nn.Sequential(*quantized_backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRB96PKbfNX4",
        "outputId": "58ce882a-3521-4b40-ff98-44dcea373546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (backbone): Sequential(\n",
            "    (0): Preprocess()\n",
            "    (1): QuantizedLinear(in_channels=784, out_channels=120)\n",
            "    (2): QuantizedLinear(in_channels=120, out_channels=84)\n",
            "    (3): QuantizedLinear(in_channels=84, out_channels=10)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(quantized_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BMccqTL6URaZ"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRMXzNeCgDuq",
        "outputId": "0fa65b9f-3a60-41e0-a935-a601a8db0484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.000704 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knTzO1mbheBe",
        "outputId": "411fc256-b7ea-4757-9f75-6acaae605324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.002995 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, quantized_model, loss_fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
