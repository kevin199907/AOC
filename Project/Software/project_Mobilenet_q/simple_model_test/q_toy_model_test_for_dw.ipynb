{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin199907/.conda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "    \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8),bias=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel,bias=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.Conv = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=1, out_channels=8, kernel_size=1, stride=1,padding= 0, bias=True),\n",
    "      h_swish(inplace=True),\n",
    "      nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=3,padding= 1, bias=True,groups=8),\n",
    "      h_swish(inplace=True),\n",
    "      SELayer(8),\n",
    "      nn.Conv2d(in_channels=8, out_channels=1, kernel_size=1, stride=1,padding= 0, bias=True),\n",
    "      h_swish(inplace=True)\n",
    "    )  \n",
    "    self.backbone = nn.Sequential(\n",
    "      nn.Linear(10*10, 120, bias=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(120, 84, bias=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(84, 10, bias=False)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x=self.Conv(x)\n",
    "    x = x.view(-1, 10*10) #transform 28*28 figure to 784 vector\n",
    "    x = self.backbone(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Q_SELayer(nn.Module):\n",
    "  def __init__(self,weights1,input_scale1,weight_scale1,output_scale1, input_zero_point1, weight_zero_point1, output_zero_point1,\n",
    "               weights2, input_scale2, weight_scale2, output_scale2, input_zero_point2, weight_zero_point2, output_zero_point2,\n",
    "               input_SE_scale,in_SE_zero_point,\n",
    "               output_SE_scale,output_SE_zero_point,\n",
    "               out_pool_scale,out_pool_zero_point):\n",
    "    super().__init__()\n",
    "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    self.fc = nn.Sequential(\n",
    "        QuantizedLinear(weights1,input_scale1,weight_scale1,output_scale1, input_zero_point1, weight_zero_point1, output_zero_point1),\n",
    "        QuantizedLinear(weights2, input_scale2, weight_scale2, output_scale2, input_zero_point2, weight_zero_point2, output_zero_point2)\n",
    "    )   \n",
    "    self.input_SE_scale, self.in_SE_zero_point = input_SE_scale,in_SE_zero_point\n",
    "    self.output_SE_scale, self.output_SE_zero_point = output_SE_scale, output_SE_zero_point,\n",
    "    self.out_pool_scale, self.out_pool_zero_point = out_pool_scale,out_pool_zero_point\n",
    "    self.linear_out_scale, self.linear_out_zero_point = output_scale2, output_zero_point2\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    b, c, _, _ = x.size()\n",
    "    y = self.avg_pool(x).view(b, c)\n",
    "    y = (y- self.in_SE_zero_point) \n",
    "    y = do_fake_quant(y, self.input_SE_scale, self.out_pool_scale, self.out_pool_zero_point,bitwidth=8)\n",
    "    y = self.fc(y).view(b, c, 1, 1)\n",
    "    z = (x-self.in_SE_zero_point)*(y-self.linear_out_zero_point)\n",
    "    return do_fake_quant(z, deq_scale=(self.input_SE_scale *self.linear_out_scale), \n",
    "                         q_scale=self.output_SE_scale, q_zero_point=self.output_SE_zero_point,bitwidth=8)\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"Quantized_SE(in_channels={self.fc[0].weights.size(1)}, out_channels={self.fc[1].weights.size(0)})\"\n",
    "    \n",
    "\n",
    "\n",
    "class QuantizedConv(nn.Module):\n",
    "  def __init__(self,bias ,weights,stride,padding,groups ,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
    "    super().__init__()\n",
    "    self.stride, self.padding, self.groups = stride, padding,groups\n",
    "    self.weights = weights\n",
    "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
    "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
    "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
    "    self.bias = bias\n",
    "    self.bitwidth = bitwidth\n",
    "    self.activation_bitwidth = activation_bitwidth\n",
    "    self.q_bias = torch.round(bias / (input_scale*weight_scale))\n",
    "    self.q_weight = weights - weight_zero_point\n",
    "    self.DeQ_scale = input_scale*weight_scale*8192\n",
    "  def forward(self, x):\n",
    "    return quantized_conv(x, self.bias, self.weights, self.stride, self.padding, self.groups, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
    "  def __repr__(self):\n",
    "    return f\"QuantizedConv(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "  def __init__(self, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
    "    super().__init__()\n",
    "    self.weights = weights\n",
    "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
    "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
    "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
    "\n",
    "    self.bitwidth = bitwidth\n",
    "    self.activation_bitwidth = activation_bitwidth\n",
    "\n",
    "  def forward(self, x):\n",
    "    return quantized_linear(x, self.weights, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
    "  def __repr__(self):\n",
    "    return f\"QuantizedLinear(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
    "\n",
    "#Transform input data to correct integer range\n",
    "class Preprocess(nn.Module):\n",
    "  def __init__(self, input_scale, input_zero_point, activation_bitwidth=8):\n",
    "    super().__init__()\n",
    "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
    "    self.activation_bitwidth = activation_bitwidth\n",
    "  def forward(self, x):\n",
    "    x = x / self.input_scale + self.input_zero_point\n",
    "    x = x.round() \n",
    "    return x\n",
    "  \n",
    "class Quantizer(nn.Module):\n",
    "  def __init__(self,scale,zero_point,bitwidth=8):\n",
    "    super().__init__()\n",
    "    self.scale = scale\n",
    "    self.zero = zero_point\n",
    "    self.store_scale = scale *64\n",
    "\n",
    "  def forward(self,x):\n",
    "    return do_requant(x,self.scale,self.zero)\n",
    "    \n",
    "    \n",
    "\n",
    "def quantized_linear(input, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=8):\n",
    "  input, weights = input.to(device), weights.to(device)\n",
    "\n",
    "  #####################################################\n",
    "\n",
    "  M = input_scale * weight_scale / output_scale\n",
    "  output = torch.nn.functional.linear((input - input_zero_point ), (weights - weight_zero_point))\n",
    "  output *= M\n",
    "  output += output_zero_point\n",
    "\n",
    "  #####################################################\n",
    "\n",
    "  #clamp and round\n",
    "  output = output.round().clamp(-(2**(activation_bitwidth-1)), 2**(activation_bitwidth-1)-1)\n",
    "\n",
    "  return output\n",
    "\n",
    "def quantized_conv(input, bias,weights,stride, padding,groups,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=16):\n",
    "  input, weights = input.to(device), weights.to(device)\n",
    "\n",
    "  #####################################################\n",
    "\n",
    "  M = input_scale * weight_scale \n",
    "  conv_bias = bias /M\n",
    "  conv_bias = conv_bias.round()\n",
    "  output_only_conv = torch.nn.functional.conv2d((input - input_zero_point ), (weights - weight_zero_point),bias = conv_bias ,stride=stride,padding=padding,groups=groups)\n",
    "  output = M * output_only_conv\n",
    "  #output += output_zero_point\n",
    "  #####################################################\n",
    "\n",
    "  #clamp and round\n",
    "  output = output.clamp(-(2**(activation_bitwidth-1)), 2**(activation_bitwidth-1)-1)\n",
    "  return output\n",
    "\n",
    "def do_requant(input, scale,zero_point,bitwidth=8):\n",
    "    output = input / scale\n",
    "    output = output.round()\n",
    "    output += zero_point\n",
    "    output = output.round().clamp(-(2**(bitwidth-1)), 2**(bitwidth-1)-1)\n",
    "    return output\n",
    "\n",
    "def do_fake_quant(input, deq_scale, q_scale, q_zero_point,bitwidth=8):\n",
    "    M = deq_scale/q_scale\n",
    "    N = q_zero_point\n",
    "    output = input * M\n",
    "    output += N\n",
    "    output = output.round().clamp(-(2**(bitwidth-1)), 2**(bitwidth-1)-1)\n",
    "    return output\n",
    "\n",
    "def signed_dec2hex_matrix_16b(input):\n",
    "    '''Convert a matrix which data is signed decimal to 8 bits hex with 2's complement'''\n",
    "    temp = []\n",
    "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(16)] ) )\n",
    "    for i in input:\n",
    "        test =bin8(i)\n",
    "        test = int(test,base=2)\n",
    "        hex_test = hex(test)[2:].zfill(2)\n",
    "        temp.append(hex_test)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def signed_dec2hex_16b(input):\n",
    "    '''Convert a number which data is signed decimal to 8 bits hex with 2's complement'''\n",
    "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(16)] ) )\n",
    "    test =bin8(input)\n",
    "    test = int(test,base=2)\n",
    "    hex_test = hex(test)[2:].zfill(2)\n",
    "\n",
    "    return hex_test\n",
    "\n",
    "\n",
    "def signed_dec2hex_matrix(input):\n",
    "    '''Convert a matrix which data is signed decimal to 8 bits hex with 2's complement'''\n",
    "    temp = []\n",
    "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
    "    for i in input:\n",
    "        test =bin8(i)\n",
    "        test = int(test,base=2)\n",
    "        hex_test = hex(test)[2:].zfill(2)\n",
    "        temp.append(hex_test)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def signed_dec2hex(input):\n",
    "    '''Convert a number which data is signed decimal to 8 bits hex with 2's complement'''\n",
    "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
    "    test =bin8(input)\n",
    "    test = int(test,base=2)\n",
    "    hex_test = hex(test)[2:].zfill(2)\n",
    "\n",
    "    return hex_test\n",
    "\n",
    "\n",
    "def golden_gen(golden_layer_decimal):\n",
    "    '''Convert a layer output which is signed decimal in GPU to 8 bits hex with 2's complement in CPU and\n",
    "     make it 4 element in a line, example of use : golden_gen(q_output_activation[\"Conv.3\"]) '''\n",
    "    golden = []\n",
    "    i=0\n",
    "    golden_in_numpy = golden_layer_decimal.cpu().numpy()\n",
    "    test = golden_in_numpy.flatten()\n",
    "    test =test.astype('int32')\n",
    "    golden.append([])\n",
    "    for j, data in enumerate(test):\n",
    "        if(j%4==0 ):\n",
    "            golden.append([])\n",
    "            i = i+1\n",
    "            golden[i].append(signed_dec2hex(data))\n",
    "        if(j%4!=0):\n",
    "            golden[i].append(signed_dec2hex(data))\n",
    "    golden.pop(0)\n",
    "    for indice,data in enumerate(golden):\n",
    "        print(*data,sep='')\n",
    "\n",
    "def input_or_weight_gen(layer_decimal):\n",
    "    '''Convert a layer output which is signed decimal in GPU to 8 bits hex with 2's complement in CPU and\n",
    "     make each byte display in different place, example of use : input_or_weight_gen(quantized_model.Conv[1].weights)'''\n",
    "    byte0 = []\n",
    "    byte1 = []\n",
    "    byte2 = []\n",
    "    byte3 = []\n",
    "\n",
    "    data_in_numpy = layer_decimal.cpu().numpy()\n",
    "    data_test = data_in_numpy.flatten()\n",
    "    data_test = data_test.astype('int32')\n",
    "    data_test = signed_dec2hex_matrix(data_test)\n",
    "    for indice,data in enumerate(data_test):\n",
    "        if(indice%4 == 0):\n",
    "            byte0.append(data)\n",
    "        elif(indice%4 == 1):\n",
    "            byte1.append(data)\n",
    "        elif(indice%4 == 2):\n",
    "            byte2.append(data)\n",
    "        else:\n",
    "            byte3.append(data)\n",
    "    print(\"byte0:\",*byte0)\n",
    "    print(\"=======\")\n",
    "    print(\"byte1:\",*byte1)\n",
    "    print(\"=======\")\n",
    "    print(\"byte2:\",*byte2)\n",
    "    print(\"=======\")\n",
    "    print(\"byte3:\",*byte3)\n",
    "    print(\"=======\")\n",
    "    return byte0,byte1,byte2,byte3\n",
    "\n",
    "\n",
    "def bias_gen(layer_decimal):\n",
    "    byte0 = []\n",
    "    byte1 = []\n",
    "    byte2 = []\n",
    "    byte3 = []\n",
    "    temp1 = []\n",
    "\n",
    "    data_in_numpy = layer_decimal.cpu().numpy()\n",
    "    data_test = data_in_numpy.flatten()\n",
    "    data_test = data_test.astype('int32')\n",
    "    data_test = signed_dec2hex_matrix_16b(data_test)\n",
    "    for indice,data in enumerate(data_test):\n",
    "        if(indice%2 == 0):\n",
    "            if(len(data)==1):\n",
    "                byte1.append('00')\n",
    "                byte0.append('0'+str(data))\n",
    "            elif(len(data)==2):\n",
    "                byte1.append('00')\n",
    "                byte0.append(data)\n",
    "            elif(len(data)==3):\n",
    "                temp = data[2]\n",
    "                byte1.append('0'+data[0])\n",
    "                byte0.append(data[1]+data[2])\n",
    "            elif(len(data)==4):\n",
    "                byte1.append(data[0:2])\n",
    "                byte0.append(data[2:4])\n",
    "        elif(indice%2 == 1):\n",
    "            if(len(data)==1):\n",
    "                byte3.append('00')\n",
    "                byte2.append('0'+str(data))\n",
    "            elif(len(data)==2):\n",
    "                byte3.append('00')\n",
    "                byte2.append(data)\n",
    "            elif(len(data)==3):\n",
    "                byte3.append('0'+data[0])\n",
    "                byte2.append(data[1]+data[2])\n",
    "            elif(len(data)==4):\n",
    "                byte3.append(data[0:2])\n",
    "                byte2.append(data[2:4])\n",
    "\n",
    "    print(\"byte0:\",*byte0)\n",
    "    print(\"=======\")\n",
    "    print(\"byte1:\",*byte1)\n",
    "    print(\"=======\")\n",
    "    print(\"byte2:\",*byte2)\n",
    "    print(\"=======\")\n",
    "    print(\"byte3:\",*byte3)\n",
    "    print(\"=======\")\n",
    "    return byte0,byte1,byte2,byte3\n",
    "\n",
    "def DecToBin_machine(num,accuracy):\n",
    "    integer = int(num)\n",
    "    flo = num - integer\n",
    "    integercom = '{:1b}'.format(integer)\n",
    "    tem = flo\n",
    "    flo_list = []\n",
    "    for i in range(accuracy):\n",
    "        tem *= 2\n",
    "        flo_list += str(int(tem))\n",
    "        tem -= int(tem)\n",
    "    flocom = flo_list\n",
    "    binary_value =  ''.join(flocom)\n",
    "    return binary_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyModel(\n",
      "  (Conv): Sequential(\n",
      "    (0): Preprocess()\n",
      "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (2): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Quantizer()\n",
      "    (4): QuantizedConv(in_channels=1, out_channels=8)\n",
      "    (5): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Quantizer()\n",
      "    (7): Quantized_SE(in_channels=8, out_channels=8)\n",
      "    (8): QuantizedConv(in_channels=8, out_channels=1)\n",
      "    (9): h_swish(\n",
      "      (sigmoid): h_sigmoid(\n",
      "        (relu): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (10): Quantizer()\n",
      "  )\n",
      "  (backbone): Sequential(\n",
      "    (0): QuantizedLinear(in_channels=100, out_channels=120)\n",
      "    (1): QuantizedLinear(in_channels=120, out_channels=84)\n",
      "    (2): QuantizedLinear(in_channels=84, out_channels=10)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "model = torch.load('dw_customize_Toymodel.pt',map_location=device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Conv.0', 'Conv.1', 'Conv.2', 'Conv.3', 'Conv.4', 'Conv.5', 'Conv.6', 'Conv.7.fc.0', 'Conv.7.fc.1', 'Conv.8', 'Conv.9', 'Conv.10', 'backbone.0', 'backbone.1', 'backbone.2'])\n",
      "tensor([[[[ 0.8789,  1.0423,  1.0423,  1.0423,  1.0423,  1.0423,  1.0423,\n",
      "            1.0423,  1.0423,  0.7999],\n",
      "          [ 1.0275,  1.2699,  1.2699,  1.2699,  1.2699,  1.2699,  1.2699,\n",
      "            1.2699,  1.2699,  0.9566],\n",
      "          [ 1.0275,  1.2699,  1.2699,  1.2699,  1.2699,  1.2699,  1.2737,\n",
      "            1.2699,  1.3247,  0.9566],\n",
      "          [ 1.0275,  1.2699,  1.2699,  1.2699,  1.2775,  1.5530,  1.8345,\n",
      "            1.5799,  1.8884,  0.9566],\n",
      "          [ 1.0275,  1.2699,  1.2699,  1.2759,  1.3734,  1.9976,  2.1995,\n",
      "            2.2363,  2.2052,  1.0114],\n",
      "          [ 1.0293,  1.2756,  1.3058,  1.5277,  1.9682,  2.0979,  2.2072,\n",
      "            2.2172,  2.1976,  1.2654],\n",
      "          [ 1.2953,  1.9075,  2.0010,  2.0755,  2.1033,  2.1709,  2.2029,\n",
      "            2.2445,  2.2442,  1.3973],\n",
      "          [ 1.1181,  1.6519,  1.9033,  1.8993,  1.9205,  1.7379,  1.5613,\n",
      "            1.9444,  1.9295,  1.1917],\n",
      "          [ 1.0275,  1.2699,  1.2699,  1.2699,  1.2699,  1.2699,  1.2699,\n",
      "            1.2699,  1.2699,  0.9566],\n",
      "          [ 0.8548,  1.0021,  1.0021,  1.0021,  1.0021,  1.0021,  1.0021,\n",
      "            1.0021,  1.0021,  0.8066]],\n",
      "\n",
      "         [[ 0.9200,  1.0469,  1.0469,  1.0469,  1.0469,  1.0469,  1.0469,\n",
      "            1.0469,  1.0469,  0.7023],\n",
      "          [ 1.0076,  1.2011,  1.2011,  1.2011,  1.2011,  1.2011,  1.2011,\n",
      "            1.2011,  1.2011,  0.7325],\n",
      "          [ 1.0076,  1.2011,  1.2011,  1.2011,  1.2011,  1.2011,  1.2011,\n",
      "            1.2011,  1.1942,  0.7325],\n",
      "          [ 1.0076,  1.2011,  1.2011,  1.2011,  1.2011,  1.1706,  1.1392,\n",
      "            1.1538,  1.1161,  0.7325],\n",
      "          [ 1.0076,  1.2011,  1.2011,  1.2011,  1.1918,  1.1329,  1.0893,\n",
      "            1.0777,  1.0904,  0.7284],\n",
      "          [ 1.0076,  1.2011,  1.1988,  1.1748,  1.1318,  1.1133,  1.0905,\n",
      "            1.0874,  1.0937,  0.7070],\n",
      "          [ 0.9734,  1.1400,  1.1330,  1.1243,  1.1205,  1.0977,  1.0867,\n",
      "            1.0712,  1.0686,  0.6937],\n",
      "          [ 0.9958,  1.1664,  1.1266,  1.1206,  1.1185,  1.1640,  1.1718,\n",
      "            1.1021,  1.1241,  0.7230],\n",
      "          [ 1.0076,  1.2011,  1.2011,  1.2011,  1.2011,  1.2011,  1.2011,\n",
      "            1.2011,  1.2011,  0.7325],\n",
      "          [ 0.7718,  0.8051,  0.8051,  0.8051,  0.8051,  0.8051,  0.8051,\n",
      "            0.8051,  0.8051,  0.4997]],\n",
      "\n",
      "         [[ 2.0976,  3.0555,  3.0555,  3.0555,  3.0555,  3.0555,  3.0555,\n",
      "            3.0555,  3.0555,  2.0064],\n",
      "          [ 3.0707,  4.3707,  4.3707,  4.3707,  4.3707,  4.3707,  4.3707,\n",
      "            4.3707,  4.3707,  2.8883],\n",
      "          [ 3.0707,  4.3707,  4.3707,  4.3707,  4.3707,  4.3707,  4.3616,\n",
      "            4.3675,  4.2519,  2.8883],\n",
      "          [ 3.0707,  4.3707,  4.3707,  4.3707,  4.3532,  3.7432,  3.0415,\n",
      "            3.5504,  2.9280,  2.8883],\n",
      "          [ 3.0707,  4.3677,  4.3696,  4.3559,  4.1339,  2.5540,  1.9409,\n",
      "            1.7459,  1.9037,  2.7361],\n",
      "          [ 3.0680,  4.3609,  4.2927,  3.8030,  2.6758,  2.3264,  1.9005,\n",
      "            1.8488,  1.9178,  1.9660],\n",
      "          [ 2.4568,  2.8890,  2.6425,  2.4161,  2.3417,  2.0817,  1.8807,\n",
      "            1.6999,  1.6879,  1.6099],\n",
      "          [ 2.7816,  3.1642,  2.6726,  2.6655,  2.5533,  2.8881,  3.2325,\n",
      "            2.1887,  2.3162,  2.1888],\n",
      "          [ 3.0707,  4.3707,  4.3707,  4.3707,  4.3707,  4.3707,  4.3707,\n",
      "            4.3707,  4.3707,  2.8883],\n",
      "          [ 2.3789,  3.1239,  3.1239,  3.1239,  3.1239,  3.1239,  3.1239,\n",
      "            3.1239,  3.1239,  2.1737]],\n",
      "\n",
      "         [[ 0.6091,  0.6550,  0.6550,  0.6550,  0.6550,  0.6550,  0.6550,\n",
      "            0.6550,  0.6550,  0.5733],\n",
      "          [ 0.6993,  0.7584,  0.7584,  0.7584,  0.7584,  0.7584,  0.7584,\n",
      "            0.7584,  0.7584,  0.6456],\n",
      "          [ 0.6993,  0.7584,  0.7584,  0.7584,  0.7584,  0.7584,  0.7604,\n",
      "            0.7584,  0.7894,  0.6456],\n",
      "          [ 0.6993,  0.7584,  0.7584,  0.7584,  0.7604,  1.0053,  1.2953,\n",
      "            1.0498,  1.3467,  0.6456],\n",
      "          [ 0.6993,  0.7584,  0.7584,  0.7604,  0.8230,  1.4790,  1.6699,\n",
      "            1.7159,  1.6787,  0.6679],\n",
      "          [ 0.7028,  0.7584,  0.7763,  0.9912,  1.4638,  1.5527,  1.6642,\n",
      "            1.6863,  1.6619,  0.8533],\n",
      "          [ 0.9459,  1.3649,  1.4501,  1.5265,  1.5652,  1.6425,  1.6812,\n",
      "            1.7130,  1.7186,  1.0123],\n",
      "          [ 0.7881,  1.1241,  1.4007,  1.4092,  1.4381,  1.2549,  1.0526,\n",
      "            1.4773,  1.4400,  0.8339],\n",
      "          [ 0.6993,  0.7584,  0.7584,  0.7584,  0.7584,  0.7584,  0.7584,\n",
      "            0.7584,  0.7584,  0.6456],\n",
      "          [ 0.5990,  0.6425,  0.6425,  0.6425,  0.6425,  0.6425,  0.6425,\n",
      "            0.6425,  0.6425,  0.5663]],\n",
      "\n",
      "         [[-0.6444, -0.6417, -0.6417, -0.6417, -0.6417, -0.6417, -0.6417,\n",
      "           -0.6417, -0.6417, -0.6416],\n",
      "          [-0.6461, -0.6435, -0.6435, -0.6435, -0.6435, -0.6435, -0.6435,\n",
      "           -0.6435, -0.6435, -0.6417],\n",
      "          [-0.6461, -0.6435, -0.6435, -0.6435, -0.6435, -0.6435, -0.6435,\n",
      "           -0.6435, -0.6466, -0.6417],\n",
      "          [-0.6461, -0.6435, -0.6435, -0.6435, -0.6440, -0.6431, -0.5514,\n",
      "           -0.5702, -0.5931, -0.6417],\n",
      "          [-0.6461, -0.6435, -0.6435, -0.6435, -0.6541, -0.5902, -0.5833,\n",
      "           -0.5800, -0.5755, -0.6252],\n",
      "          [-0.6461, -0.6435, -0.6480, -0.6124, -0.6029, -0.5829, -0.5690,\n",
      "           -0.5779, -0.5873, -0.5399],\n",
      "          [-0.6361, -0.5739, -0.5978, -0.5951, -0.5897, -0.5814, -0.5668,\n",
      "           -0.5561, -0.5652, -0.5374],\n",
      "          [-0.6899, -0.7013, -0.6504, -0.6191, -0.6353, -0.6728, -0.7084,\n",
      "           -0.6596, -0.6568, -0.6066],\n",
      "          [-0.6461, -0.6435, -0.6435, -0.6435, -0.6435, -0.6435, -0.6435,\n",
      "           -0.6435, -0.6435, -0.6417],\n",
      "          [-0.6469, -0.6457, -0.6457, -0.6457, -0.6457, -0.6457, -0.6457,\n",
      "           -0.6457, -0.6457, -0.6445]],\n",
      "\n",
      "         [[-0.6126, -0.7751, -0.7751, -0.7751, -0.7751, -0.7751, -0.7751,\n",
      "           -0.7751, -0.7751, -0.6241],\n",
      "          [-0.7554, -1.0205, -1.0205, -1.0205, -1.0205, -1.0205, -1.0205,\n",
      "           -1.0205, -1.0205, -0.7784],\n",
      "          [-0.7554, -1.0205, -1.0205, -1.0205, -1.0205, -1.0205, -1.0256,\n",
      "           -1.0205, -1.1049, -0.7784],\n",
      "          [-0.7554, -1.0205, -1.0205, -1.0205, -1.0307, -1.4657, -2.1717,\n",
      "           -1.6622, -2.1357, -0.7784],\n",
      "          [-0.7554, -1.0205, -1.0205, -1.0298, -1.1803, -2.2750, -2.7313,\n",
      "           -2.8358, -2.7602, -0.8736],\n",
      "          [-0.7605, -1.0298, -1.0777, -1.4798, -2.2570, -2.5313, -2.7666,\n",
      "           -2.7787, -2.7277, -1.4580],\n",
      "          [-1.1868, -2.1314, -2.3290, -2.4687, -2.5354, -2.6711, -2.7780,\n",
      "           -2.8490, -2.8503, -1.6932],\n",
      "          [-0.9587, -1.7368, -2.1362, -2.1515, -2.1907, -1.9974, -1.6654,\n",
      "           -2.2878, -2.2457, -1.3024],\n",
      "          [-0.7554, -1.0205, -1.0205, -1.0205, -1.0205, -1.0205, -1.0205,\n",
      "           -1.0205, -1.0205, -0.7784],\n",
      "          [-0.5609, -0.7431, -0.7431, -0.7431, -0.7431, -0.7431, -0.7431,\n",
      "           -0.7431, -0.7431, -0.5978]],\n",
      "\n",
      "         [[ 0.4622,  0.6391,  0.6391,  0.6391,  0.6391,  0.6391,  0.6391,\n",
      "            0.6391,  0.6391,  0.6093],\n",
      "          [ 0.4473,  0.7066,  0.7066,  0.7066,  0.7066,  0.7066,  0.7066,\n",
      "            0.7066,  0.7066,  0.6808],\n",
      "          [ 0.4473,  0.7066,  0.7066,  0.7066,  0.7066,  0.7066,  0.7058,\n",
      "            0.7045,  0.6905,  0.6808],\n",
      "          [ 0.4473,  0.7066,  0.7066,  0.7066,  0.7051,  0.5833, -0.4463,\n",
      "            0.2878,  0.0968,  0.6808],\n",
      "          [ 0.4473,  0.7062,  0.7053,  0.6989,  0.6749, -0.1142, -0.6752,\n",
      "           -0.8968, -0.7697,  0.5932],\n",
      "          [ 0.4446,  0.6988,  0.6975,  0.5216, -0.0489, -0.4057, -0.8810,\n",
      "           -0.8255, -0.6921, -0.5708],\n",
      "          [ 0.3090, -0.0603, -0.2113, -0.3841, -0.4812, -0.5799, -0.7805,\n",
      "           -1.0344, -0.8932, -0.9487],\n",
      "          [ 0.4653,  0.3175, -0.3358, -0.7495, -0.7420, -0.3496,  0.0649,\n",
      "           -0.9016, -0.8435, -0.4449],\n",
      "          [ 0.4473,  0.7066,  0.7066,  0.7066,  0.7066,  0.7066,  0.7066,\n",
      "            0.7066,  0.7066,  0.6808],\n",
      "          [ 0.4066,  0.6182,  0.6182,  0.6182,  0.6182,  0.6182,  0.6182,\n",
      "            0.6182,  0.6182,  0.6013]],\n",
      "\n",
      "         [[ 1.1120,  1.2910,  1.2910,  1.2910,  1.2910,  1.2910,  1.2910,\n",
      "            1.2910,  1.2910,  1.0830],\n",
      "          [ 1.3152,  1.7143,  1.7143,  1.7143,  1.7143,  1.7143,  1.7143,\n",
      "            1.7143,  1.7143,  1.3442],\n",
      "          [ 1.3152,  1.7143,  1.7143,  1.7143,  1.7143,  1.7143,  1.7143,\n",
      "            1.7143,  1.7165,  1.3442],\n",
      "          [ 1.3152,  1.7143,  1.7143,  1.7143,  1.7143,  1.7405,  1.7987,\n",
      "            1.7611,  1.7951,  1.3442],\n",
      "          [ 1.3152,  1.7143,  1.7143,  1.7143,  1.7231,  1.7952,  1.8412,\n",
      "            1.8581,  1.8505,  1.3476],\n",
      "          [ 1.3152,  1.7143,  1.7165,  1.7427,  1.7972,  1.8261,  1.8495,\n",
      "            1.8454,  1.8439,  1.3925],\n",
      "          [ 1.3445,  1.7853,  1.8037,  1.8160,  1.8225,  1.8332,  1.8518,\n",
      "            1.8599,  1.8594,  1.4117],\n",
      "          [ 1.3297,  1.7754,  1.8064,  1.8075,  1.8130,  1.7941,  1.7798,\n",
      "            1.8360,  1.8321,  1.3862],\n",
      "          [ 1.3152,  1.7143,  1.7143,  1.7143,  1.7143,  1.7143,  1.7143,\n",
      "            1.7143,  1.7143,  1.3442],\n",
      "          [ 1.0177,  1.3225,  1.3225,  1.3225,  1.3225,  1.3225,  1.3225,\n",
      "            1.3225,  1.3225,  1.0758]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "q_input_activation = {}\n",
    "q_output_activation = {}\n",
    "\n",
    "#Define a hook to record the feature map of each layer\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        q_input_activation[module_name] = x.detach()\n",
    "        q_output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "\n",
    "\n",
    "    return all_hooks\n",
    "\n",
    "\n",
    "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "hooks = add_range_recoder_hook(model)\n",
    "sample_data = iter(q_test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
    "model(sample_data) #Forward to use hook\n",
    "#print(quantized_model.Conv[1].weights)\n",
    "print(q_output_activation.keys())\n",
    "# print(q_input_activation[\"Conv.2\"])\n",
    "print(q_output_activation[\"Conv.4\"])\n",
    "# print(quantized_model.Conv[1].bias)\n",
    "# print(quantized_model.Conv[1].q_bias)\n",
    "# print(quantized_model.Conv[1].input_scale)\n",
    "# print(quantized_model.Conv[1].weight_scale)\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "#Dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "#Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  #set model to evaluate mode\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "      if use_gpu:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      pred = model(x)\n",
    "      test_loss = loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "test_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-22791., -22080.,  14448., -19011.,  -7044., -19033.,  11902., -20904.],\n",
      "       device='cuda:0')\n",
      "byte0: f9 70 7c 7e\n",
      "=======\n",
      "byte1: a6 38 e4 2e\n",
      "=======\n",
      "byte2: c0 bd a7 58\n",
      "=======\n",
      "byte3: a9 b5 b5 ae\n",
      "=======\n",
      "=========dividen=============\n",
      "byte0: f9 70 7c 7e\n",
      "=======\n",
      "byte1: a6 38 e4 2e\n",
      "=======\n",
      "byte2: c0 bd a7 58\n",
      "=======\n",
      "byte3: a9 b5 b5 ae\n",
      "=======\n",
      "deq_scale (shift 13): 01011010\n",
      "req_scale (shift 6): 00000001\n"
     ]
    }
   ],
   "source": [
    "#golden_gen(q_output_activation[\"Conv.3\"])\n",
    "#print(model.Conv[1].input_zero_point)\n",
    "#input_or_weight_gen(q_input_activation[\"Conv.1\"])\n",
    "#print(\"===\")\n",
    "\n",
    "#input_or_weight_gen(model.Conv[1].weights)\n",
    "print(model.Conv[1].q_bias)\n",
    "bias_gen(model.Conv[1].q_bias)\n",
    "print(\"=========dividen=============\")\n",
    "bias_gen(model.Conv[1].q_bias)\n",
    "#golden_gen(q_output_activation[\"Conv.3\"])\n",
    "#print(model.Conv[1].DeQ_scale)\n",
    "#print(model.Conv[3].scale)\n",
    "print(\"deq_scale (shift 13):\",DecToBin_machine(model.Conv[1].DeQ_scale*8192 ,8))\n",
    "print(\"req_scale (shift 6):\",DecToBin_machine(model.Conv[3].scale ,8))\n",
    "#print(\"output zero\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2bababa\n",
      "babababa\n",
      "baafb9c4\n",
      "c4c4c4c4\n",
      "c4c4c4b6\n",
      "b9c4c4c4\n",
      "c4c4c5c4\n",
      "c7b6b9c4\n",
      "c4c4c5d3\n",
      "e3d5e6b6\n",
      "b9c4c4c5\n",
      "caedfafd\n",
      "fab8b9c5\n",
      "c6d2ebf3\n",
      "fbfbfac4\n",
      "c6e8edf2\n",
      "f4f8fafd\n",
      "fdcbbdd9\n",
      "e7e7e8de\n",
      "d4eae9c1\n",
      "b9c4c4c4\n",
      "c4c4c4c4\n",
      "c4b6b1b8\n",
      "b8b8b8b8\n",
      "b8b8b8af\n",
      "b4bababa\n",
      "babababa\n",
      "baabb8c1\n",
      "c1c1c1c1\n",
      "c1c1c1ac\n",
      "b8c1c1c1\n",
      "c1c1c1c1\n",
      "c1acb8c1\n",
      "c1c1c1c0\n",
      "bebfbdac\n",
      "b8c1c1c1\n",
      "c1bebcbb\n",
      "bcacb8c1\n",
      "c1c0bebd\n",
      "bcbcbcab\n",
      "b7bebebd\n",
      "bdbcbcbb\n",
      "bbabb8bf\n",
      "bebdbdbf\n",
      "c0bcbdac\n",
      "b8c1c1c1\n",
      "c1c1c1c1\n",
      "c1acaeaf\n",
      "afafafaf\n",
      "afafafa4\n",
      "f3383838\n",
      "38383838\n",
      "38ee397e\n",
      "7e7e7e7e\n",
      "7e7e7e2c\n",
      "397e7e7e\n",
      "7e7e7e7e\n",
      "782c397e\n",
      "7e7e7d5d\n",
      "37522f2c\n",
      "397e7e7d\n",
      "7113eade\n",
      "e720387e\n",
      "7a601c03\n",
      "e7e4e8eb\n",
      "0c2c1909\n",
      "04f2e6db\n",
      "dbd6243e\n",
      "1b1b132c\n",
      "41f902f9\n",
      "397e7e7e\n",
      "7e7e7e7e\n",
      "7e2c063b\n",
      "3b3b3b3b\n",
      "3b3b3bf8\n",
      "a8a9a9a9\n",
      "a9a9a9a9\n",
      "a9a6abad\n",
      "adadadad\n",
      "adadada9\n",
      "abadadad\n",
      "adadaead\n",
      "afa9abad\n",
      "adadaeb8\n",
      "c6bac8a9\n",
      "abadadae\n",
      "b0cfdadc\n",
      "daaaabad\n",
      "aeb7ced3\n",
      "d9dbd9b1\n",
      "b5c9ced2\n",
      "d4d8dadc\n",
      "dcb8afbd\n",
      "cbcbcdc4\n",
      "bacfcdb1\n",
      "abadadad\n",
      "adadadad\n",
      "ada9a7a9\n",
      "a9a9a9a9\n",
      "a9a9a9a6\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "88888786\n",
      "86868686\n",
      "86878787\n",
      "88878686\n",
      "86878787\n",
      "88888788\n",
      "87888787\n",
      "87878888\n",
      "88888686\n",
      "86878786\n",
      "86868687\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "86868686\n",
      "87858585\n",
      "85858585\n",
      "85878582\n",
      "82828282\n",
      "82828285\n",
      "85828282\n",
      "82828282\n",
      "81858582\n",
      "82828280\n",
      "84808485\n",
      "85828282\n",
      "81858d90\n",
      "8e838582\n",
      "82808589\n",
      "8e8f8d80\n",
      "81838688\n",
      "898c8e90\n",
      "90808380\n",
      "84848482\n",
      "80858580\n",
      "85828282\n",
      "82828282\n",
      "82858885\n",
      "85858585\n",
      "85858587\n",
      "a2a9a9a9\n",
      "a9a9a9a9\n",
      "a9a8a2ab\n",
      "abababab\n",
      "abababaa\n",
      "a2ababab\n",
      "abababab\n",
      "abaaa2ab\n",
      "abababa7\n",
      "8a9c97aa\n",
      "a2ababab\n",
      "aa918683\n",
      "85a7a2ab\n",
      "aba4938b\n",
      "83848688\n",
      "9d928f8b\n",
      "89878582\n",
      "8383a29d\n",
      "8c85858c\n",
      "9683848a\n",
      "a2ababab\n",
      "abababab\n",
      "abaaa0a8\n",
      "a8a8a8a8\n",
      "a8a8a8a7\n",
      "bdc5c5c5\n",
      "c5c5c5c5\n",
      "c5bbc7dc\n",
      "dcdcdcdc\n",
      "dcdcdcc8\n",
      "c7dcdcdc\n",
      "dcdcdcdc\n",
      "dcc8c7dc\n",
      "dcdcdcde\n",
      "e1dfe1c8\n",
      "c7dcdcdc\n",
      "dde1e4e5\n",
      "e4c8c7dc\n",
      "dcdee1e3\n",
      "e4e4e4cb\n",
      "c8e0e1e2\n",
      "e2e3e4e5\n",
      "e5ccc7e0\n",
      "e2e2e2e1\n",
      "e0e3e3ca\n",
      "c7dcdcdc\n",
      "dcdcdcdc\n",
      "dcc8b9c7\n",
      "c7c7c7c7\n",
      "c7c7c7bb\n"
     ]
    }
   ],
   "source": [
    "golden_gen(q_output_activation[\"Conv.6\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
