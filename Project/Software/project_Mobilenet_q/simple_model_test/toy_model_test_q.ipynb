{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHXk_L8g3fDh"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tVH9mlCdXrkw"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "\n",
        "\n",
        "no_cuda = False\n",
        "use_gpu = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) / 6\n",
        "\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk1U6PcMXtDB",
        "outputId": "58b165e2-e66b-41c5-987d-4fcc1b6a26fa"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#Dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzMtMm9n3hsZ"
      },
      "source": [
        "Create NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-pKE7xJbOc7",
        "outputId": "fce122fd-5aa8-4d0c-fe2c-94bb60cc64ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (Conv): Sequential(\n",
            "    (0): Conv2d(1, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): Conv2d(5, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (backbone): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=120, bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=120, out_features=84, bias=False)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=84, out_features=10, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=1, out_channels=5, kernel_size=1, stride=1,padding= 0, bias=False),\n",
        "      h_swish(inplace=True),\n",
        "      nn.Conv2d(in_channels=5, out_channels=1, kernel_size=1, stride=1,padding= 0, bias=False),\n",
        "      h_swish(inplace=True)\n",
        "    )  \n",
        "    self.backbone = nn.Sequential(\n",
        "      nn.Linear(28*28, 120, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(120, 84, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(84, 10, bias=False)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.Conv(x)\n",
        "    x = x.view(-1, 28 * 28) #transform 28*28 figure to 784 vector\n",
        "    x = self.backbone(x)\n",
        "    return x\n",
        "\n",
        "FP32_model = ToyModel()\n",
        "print(FP32_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "MOyeqSPDbvr5"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  #Set the model to train mode\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    if use_gpu:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    #forward\n",
        "    pred = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #backward\n",
        "    loss.backward()\n",
        "\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  #set model to evaluate mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      if use_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "      pred = model(x)\n",
        "      test_loss = loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LsfzIw4b1AU",
        "outputId": "03aeb700-d0fe-4e88-a24e-d000b63c4184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (Conv): Sequential(\n",
              "    (0): Conv2d(1, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (2): Conv2d(5, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (backbone): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=120, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=120, out_features=84, bias=False)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=84, out_features=10, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 3\n",
        "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
        "optimizer = torch.optim.SGD(FP32_model.parameters(), lr=learning_rate, momentum=0.9)  #define optimizer\n",
        "\n",
        "FP32_model.to(device) #let model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "LH6kt0eqb9tl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302464  [   32/60000]\n",
            "loss: 2.301297  [ 3232/60000]\n",
            "loss: 2.299700  [ 6432/60000]\n",
            "loss: 2.294851  [ 9632/60000]\n",
            "loss: 2.294723  [12832/60000]\n",
            "loss: 2.282074  [16032/60000]\n",
            "loss: 2.275557  [19232/60000]\n",
            "loss: 2.238265  [22432/60000]\n",
            "loss: 2.170888  [25632/60000]\n",
            "loss: 1.727330  [28832/60000]\n",
            "loss: 0.981952  [32032/60000]\n",
            "loss: 0.651869  [35232/60000]\n",
            "loss: 0.797447  [38432/60000]\n",
            "loss: 0.621260  [41632/60000]\n",
            "loss: 0.383997  [44832/60000]\n",
            "loss: 0.526528  [48032/60000]\n",
            "loss: 0.594913  [51232/60000]\n",
            "loss: 0.434933  [54432/60000]\n",
            "loss: 0.332496  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.000947 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.439545  [   32/60000]\n",
            "loss: 0.378272  [ 3232/60000]\n",
            "loss: 0.454065  [ 6432/60000]\n",
            "loss: 0.348154  [ 9632/60000]\n",
            "loss: 0.262269  [12832/60000]\n",
            "loss: 0.659274  [16032/60000]\n",
            "loss: 0.231311  [19232/60000]\n",
            "loss: 0.417038  [22432/60000]\n",
            "loss: 0.399859  [25632/60000]\n",
            "loss: 0.295577  [28832/60000]\n",
            "loss: 0.338230  [32032/60000]\n",
            "loss: 0.354428  [35232/60000]\n",
            "loss: 0.315698  [38432/60000]\n",
            "loss: 0.611784  [41632/60000]\n",
            "loss: 0.439820  [44832/60000]\n",
            "loss: 0.295936  [48032/60000]\n",
            "loss: 0.298091  [51232/60000]\n",
            "loss: 0.445024  [54432/60000]\n",
            "loss: 0.439472  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.000674 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.365472  [   32/60000]\n",
            "loss: 0.515133  [ 3232/60000]\n",
            "loss: 0.336437  [ 6432/60000]\n",
            "loss: 0.539898  [ 9632/60000]\n",
            "loss: 0.503634  [12832/60000]\n",
            "loss: 0.274241  [16032/60000]\n",
            "loss: 0.419305  [19232/60000]\n",
            "loss: 0.263659  [22432/60000]\n",
            "loss: 0.205673  [25632/60000]\n",
            "loss: 0.284162  [28832/60000]\n",
            "loss: 0.610005  [32032/60000]\n",
            "loss: 0.591726  [35232/60000]\n",
            "loss: 0.688149  [38432/60000]\n",
            "loss: 0.268737  [41632/60000]\n",
            "loss: 0.231534  [44832/60000]\n",
            "loss: 0.521940  [48032/60000]\n",
            "loss: 0.356958  [51232/60000]\n",
            "loss: 0.379526  [54432/60000]\n",
            "loss: 0.360419  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.000520 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, FP32_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAK3-fH3qGh"
      },
      "source": [
        "# Quantization definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lWzxrde4G5i"
      },
      "source": [
        "####Question 1.####\n",
        "\n",
        "Use\n",
        ">$S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})$\n",
        "\n",
        ">$Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S$\n",
        "\n",
        "to calculate scale factor and zero point of a tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kJIr-5SpcgQr"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero_point(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -2**(bitwidth-1), 2**(bitwidth-1) - 1\n",
        "  fp_min = fp32_tensor.min().item()\n",
        "  fp_max = fp32_tensor.max().item()\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  scale = (fp_max-fp_min) / (q_max-q_min)\n",
        "  zero_point = q_min-fp_min /scale\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "\n",
        "  zero_point = round(zero_point)          #round\n",
        "  zero_point = max(q_min, min(zero_point, q_max)) #clip\n",
        "\n",
        "  return scale, int(zero_point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YA7ano5nS1"
      },
      "source": [
        "####Question 2.####\n",
        "\n",
        "Use $q=r/S + Z$ to quantize a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "sBMKB5Le54wr"
      },
      "outputs": [],
      "source": [
        "def linear_quantize(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -2**(bitwidth-1), 2**(bitwidth-1) - 1\n",
        "\n",
        "  scale, zero_point = get_scale_and_zero_point(fp32_tensor)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  q_tensor = torch.round( fp32_tensor/scale ) +zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp\n",
        "  q_tensor = torch.clamp(q_tensor, q_min, q_max)\n",
        "  return q_tensor, scale, zero_point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taJXSmz6KDS"
      },
      "source": [
        "####Question 3.####\n",
        "\n",
        "Use\n",
        "> $q_{\\mathrm{output}} = M * \\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Z_{\\mathrm{output}}$\n",
        "\n",
        "> $M = S_{\\mathrm{input}} * S_{\\mathrm{weight}} / S_{\\mathrm{output}}$\n",
        "\n",
        "to compute quantized linear operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sbXY0vaCcn7l"
      },
      "outputs": [],
      "source": [
        "def quantized_linear(input, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=8):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale / output_scale\n",
        "  output = torch.nn.functional.linear((input - input_zero_point ), (weights - weight_zero_point))\n",
        "  output *= M\n",
        "  output += output_zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.round().clamp(-2**(activation_bitwidth-1), 2**(activation_bitwidth-1)-1)\n",
        "\n",
        "  return output\n",
        "\n",
        "def quantized_conv(input, weights,stride, padding,groups,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=8):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale / output_scale\n",
        "  output = torch.nn.functional.conv2d((input - input_zero_point ), (weights - weight_zero_point),stride=stride,padding=padding,groups=groups)\n",
        "  output *= M\n",
        "  output += output_zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.round().clamp(-2**(activation_bitwidth-1), 2**(activation_bitwidth-1)-1)\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK10k10R7II7"
      },
      "source": [
        "# Design quantized linear layer and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "IwrXNVKadKfG"
      },
      "outputs": [],
      "source": [
        "class QuantizedConv(nn.Module):\n",
        "  def __init__(self, weights,stride,padding,groups ,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.stride, self.padding, self.groups = stride, padding,groups\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "\n",
        "  def forward(self, x):\n",
        "    return quantized_conv(x, self.weights, self.stride, self.padding, self.groups, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedConv(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "  def __init__(self, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "\n",
        "  def forward(self, x):\n",
        "    return quantized_linear(x, self.weights, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedLinear(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "#Transform input data to correct integer range\n",
        "class Preprocess(nn.Module):\n",
        "  def __init__(self, input_scale, input_zero_point, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "  def forward(self, x):\n",
        "    x = x / self.input_scale + self.input_zero_point\n",
        "    return x\n",
        "  \n",
        "class Quantizer(nn.Module):\n",
        "  def __init__(self,scale,bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.scale = scale\n",
        "\n",
        "    def do_requant(self, input, scale,zero_point,bitwidth):\n",
        "        output = input / scale\n",
        "        output = output.round()\n",
        "        output += zero_point\n",
        "        output = output.clamp(-2**(bitwidth-1), 2**(bitwidth-1)-1)\n",
        "        return output\n",
        "    \n",
        "    def forward(self,x):\n",
        "      return do_requant(x,self.scale)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUpiPDiu7RCH"
      },
      "source": [
        "# Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "cBdXFnr5dZqT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[ 0.9064]]],\n",
            "\n",
            "\n",
            "        [[[-0.3772]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6363]]],\n",
            "\n",
            "\n",
            "        [[[-2.0393]]],\n",
            "\n",
            "\n",
            "        [[[-0.6909]]]], device='cuda:0', requires_grad=True)\n",
            "tensor([[[[-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          ...,\n",
            "          [-0.9064, -0.9064, -0.9064,  ...,  0.1813, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064]],\n",
            "\n",
            "         [[ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          ...,\n",
            "          [ 0.3772,  0.3772,  0.3772,  ..., -0.0754,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772]],\n",
            "\n",
            "         [[-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          ...,\n",
            "          [-0.6363, -0.6363, -0.6363,  ...,  0.1273, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363]],\n",
            "\n",
            "         [[ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          ...,\n",
            "          [ 2.0393,  2.0393,  2.0393,  ..., -0.4079,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393]],\n",
            "\n",
            "         [[ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          ...,\n",
            "          [ 0.6909,  0.6909,  0.6909,  ..., -0.1382,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909]]],\n",
            "\n",
            "\n",
            "        [[[-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          ...,\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064]],\n",
            "\n",
            "         [[ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          ...,\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772]],\n",
            "\n",
            "         [[-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          ...,\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363]],\n",
            "\n",
            "         [[ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          ...,\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393]],\n",
            "\n",
            "         [[ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          ...,\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909]]],\n",
            "\n",
            "\n",
            "        [[[-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          ...,\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064]],\n",
            "\n",
            "         [[ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          ...,\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772]],\n",
            "\n",
            "         [[-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          ...,\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363]],\n",
            "\n",
            "         [[ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          ...,\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393]],\n",
            "\n",
            "         [[ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          ...,\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          ...,\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064]],\n",
            "\n",
            "         [[ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          ...,\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772]],\n",
            "\n",
            "         [[-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          ...,\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363]],\n",
            "\n",
            "         [[ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          ...,\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393]],\n",
            "\n",
            "         [[ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          ...,\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909]]],\n",
            "\n",
            "\n",
            "        [[[-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          ...,\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064]],\n",
            "\n",
            "         [[ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          ...,\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772]],\n",
            "\n",
            "         [[-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          ...,\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363]],\n",
            "\n",
            "         [[ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          ...,\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393]],\n",
            "\n",
            "         [[ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          ...,\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909]]],\n",
            "\n",
            "\n",
            "        [[[-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          ...,\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064],\n",
            "          [-0.9064, -0.9064, -0.9064,  ..., -0.9064, -0.9064, -0.9064]],\n",
            "\n",
            "         [[ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          ...,\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772],\n",
            "          [ 0.3772,  0.3772,  0.3772,  ...,  0.3772,  0.3772,  0.3772]],\n",
            "\n",
            "         [[-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          ...,\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363],\n",
            "          [-0.6363, -0.6363, -0.6363,  ..., -0.6363, -0.6363, -0.6363]],\n",
            "\n",
            "         [[ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          ...,\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393],\n",
            "          [ 2.0393,  2.0393,  2.0393,  ...,  2.0393,  2.0393,  2.0393]],\n",
            "\n",
            "         [[ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          ...,\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909],\n",
            "          [ 0.6909,  0.6909,  0.6909,  ...,  0.6909,  0.6909,  0.6909]]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# add hook to record the min max value of the activation\n",
        "input_activation = {}\n",
        "output_activation = {}\n",
        "\n",
        "#Define a hook to record the feature map of each layer\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        input_activation[module_name] = x.detach()\n",
        "        output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (nn.Linear, nn.ReLU,nn.Conv2d,h_swish)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "\n",
        "\n",
        "    return all_hooks\n",
        "\n",
        "hooks = add_range_recoder_hook(FP32_model)\n",
        "sample_data = iter(train_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
        "FP32_model(sample_data) #Forward to use hook\n",
        "print(FP32_model.Conv[0].weight)\n",
        "print(output_activation[\"Conv.0\"])\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVf8vpiVTsDa"
      },
      "source": [
        "# Quantize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "SVh-SRj8eOrs"
      },
      "outputs": [],
      "source": [
        "#copy original model\n",
        "quantized_model = copy.deepcopy(FP32_model)\n",
        "\n",
        "#Record each layer in original model\n",
        "quantized_backbone = []\n",
        "quantized_Conv = []\n",
        "i = 0\n",
        "\n",
        "#Record input scale and zero point\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"Conv.0\"])\n",
        "preprocess = Preprocess(input_scale, input_zero_point)\n",
        "quantized_Conv.append(preprocess)\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['Conv.0'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['Conv.1'])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(FP32_model.Conv[0].weight.data)\n",
        "quantizedConv1 = QuantizedConv(quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "#quantized_model.Conv[0] = quantizedConv1 \n",
        "\n",
        "quantized_Conv.append(quantizedConv1)\n",
        "\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['Conv.2'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['Conv.3'])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(FP32_model.Conv[2].weight.data)\n",
        "quantizedConv2 = QuantizedConv(quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "#quantized_model.Conv[2] = quantizedConv2 \n",
        "quantized_Conv.append(quantizedConv2)\n",
        "\n",
        "#Record Linear + ReLU of the model (except the last Linear)\n",
        "while i < len(quantized_model.backbone) - 1:\n",
        "  if isinstance(quantized_model.backbone[i], nn.Linear) and isinstance(quantized_model.backbone[i+1], nn.ReLU):\n",
        "    linear = quantized_model.backbone[i]\n",
        "    linear_name = f\"backbone.{i}\"\n",
        "    relu = quantized_model.backbone[i + 1]\n",
        "    relu_name = f\"backbone.{i + 1}\"\n",
        "\n",
        "    #Use the calibration data to calculate scale and zero point of each layer\n",
        "    input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "    output_scale, output_zero_point = get_scale_and_zero_point(output_activation[relu_name])\n",
        "    quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "\n",
        "    quantizedLinear = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "\n",
        "    quantized_backbone.append(quantizedLinear)\n",
        "    i += 2\n",
        "\n",
        "#Record the last Linear layer\n",
        "linear = quantized_model.backbone[4]\n",
        "linear_name = f\"backbone.4\"\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation[linear_name])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "quantizedLinear = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "quantized_backbone.append(quantizedLinear)\n",
        "\n",
        "quantized_model.Conv = nn.Sequential(*quantized_Conv)\n",
        "quantized_model.backbone = nn.Sequential(*quantized_backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRB96PKbfNX4",
        "outputId": "58ce882a-3521-4b40-ff98-44dcea373546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (Conv): Sequential(\n",
            "    (0): Preprocess()\n",
            "    (1): QuantizedConv(in_channels=1, out_channels=5)\n",
            "    (2): QuantizedConv(in_channels=5, out_channels=1)\n",
            "  )\n",
            "  (backbone): Sequential(\n",
            "    (0): QuantizedLinear(in_channels=784, out_channels=120)\n",
            "    (1): QuantizedLinear(in_channels=120, out_channels=84)\n",
            "    (2): QuantizedLinear(in_channels=84, out_channels=10)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(quantized_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Conv.1', 'Conv.2', 'backbone.0', 'backbone.1', 'backbone.2'])\n"
          ]
        }
      ],
      "source": [
        "# add hook to record the min max value of the activation\n",
        "q_input_activation = {}\n",
        "q_output_activation = {}\n",
        "\n",
        "#Define a hook to record the feature map of each layer\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        q_input_activation[module_name] = x.detach()\n",
        "        q_output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "\n",
        "\n",
        "    return all_hooks\n",
        "\n",
        "hooks = add_range_recoder_hook(quantized_model)\n",
        "sample_data = iter(train_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
        "quantized_model(sample_data) #Forward to use hook\n",
        "#print(quantized_model.Conv[1].weights)\n",
        "print(q_output_activation.keys())\n",
        "#print(q_output_activation[\"Conv.1\"])\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMccqTL6URaZ"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRMXzNeCgDuq",
        "outputId": "0fa65b9f-3a60-41e0-a935-a601a8db0484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.000520 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knTzO1mbheBe",
        "outputId": "411fc256-b7ea-4757-9f75-6acaae605324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.004596 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, quantized_model, loss_fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
