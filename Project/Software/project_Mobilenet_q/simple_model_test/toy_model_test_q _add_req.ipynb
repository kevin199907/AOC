{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHXk_L8g3fDh"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tVH9mlCdXrkw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\胡家豪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "\n",
        "\n",
        "no_cuda = False\n",
        "use_gpu = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) / 6\n",
        "\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk1U6PcMXtDB",
        "outputId": "58b165e2-e66b-41c5-987d-4fcc1b6a26fa"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#Dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzMtMm9n3hsZ"
      },
      "source": [
        "Create NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-pKE7xJbOc7",
        "outputId": "fce122fd-5aa8-4d0c-fe2c-94bb60cc64ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (Conv): Sequential(\n",
            "    (0): Conv2d(1, 5, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): Conv2d(5, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (backbone): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=120, bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=120, out_features=84, bias=False)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=84, out_features=10, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=1, out_channels=5, kernel_size=1, stride=1,padding= 0, bias=True),\n",
        "      h_swish(inplace=True),\n",
        "      nn.Conv2d(in_channels=5, out_channels=1, kernel_size=1, stride=1,padding= 0, bias=True),\n",
        "      h_swish(inplace=True)\n",
        "    )  \n",
        "    self.backbone = nn.Sequential(\n",
        "      nn.Linear(28*28, 120, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(120, 84, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(84, 10, bias=False)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.Conv(x)\n",
        "    x = x.view(-1, 28 * 28) #transform 28*28 figure to 784 vector\n",
        "    x = self.backbone(x)\n",
        "    return x\n",
        "\n",
        "FP32_model = ToyModel()\n",
        "print(FP32_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MOyeqSPDbvr5"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  #Set the model to train mode\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    if use_gpu:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    #forward\n",
        "    pred = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #backward\n",
        "    loss.backward()\n",
        "\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  #set model to evaluate mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      if use_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "      pred = model(x)\n",
        "      test_loss = loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LsfzIw4b1AU",
        "outputId": "03aeb700-d0fe-4e88-a24e-d000b63c4184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (Conv): Sequential(\n",
              "    (0): Conv2d(1, 5, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (2): Conv2d(5, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (backbone): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=120, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=120, out_features=84, bias=False)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=84, out_features=10, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 3\n",
        "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
        "optimizer = torch.optim.SGD(FP32_model.parameters(), lr=learning_rate, momentum=0.9)  #define optimizer\n",
        "\n",
        "FP32_model.to(device) #let model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LH6kt0eqb9tl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.301396  [   32/60000]\n",
            "loss: 2.300920  [ 3232/60000]\n",
            "loss: 2.299577  [ 6432/60000]\n",
            "loss: 2.303290  [ 9632/60000]\n",
            "loss: 2.303569  [12832/60000]\n",
            "loss: 2.302467  [16032/60000]\n",
            "loss: 2.305001  [19232/60000]\n",
            "loss: 2.304802  [22432/60000]\n",
            "loss: 2.298074  [25632/60000]\n",
            "loss: 2.298676  [28832/60000]\n",
            "loss: 2.293783  [32032/60000]\n",
            "loss: 2.294315  [35232/60000]\n",
            "loss: 2.287030  [38432/60000]\n",
            "loss: 2.282912  [41632/60000]\n",
            "loss: 2.269661  [44832/60000]\n",
            "loss: 2.279294  [48032/60000]\n",
            "loss: 2.266826  [51232/60000]\n",
            "loss: 2.240796  [54432/60000]\n",
            "loss: 2.171411  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 27.7%, Avg loss: 0.006706 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.130034  [   32/60000]\n",
            "loss: 1.968865  [ 3232/60000]\n",
            "loss: 1.662483  [ 6432/60000]\n",
            "loss: 1.600562  [ 9632/60000]\n",
            "loss: 1.223564  [12832/60000]\n",
            "loss: 1.030667  [16032/60000]\n",
            "loss: 1.034443  [19232/60000]\n",
            "loss: 1.081744  [22432/60000]\n",
            "loss: 0.808017  [25632/60000]\n",
            "loss: 0.584018  [28832/60000]\n",
            "loss: 0.742622  [32032/60000]\n",
            "loss: 0.864561  [35232/60000]\n",
            "loss: 0.435673  [38432/60000]\n",
            "loss: 0.479690  [41632/60000]\n",
            "loss: 0.603440  [44832/60000]\n",
            "loss: 0.608610  [48032/60000]\n",
            "loss: 0.404819  [51232/60000]\n",
            "loss: 0.569354  [54432/60000]\n",
            "loss: 0.489418  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.000857 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.777157  [   32/60000]\n",
            "loss: 0.345554  [ 3232/60000]\n",
            "loss: 0.332722  [ 6432/60000]\n",
            "loss: 0.534401  [ 9632/60000]\n",
            "loss: 0.390499  [12832/60000]\n",
            "loss: 0.340784  [16032/60000]\n",
            "loss: 0.352837  [19232/60000]\n",
            "loss: 0.413177  [22432/60000]\n",
            "loss: 0.375574  [25632/60000]\n",
            "loss: 0.640623  [28832/60000]\n",
            "loss: 0.641669  [32032/60000]\n",
            "loss: 0.473435  [35232/60000]\n",
            "loss: 0.567109  [38432/60000]\n",
            "loss: 0.390875  [41632/60000]\n",
            "loss: 0.648148  [44832/60000]\n",
            "loss: 0.126346  [48032/60000]\n",
            "loss: 0.328820  [51232/60000]\n",
            "loss: 0.334339  [54432/60000]\n",
            "loss: 0.468713  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.000745 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, FP32_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAK3-fH3qGh"
      },
      "source": [
        "# Quantization definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lWzxrde4G5i"
      },
      "source": [
        "####Question 1.####\n",
        "\n",
        "Use\n",
        ">$S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})$\n",
        "\n",
        ">$Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S$\n",
        "\n",
        "to calculate scale factor and zero point of a tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kJIr-5SpcgQr"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero_point(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -(2**(bitwidth-1)-1), 2**(bitwidth-1) - 1\n",
        "  fp_min = fp32_tensor.min().item()\n",
        "  fp_max = fp32_tensor.max().item()\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  scale = (fp_max-fp_min) / (q_max-q_min)\n",
        "  zero_point = q_min-fp_min /scale\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "\n",
        "  zero_point = round(zero_point)          #round\n",
        "  zero_point = max(q_min, min(zero_point, q_max)) #clip\n",
        "\n",
        "  return scale, int(zero_point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YA7ano5nS1"
      },
      "source": [
        "####Question 2.####\n",
        "\n",
        "Use $q=r/S + Z$ to quantize a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sBMKB5Le54wr"
      },
      "outputs": [],
      "source": [
        "def linear_quantize(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -(2**(bitwidth-1)-1), 2**(bitwidth-1) - 1\n",
        "\n",
        "  scale, zero_point = get_scale_and_zero_point(fp32_tensor)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  q_tensor = torch.round( fp32_tensor/scale ) +zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp\n",
        "  q_tensor = torch.clamp(q_tensor, q_min, q_max)\n",
        "  return q_tensor, scale, zero_point  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taJXSmz6KDS"
      },
      "source": [
        "####Question 3.####\n",
        "\n",
        "Use\n",
        "> $q_{\\mathrm{output}} = M * \\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Z_{\\mathrm{output}}$\n",
        "\n",
        "> $M = S_{\\mathrm{input}} * S_{\\mathrm{weight}} / S_{\\mathrm{output}}$\n",
        "\n",
        "to compute quantized linear operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sbXY0vaCcn7l"
      },
      "outputs": [],
      "source": [
        "def quantized_linear(input, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=8):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale / output_scale\n",
        "  output = torch.nn.functional.linear((input - input_zero_point ), (weights - weight_zero_point))\n",
        "  output *= M\n",
        "  output += output_zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.round().clamp(-2**(activation_bitwidth-1)-1, 2**(activation_bitwidth-1)-1)\n",
        "\n",
        "  return output\n",
        "\n",
        "def quantized_conv(input, bias,weights,stride, padding,groups,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=16):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale \n",
        "  conv_bias = bias /M\n",
        "  conv_bias = conv_bias.round()\n",
        "  output_only_conv = torch.nn.functional.conv2d((input - input_zero_point ), (weights - weight_zero_point),bias = conv_bias ,stride=stride,padding=padding,groups=groups)\n",
        "  output = M * output_only_conv\n",
        "  #output += output_zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.round().clamp(-2**(activation_bitwidth-1)-1, 2**(activation_bitwidth-1)-1)\n",
        "\n",
        "  return output\n",
        "\n",
        "def do_requant(input, scale,zero_point,bitwidth=8):\n",
        "    output = input / scale\n",
        "    output = output.round()\n",
        "    output += zero_point\n",
        "    output = output.clamp(-2**(bitwidth-1)-1, 2**(bitwidth-1)-1)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK10k10R7II7"
      },
      "source": [
        "# Design quantized linear layer and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IwrXNVKadKfG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QuantizedConv(nn.Module):\n",
        "  def __init__(self,bias ,weights,stride,padding,groups ,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.stride, self.padding, self.groups = stride, padding,groups\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "    self.bias = bias\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "    self.q_bias = torch.round(bias / (input_scale*weight_scale))\n",
        "    self.q_weight = weights - weight_zero_point\n",
        "    self.DeQ_scale = input_scale*weight_scale*8192\n",
        "  def forward(self, x):\n",
        "    return quantized_conv(x, self.bias, self.weights, self.stride, self.padding, self.groups, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedConv(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "  def __init__(self, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "\n",
        "  def forward(self, x):\n",
        "    return quantized_linear(x, self.weights, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedLinear(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "#Transform input data to correct integer range\n",
        "class Preprocess(nn.Module):\n",
        "  def __init__(self, input_scale, input_zero_point, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "  def forward(self, x):\n",
        "    x = x / self.input_scale + self.input_zero_point\n",
        "    x = x.round() \n",
        "    return x\n",
        "  \n",
        "class Quantizer(nn.Module):\n",
        "  def __init__(self,scale,zero_point,bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.scale = scale\n",
        "    self.zero = zero_point\n",
        "    self.store_scale = scale *64\n",
        "\n",
        "  def forward(self,x):\n",
        "    return do_requant(x,self.scale,self.zero)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUpiPDiu7RCH"
      },
      "source": [
        "# Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cBdXFnr5dZqT"
      },
      "outputs": [],
      "source": [
        "# add hook to record the min max value of the activation\n",
        "input_activation = {}\n",
        "output_activation = {}\n",
        "\n",
        "#Define a hook to record the feature map of each layer\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        input_activation[module_name] = x.detach()\n",
        "        output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (nn.Linear, nn.ReLU,nn.Conv2d,h_swish)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "\n",
        "\n",
        "    return all_hooks\n",
        "\n",
        "hooks = add_range_recoder_hook(FP32_model)\n",
        "sample_data = iter(train_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
        "FP32_model(sample_data) #Forward to use hook\n",
        "# print(FP32_model.Conv[0].weight)\n",
        "# print(output_activation[\"Conv.0\"])\n",
        "# print(output_activation.keys())\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVf8vpiVTsDa"
      },
      "source": [
        "# Quantize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SVh-SRj8eOrs"
      },
      "outputs": [],
      "source": [
        "#copy original model\n",
        "quantized_model = copy.deepcopy(FP32_model)\n",
        "\n",
        "#Record each layer in original model\n",
        "quantized_backbone = []\n",
        "quantized_Conv = []\n",
        "i = 0\n",
        "\n",
        "#Record input scale and zero point\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"Conv.0\"])\n",
        "preprocess = Preprocess(input_scale, input_zero_point)\n",
        "quantized_Conv.append(preprocess)\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['Conv.0'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['Conv.0'])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(FP32_model.Conv[0].weight.data)\n",
        "Conv_bias = FP32_model.Conv[0].bias.data\n",
        "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "h_swish1 = h_swish()\n",
        "#quantized_model.Conv[0] = quantizedConv1 \n",
        "\n",
        "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['Conv.1'])\n",
        "req1 = Quantizer(req_scale,output_zero_point)\n",
        "\n",
        "quantized_Conv.append(quantizedConv1)\n",
        "quantized_Conv.append(h_swish1)\n",
        "quantized_Conv.append(req1)\n",
        "\n",
        "\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['Conv.2'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['Conv.2'])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(FP32_model.Conv[2].weight.data)\n",
        "Conv_bias = FP32_model.Conv[2].bias.data\n",
        "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "#quantized_model.Conv[2] = quantizedConv2 \n",
        "\n",
        "h_swish2 = h_swish()\n",
        "#quantized_model.Conv[0] = quantizedConv1 \n",
        "\n",
        "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['Conv.3'])\n",
        "req2 = Quantizer(req_scale,output_zero_point)\n",
        "\n",
        "quantized_Conv.append(quantizedConv2)\n",
        "quantized_Conv.append(h_swish2)\n",
        "quantized_Conv.append(req2)\n",
        "\n",
        "\n",
        "#Record Linear + ReLU of the model (except the last Linear)\n",
        "while i < len(quantized_model.backbone) - 1:\n",
        "  if isinstance(quantized_model.backbone[i], nn.Linear) and isinstance(quantized_model.backbone[i+1], nn.ReLU):\n",
        "    linear = quantized_model.backbone[i]\n",
        "    linear_name = f\"backbone.{i}\"\n",
        "    relu = quantized_model.backbone[i + 1]\n",
        "    relu_name = f\"backbone.{i + 1}\"\n",
        "\n",
        "    #Use the calibration data to calculate scale and zero point of each layer\n",
        "    input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "    output_scale, output_zero_point = get_scale_and_zero_point(output_activation[relu_name])\n",
        "    quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "\n",
        "    quantizedLinear = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "\n",
        "    quantized_backbone.append(quantizedLinear)\n",
        "    i += 2\n",
        "\n",
        "#Record the last Linear layer\n",
        "linear = quantized_model.backbone[4]\n",
        "linear_name = f\"backbone.4\"\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation[linear_name])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "quantizedLinear = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "quantized_backbone.append(quantizedLinear)\n",
        "\n",
        "quantized_model.Conv = nn.Sequential(*quantized_Conv)\n",
        "quantized_model.backbone = nn.Sequential(*quantized_backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRB96PKbfNX4",
        "outputId": "58ce882a-3521-4b40-ff98-44dcea373546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (Conv): Sequential(\n",
            "    (0): Preprocess()\n",
            "    (1): QuantizedConv(in_channels=1, out_channels=5)\n",
            "    (2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): Quantizer()\n",
            "    (4): QuantizedConv(in_channels=5, out_channels=1)\n",
            "    (5): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Quantizer()\n",
            "  )\n",
            "  (backbone): Sequential(\n",
            "    (0): QuantizedLinear(in_channels=784, out_channels=120)\n",
            "    (1): QuantizedLinear(in_channels=120, out_channels=84)\n",
            "    (2): QuantizedLinear(in_channels=84, out_channels=10)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(quantized_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add hook to record the min max value of the activation\n",
        "q_input_activation = {}\n",
        "q_output_activation = {}\n",
        "\n",
        "#Define a hook to record the feature map of each layer\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        q_input_activation[module_name] = x.detach()\n",
        "        q_output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "\n",
        "\n",
        "    return all_hooks\n",
        "\n",
        "\n",
        "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
        "hooks = add_range_recoder_hook(quantized_model)\n",
        "sample_data = iter(q_test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
        "quantized_model(sample_data) #Forward to use hook\n",
        "\n",
        "\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 28, 28])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# print(q_input_activation[\"Conv.0\"].shape)\n",
        "print(q_input_activation[\"Conv.1\"].shape)\n",
        "# print(q_input_activation[\"Conv.2\"].shape)\n",
        "\n",
        "for i in range(28):\n",
        "    print(q_output_activation[\"Conv.1\"][0][0][0])\n",
        "#print(q_input_activation[\"Conv.1\"][0][0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1173502981.py, line 99)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 99\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"deq_scale (shift 13):\",DecToBin_machine(quantized_model.Conv[1].,8))\u001b[0m\n\u001b[1;37m                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def signed_dec2hex_matrix(input):\n",
        "    '''Convert a matrix which data is signed decimal to 8 bits hex with 2's complement'''\n",
        "    temp = []\n",
        "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
        "    for i in input:\n",
        "        test =bin8(i)\n",
        "        test = int(test,base=2)\n",
        "        hex_test = hex(test)[2:].zfill(2)\n",
        "        temp.append(hex_test)\n",
        "\n",
        "    return temp\n",
        "\n",
        "def signed_dec2hex(input):\n",
        "    '''Convert a number which data is signed decimal to 8 bits hex with 2's complement'''\n",
        "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
        "    test =bin8(input)\n",
        "    test = int(test,base=2)\n",
        "    hex_test = hex(test)[2:].zfill(2)\n",
        "\n",
        "    return hex_test\n",
        "\n",
        "\n",
        "def golden_gen(golden_layer_decimal):\n",
        "    '''Convert a layer output which is signed decimal in GPU to 8 bits hex with 2's complement in CPU and\n",
        "     make it 4 element in a line, example of use : golden_gen(q_output_activation[\"Conv.3\"]) '''\n",
        "    golden = []\n",
        "    i=0\n",
        "    golden_in_numpy = golden_layer_decimal.cpu().numpy()\n",
        "    test = golden_in_numpy.flatten()\n",
        "    test =test.astype('int32')\n",
        "    golden.append([])\n",
        "    for j, data in enumerate(test):\n",
        "        if(j%4==0 ):\n",
        "            golden.append([])\n",
        "            i = i+1\n",
        "            golden[i].append(signed_dec2hex(data))\n",
        "        if(j%4!=0):\n",
        "            golden[i].append(signed_dec2hex(data))\n",
        "    golden.pop(0)\n",
        "    for indice,data in enumerate(golden):\n",
        "        print(*data,sep='')\n",
        "\n",
        "def input_or_weight_gen(layer_decimal):\n",
        "    '''Convert a layer output which is signed decimal in GPU to 8 bits hex with 2's complement in CPU and\n",
        "     make each byte display in different place, example of use : input_or_weight_gen(quantized_model.Conv[1].weights)'''\n",
        "    byte0 = []\n",
        "    byte1 = []\n",
        "    byte2 = []\n",
        "    byte3 = []\n",
        "\n",
        "    data_in_numpy = layer_decimal.cpu().numpy()\n",
        "    data_test = data_in_numpy.flatten()\n",
        "    data_test = data_test.astype('int32')\n",
        "    data_test = signed_dec2hex_matrix(data_test)\n",
        "    for indice,data in enumerate(data_test):\n",
        "        if(indice%4 == 0):\n",
        "            byte0.append(data)\n",
        "        elif(indice%4 == 1):\n",
        "            byte1.append(data)\n",
        "        elif(indice%4 == 2):\n",
        "            byte2.append(data)\n",
        "        else:\n",
        "            byte3.append(data)\n",
        "    print(\"byte0:\",*byte0)\n",
        "    print(\"=======\")\n",
        "    print(\"byte1:\",*byte1)\n",
        "    print(\"=======\")\n",
        "    print(\"byte2:\",*byte2)\n",
        "    print(\"=======\")\n",
        "    print(\"byte3:\",*byte3)\n",
        "    print(\"=======\")\n",
        "    return byte0,byte1,byte2,byte3\n",
        "\n",
        "def DecToBin_machine(num,accuracy):\n",
        "    integer = int(num)\n",
        "    flo = num - integer\n",
        "    integercom = '{:1b}'.format(integer)\n",
        "    tem = flo\n",
        "    flo_list = []\n",
        "    for i in range(accuracy):\n",
        "        tem *= 2\n",
        "        flo_list += str(int(tem))\n",
        "        tem -= int(tem)\n",
        "    flocom = flo_list\n",
        "    binary_value =  ''.join(flocom)\n",
        "    return binary_value\n",
        "\n",
        "#golden_gen(q_output_activation[\"Conv.3\"])\n",
        "print(signed_dec2hex(quantized_model.Conv[1].input_zero_point))\n",
        "input_or_weight_gen(q_input_activation[\"Conv.1\"])\n",
        "print(\"===\")\n",
        "input_or_weight_gen(quantized_model.Conv[4].weights)\n",
        "\n",
        "print(quantized_model.Conv[4].input_zero_point)\n",
        "\n",
        "DeS = quantized_model.Conv[3].store_scale\n",
        "print(DeS)\n",
        "#print(DeS *8192) # 2**13\n",
        "print(\"deq_scale (shift 13):\",DecToBin_machine(quantized_model.Conv[1].,8))\n",
        "print(\"req_scale (shift 6):\",)\n",
        "print(\"output zero\",)\n",
        "#binary_value = integercom + '.' + ''.join(flocom)\n",
        "result = DecToBin_machine(DeS,8)\n",
        "print(result)\n",
        "# 0.1100111101011100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMccqTL6URaZ"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRMXzNeCgDuq",
        "outputId": "0fa65b9f-3a60-41e0-a935-a601a8db0484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.000745 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knTzO1mbheBe",
        "outputId": "411fc256-b7ea-4757-9f75-6acaae605324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.001798 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, quantized_model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(125440,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([4.04412003e+00, 1.98682810e+00, 7.53713004e-01, 7.04638213e-01,\n",
              "        4.62418827e-01, 3.56093954e-01, 3.47915118e-01, 4.13345809e-01,\n",
              "        4.63678200e-01, 4.85067913e-01, 6.48015499e-01, 4.88842760e-01,\n",
              "        5.94539191e-01, 8.25433335e-01, 1.07268711e+00, 6.23478990e-01,\n",
              "        4.45432013e-01, 8.94828668e+00, 8.49592359e+00, 4.07683777e-01,\n",
              "        4.39770000e-01, 4.77518217e-01, 5.46094936e-01, 7.03380344e-01,\n",
              "        5.25962249e-01, 4.91988756e-01, 3.68047746e-01, 3.29041073e-01,\n",
              "        3.94471716e-01, 4.37253370e-01, 4.64306422e-01, 6.48015785e-01,\n",
              "        5.12121213e-01, 4.27187168e-01, 3.70564312e-01, 3.84405537e-01,\n",
              "        3.86292962e-01, 3.66789355e-01, 9.13199078e+00, 4.15988447e+00,\n",
              "        4.46186983e+00, 4.93247329e-01, 4.68710240e-01, 8.36065822e+00,\n",
              "        3.17716711e-01, 2.95696393e-02, 2.01325440e-02, 1.88742378e-02,\n",
              "        1.76159553e-02, 3.33444869e-02, 2.51657096e-02, 1.44702490e-02,\n",
              "        1.13245427e-02, 2.39073679e-02, 3.14570631e-02, 2.89404980e-02,\n",
              "        1.95034250e-02, 2.20199442e-02, 2.64239330e-02, 2.20199442e-02,\n",
              "        2.26490854e-02, 5.41062757e-02, 3.46027694e-02, 4.02650407e-02,\n",
              "        2.57947917e-02, 2.70530742e-02, 3.83776169e-02, 3.27154225e-02,\n",
              "        1.44702490e-02, 4.08942782e-02, 3.90066665e-02, 4.78148483e-02,\n",
              "        2.45364515e-02, 3.46028508e-02, 4.78146234e-02, 3.46028508e-02,\n",
              "        1.57285685e-02, 5.91391395e-02, 4.08942782e-02, 1.82450537e-02,\n",
              "        5.72519894e-02, 6.48017023e-02, 1.88741935e-02, 6.98348443e-02,\n",
              "        1.63576343e-02, 3.90068500e-02, 4.52982774e-02, 5.03311825e-02,\n",
              "        1.69868540e-02, 7.42384942e-02, 2.01325677e-02, 3.64902790e-02,\n",
              "        3.71192471e-02, 0.00000000e+00, 2.57947311e-02, 2.32782814e-02,\n",
              "        3.96359927e-02, 1.32119354e-02, 2.39074242e-02, 2.13907526e-02,\n",
              "        1.76159968e-02, 0.00000000e+00, 2.07616128e-02, 2.89405661e-02,\n",
              "        1.38410752e-02, 1.57285685e-02, 1.69867741e-02, 1.69868540e-02,\n",
              "        0.00000000e+00, 1.00662839e-02, 1.63575574e-02, 1.57285685e-02,\n",
              "        1.69868540e-02, 1.95034250e-02, 0.00000000e+00, 1.06953260e-02,\n",
              "        1.32119976e-02, 1.76159968e-02, 1.25828548e-02, 0.00000000e+00,\n",
              "        1.44701469e-02, 1.25828548e-02, 1.38411403e-02, 1.00662839e-02,\n",
              "        0.00000000e+00, 1.38410101e-02, 8.17885564e-03, 1.38411403e-02,\n",
              "        0.00000000e+00, 8.17885564e-03, 2.01323783e-02, 8.80799838e-03,\n",
              "        1.38411403e-02, 0.00000000e+00, 1.63577113e-02, 1.57284206e-02,\n",
              "        9.43714112e-03, 0.00000000e+00, 1.82451395e-02, 5.03314193e-03,\n",
              "        1.13244628e-02, 0.00000000e+00, 8.17885564e-03, 1.19537121e-02,\n",
              "        0.00000000e+00, 1.19535996e-02, 1.63577113e-02, 1.00662839e-02,\n",
              "        0.00000000e+00, 1.06953260e-02, 6.29142741e-03, 0.00000000e+00,\n",
              "        1.57285685e-02, 1.32119976e-02, 8.17877869e-03, 0.00000000e+00,\n",
              "        8.80799838e-03, 1.06954266e-02, 0.00000000e+00, 8.80791551e-03,\n",
              "        5.66228467e-03, 0.00000000e+00, 1.32119976e-02, 9.43714112e-03,\n",
              "        0.00000000e+00, 1.25828548e-02, 1.50994258e-02, 1.88742822e-02,\n",
              "        0.00000000e+00, 2.26489256e-02, 4.84439911e-02, 0.00000000e+00,\n",
              "        4.65565628e-02, 7.54971289e-02, 0.00000000e+00, 1.32749118e-01,\n",
              "        7.93286082e+00]),\n",
              " array([-0.37498987, -0.36231872, -0.34964758, -0.33697647, -0.32430533,\n",
              "        -0.31163418, -0.29896304, -0.2862919 , -0.27362075, -0.26094964,\n",
              "        -0.2482785 , -0.23560736, -0.22293621, -0.21026509, -0.19759394,\n",
              "        -0.18492281, -0.17225167, -0.15958053, -0.1469094 , -0.13423826,\n",
              "        -0.12156712, -0.10889599, -0.09622484, -0.08355371, -0.07088257,\n",
              "        -0.05821143, -0.0455403 , -0.03286916, -0.02019802, -0.00752689,\n",
              "         0.00514425,  0.01781539,  0.03048653,  0.04315766,  0.0558288 ,\n",
              "         0.06849994,  0.08117107,  0.09384221,  0.10651335,  0.11918449,\n",
              "         0.13185562,  0.14452676,  0.15719789,  0.16986904,  0.18254018,\n",
              "         0.19521131,  0.20788245,  0.22055358,  0.23322472,  0.24589586,\n",
              "         0.25856701,  0.27123812,  0.28390926,  0.2965804 ,  0.30925155,\n",
              "         0.32192269,  0.33459383,  0.34726495,  0.35993609,  0.37260723,\n",
              "         0.38527837,  0.39794952,  0.41062063,  0.42329177,  0.43596292,\n",
              "         0.44863406,  0.4613052 ,  0.47397634,  0.48664746,  0.4993186 ,\n",
              "         0.51198971,  0.52466089,  0.537332  ,  0.55000317,  0.56267428,\n",
              "         0.57534546,  0.58801657,  0.60068768,  0.61335886,  0.62602997,\n",
              "         0.63870114,  0.65137225,  0.66404337,  0.67671454,  0.68938565,\n",
              "         0.70205683,  0.71472794,  0.72739905,  0.74007022,  0.75274134,\n",
              "         0.76541251,  0.77808362,  0.79075474,  0.80342591,  0.81609702,\n",
              "         0.82876819,  0.84143931,  0.85411042,  0.86678159,  0.87945271,\n",
              "         0.89212388,  0.90479499,  0.9174661 ,  0.93013728,  0.94280839,\n",
              "         0.95547956,  0.96815068,  0.98082185,  0.99349296,  1.00616407,\n",
              "         1.01883519,  1.03150642,  1.04417753,  1.05684865,  1.06951976,\n",
              "         1.08219087,  1.0948621 ,  1.10753322,  1.12020433,  1.13287544,\n",
              "         1.14554656,  1.15821779,  1.1708889 ,  1.18356001,  1.19623113,\n",
              "         1.20890224,  1.22157347,  1.23424459,  1.2469157 ,  1.25958681,\n",
              "         1.27225792,  1.28492916,  1.29760027,  1.31027138,  1.3229425 ,\n",
              "         1.33561361,  1.34828484,  1.36095595,  1.37362707,  1.38629818,\n",
              "         1.39896929,  1.41164052,  1.42431164,  1.43698275,  1.44965386,\n",
              "         1.46232498,  1.47499621,  1.48766732,  1.50033844,  1.51300955,\n",
              "         1.52568078,  1.53835189,  1.55102301,  1.56369412,  1.57636523,\n",
              "         1.58903646,  1.60170758,  1.61437869,  1.6270498 ,  1.63972092,\n",
              "         1.65239215,  1.66506326,  1.67773438,  1.69040549,  1.7030766 ,\n",
              "         1.71574783,  1.72841895,  1.74109006,  1.75376117,  1.76643229,\n",
              "         1.77910352,  1.79177463,  1.80444574,  1.81711686,  1.82978797,\n",
              "         1.8424592 ,  1.85513031,  1.86780143]),\n",
              " <BarContainer object of 177 artists>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#y = torch.flatten(FP32_model.Conv[0].weight)\n",
        "y = torch.flatten(output_activation['Conv.1'])\n",
        "y = y.cpu()\n",
        "y = torch.flatten(y)\n",
        "y = y.detach()\n",
        "y = y.numpy()\n",
        "print(y.shape)\n",
        "\n",
        "plt.hist(y, bins='auto',density=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          ...,\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.]],\n",
            "\n",
            "         [[-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          ...,\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.]],\n",
            "\n",
            "         [[104., 104., 104.,  ..., 104., 104., 104.],\n",
            "          [104., 104., 104.,  ..., 104., 104., 104.],\n",
            "          [104., 104., 104.,  ..., 104., 104., 104.],\n",
            "          ...,\n",
            "          [104., 104., 104.,  ..., 104., 104., 104.],\n",
            "          [104., 104., 104.,  ..., 104., 104., 104.],\n",
            "          [104., 104., 104.,  ..., 104., 104., 104.]],\n",
            "\n",
            "         [[-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          ...,\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.]],\n",
            "\n",
            "         [[-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          ...,\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.],\n",
            "          [-85., -85., -85.,  ..., -85., -85., -85.]]]], device='cuda:0')\n",
            "(3920,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([0.00427671, 0.        , 0.0457383 , 0.        , 0.        ,\n",
              "        0.        , 0.0005102 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.00829832, 0.        ]),\n",
              " array([-127., -110.,  -93.,  -76.,  -59.,  -42.,  -25.,   -8.,    9.,\n",
              "          26.,   43.,   60.,   77.,   94.,  111.,  128.]),\n",
              " <BarContainer object of 15 artists>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#x = torch.flatten(quantized_weights12)\n",
        "#x = torch.flatten(quantized_model.Conv[1].weights)\n",
        "print(q_output_activation['Conv.3'])\n",
        "x = torch.flatten(q_output_activation['Conv.3'])\n",
        "x = x.cpu()\n",
        "x = torch.flatten(x)\n",
        "x = x.detach()\n",
        "x = x.numpy()\n",
        "print(x.shape)\n",
        "\n",
        "plt.hist(x, bins='auto',range=(-127,128),density=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
