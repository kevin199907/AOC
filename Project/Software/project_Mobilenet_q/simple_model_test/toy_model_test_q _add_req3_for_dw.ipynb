{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHXk_L8g3fDh"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tVH9mlCdXrkw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\胡家豪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "no_cuda = False\n",
        "use_gpu = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) / 6\n",
        "\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(channel, _make_divisible(channel // reduction, 8),bias=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(_make_divisible(channel // reduction, 8), channel,bias=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk1U6PcMXtDB",
        "outputId": "58b165e2-e66b-41c5-987d-4fcc1b6a26fa"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#Dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzMtMm9n3hsZ"
      },
      "source": [
        "Create NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-pKE7xJbOc7",
        "outputId": "fce122fd-5aa8-4d0c-fe2c-94bb60cc64ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (Conv): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1), groups=8)\n",
            "    (3): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (4): SELayer(\n",
            "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=8, out_features=8, bias=False)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=8, out_features=8, bias=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (6): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (backbone): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=120, bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=120, out_features=84, bias=False)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=84, out_features=10, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=1, out_channels=8, kernel_size=1, stride=1,padding= 0, bias=True),\n",
        "      h_swish(inplace=True),\n",
        "      nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=3,padding= 1, bias=True,groups=8),\n",
        "      h_swish(inplace=True),\n",
        "      SELayer(8),\n",
        "      nn.Conv2d(in_channels=8, out_channels=1, kernel_size=1, stride=1,padding= 0, bias=True),\n",
        "      h_swish(inplace=True)\n",
        "    )  \n",
        "    self.backbone = nn.Sequential(\n",
        "      nn.Linear(10*10, 120, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(120, 84, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(84, 10, bias=False)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.Conv(x)\n",
        "    x = x.view(-1, 10*10) #transform 28*28 figure to 784 vector\n",
        "    x = self.backbone(x)\n",
        "    return x\n",
        "\n",
        "FP32_model = ToyModel()\n",
        "print(FP32_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MOyeqSPDbvr5"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  #Set the model to train mode\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    if use_gpu:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    #forward\n",
        "    pred = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #backward\n",
        "    loss.backward()\n",
        "\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  #set model to evaluate mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      if use_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "      pred = model(x)\n",
        "      test_loss = loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LsfzIw4b1AU",
        "outputId": "03aeb700-d0fe-4e88-a24e-d000b63c4184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (Conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1), groups=8)\n",
              "    (3): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (4): SELayer(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=8, out_features=8, bias=False)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Linear(in_features=8, out_features=8, bias=False)\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (6): h_swish(\n",
              "      (sigmoid): h_sigmoid(\n",
              "        (relu): ReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (backbone): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=120, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=120, out_features=84, bias=False)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=84, out_features=10, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 3\n",
        "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
        "optimizer = torch.optim.Adam(FP32_model.parameters(), lr=learning_rate)  #define optimizer\n",
        "\n",
        "FP32_model.to(device) #let model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LH6kt0eqb9tl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302275  [   32/60000]\n",
            "loss: 2.305182  [ 3232/60000]\n",
            "loss: 2.302027  [ 6432/60000]\n",
            "loss: 2.300669  [ 9632/60000]\n",
            "loss: 1.515746  [12832/60000]\n",
            "loss: 0.522350  [16032/60000]\n",
            "loss: 0.803070  [19232/60000]\n",
            "loss: 0.556386  [22432/60000]\n",
            "loss: 0.557975  [25632/60000]\n",
            "loss: 0.888240  [28832/60000]\n",
            "loss: 0.619301  [32032/60000]\n",
            "loss: 0.606474  [35232/60000]\n",
            "loss: 0.608921  [38432/60000]\n",
            "loss: 0.436622  [41632/60000]\n",
            "loss: 0.229272  [44832/60000]\n",
            "loss: 0.572817  [48032/60000]\n",
            "loss: 0.324114  [51232/60000]\n",
            "loss: 0.457973  [54432/60000]\n",
            "loss: 0.312358  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.000894 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.696731  [   32/60000]\n",
            "loss: 0.440497  [ 3232/60000]\n",
            "loss: 0.398180  [ 6432/60000]\n",
            "loss: 0.169042  [ 9632/60000]\n",
            "loss: 0.509739  [12832/60000]\n",
            "loss: 0.631651  [16032/60000]\n",
            "loss: 0.272428  [19232/60000]\n",
            "loss: 0.314141  [22432/60000]\n",
            "loss: 0.417821  [25632/60000]\n",
            "loss: 0.296751  [28832/60000]\n",
            "loss: 0.357853  [32032/60000]\n",
            "loss: 0.534621  [35232/60000]\n",
            "loss: 0.616099  [38432/60000]\n",
            "loss: 0.579593  [41632/60000]\n",
            "loss: 0.310808  [44832/60000]\n",
            "loss: 0.575475  [48032/60000]\n",
            "loss: 0.375286  [51232/60000]\n",
            "loss: 0.426302  [54432/60000]\n",
            "loss: 0.420260  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.000843 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.371540  [   32/60000]\n",
            "loss: 0.338747  [ 3232/60000]\n",
            "loss: 0.474884  [ 6432/60000]\n",
            "loss: 0.296151  [ 9632/60000]\n",
            "loss: 0.545188  [12832/60000]\n",
            "loss: 0.629703  [16032/60000]\n",
            "loss: 0.308186  [19232/60000]\n",
            "loss: 0.502664  [22432/60000]\n",
            "loss: 0.290154  [25632/60000]\n",
            "loss: 0.410907  [28832/60000]\n",
            "loss: 0.479340  [32032/60000]\n",
            "loss: 0.347319  [35232/60000]\n",
            "loss: 0.315419  [38432/60000]\n",
            "loss: 0.332640  [41632/60000]\n",
            "loss: 0.219846  [44832/60000]\n",
            "loss: 0.685363  [48032/60000]\n",
            "loss: 0.514745  [51232/60000]\n",
            "loss: 0.409230  [54432/60000]\n",
            "loss: 0.175660  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 0.000792 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, FP32_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAK3-fH3qGh"
      },
      "source": [
        "# Quantization definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lWzxrde4G5i"
      },
      "source": [
        "####Question 1.####\n",
        "\n",
        "Use\n",
        ">$S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})$\n",
        "\n",
        ">$Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S$\n",
        "\n",
        "to calculate scale factor and zero point of a tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kJIr-5SpcgQr"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero_point(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -(2**(bitwidth-1)), 2**(bitwidth-1) - 1\n",
        "  fp_min = fp32_tensor.min().item()\n",
        "  fp_max = fp32_tensor.max().item()\n",
        "  \n",
        "  #####################################################\n",
        "\n",
        "  scale = (fp_max-fp_min) / (q_max-q_min)\n",
        "  zero_point = q_min-fp_min /scale\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "\n",
        "  zero_point = round(zero_point)          #round\n",
        "  zero_point = max(q_min, min(zero_point, q_max)) #clip\n",
        "\n",
        "  return scale, int(zero_point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YA7ano5nS1"
      },
      "source": [
        "####Question 2.####\n",
        "\n",
        "Use $q=r/S + Z$ to quantize a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sBMKB5Le54wr"
      },
      "outputs": [],
      "source": [
        "def linear_quantize(fp32_tensor, bitwidth=8):\n",
        "  q_min, q_max = -(2**(bitwidth-1)), 2**(bitwidth-1) - 1\n",
        "\n",
        "  scale, zero_point = get_scale_and_zero_point(fp32_tensor)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  q_tensor = torch.round( fp32_tensor/scale ) +zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp\n",
        "  q_tensor = torch.clamp(q_tensor, q_min, q_max)\n",
        "  return q_tensor, scale, zero_point  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taJXSmz6KDS"
      },
      "source": [
        "####Question 3.####\n",
        "\n",
        "Use\n",
        "> $q_{\\mathrm{output}} = M * \\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Z_{\\mathrm{output}}$\n",
        "\n",
        "> $M = S_{\\mathrm{input}} * S_{\\mathrm{weight}} / S_{\\mathrm{output}}$\n",
        "\n",
        "to compute quantized linear operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sbXY0vaCcn7l"
      },
      "outputs": [],
      "source": [
        "def quantized_linear(input, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=8):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale / output_scale\n",
        "  output = torch.nn.functional.linear((input - input_zero_point ), (weights - weight_zero_point))\n",
        "  output *= M\n",
        "  output += output_zero_point\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.round().clamp(-(2**(activation_bitwidth-1)), 2**(activation_bitwidth-1)-1)\n",
        "\n",
        "  return output\n",
        "\n",
        "def quantized_conv(input, bias,weights,stride, padding,groups,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, device, bitwidth=8, activation_bitwidth=16):\n",
        "  input, weights = input.to(device), weights.to(device)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "  M = input_scale * weight_scale \n",
        "  conv_bias = bias /M\n",
        "  conv_bias = conv_bias.round()\n",
        "  output_only_conv = torch.nn.functional.conv2d((input - input_zero_point ), (weights - weight_zero_point),bias = conv_bias ,stride=stride,padding=padding,groups=groups)\n",
        "  output = M * output_only_conv\n",
        "  #output += output_zero_point\n",
        "  #####################################################\n",
        "\n",
        "  #clamp and round\n",
        "  output = output.clamp(-(2**(activation_bitwidth-1)), 2**(activation_bitwidth-1)-1)\n",
        "  return output\n",
        "\n",
        "def do_requant(input, scale,zero_point,bitwidth=8):\n",
        "    output = input / scale\n",
        "    output = output.round()\n",
        "    output += zero_point\n",
        "    output = output.round().clamp(-(2**(bitwidth-1)), 2**(bitwidth-1)-1)\n",
        "    return output\n",
        "\n",
        "def do_fake_quant(input, deq_scale, q_scale, q_zero_point,bitwidth=8):\n",
        "    M = deq_scale/q_scale\n",
        "    N = q_zero_point\n",
        "    output = input * M\n",
        "    output += N\n",
        "    output = output.round().clamp(-(2**(bitwidth-1)), 2**(bitwidth-1)-1)\n",
        "    return output\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK10k10R7II7"
      },
      "source": [
        "# Design quantized linear layer and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IwrXNVKadKfG"
      },
      "outputs": [],
      "source": [
        "class Q_SELayer(nn.Module):\n",
        "  def __init__(self,weights1,input_scale1,weight_scale1,output_scale1, input_zero_point1, weight_zero_point1, output_zero_point1,\n",
        "               weights2, input_scale2, weight_scale2, output_scale2, input_zero_point2, weight_zero_point2, output_zero_point2,\n",
        "               input_SE_scale,in_SE_zero_point,\n",
        "               output_SE_scale,output_SE_zero_point,\n",
        "               out_pool_scale,out_pool_zero_point):\n",
        "    super().__init__()\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc = nn.Sequential(\n",
        "        QuantizedLinear(weights1,input_scale1,weight_scale1,output_scale1, input_zero_point1, weight_zero_point1, output_zero_point1),\n",
        "        QuantizedLinear(weights2, input_scale2, weight_scale2, output_scale2, input_zero_point2, weight_zero_point2, output_zero_point2)\n",
        "    )   \n",
        "    self.input_SE_scale, self.in_SE_zero_point = input_SE_scale,in_SE_zero_point\n",
        "    self.output_SE_scale, self.output_SE_zero_point = output_SE_scale, output_SE_zero_point,\n",
        "    self.out_pool_scale, self.out_pool_zero_point = out_pool_scale,out_pool_zero_point\n",
        "    self.linear_out_scale, self.linear_out_zero_point = output_scale2, output_zero_point2\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b, c, _, _ = x.size()\n",
        "    y = self.avg_pool(x).view(b, c)\n",
        "    y = (y- self.in_SE_zero_point) \n",
        "    y = do_fake_quant(y, self.input_SE_scale, self.out_pool_scale, self.out_pool_zero_point,bitwidth=8)\n",
        "    y = self.fc(y).view(b, c, 1, 1)\n",
        "    z = (x-self.in_SE_zero_point)*(y-self.linear_out_zero_point)\n",
        "    return do_fake_quant(z, deq_scale=(self.input_SE_scale *self.linear_out_scale), \n",
        "                         q_scale=self.output_SE_scale, q_zero_point=self.output_SE_zero_point,bitwidth=8)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"Quantized_SE(in_channels={self.fc[0].weights.size(1)}, out_channels={self.fc[1].weights.size(0)})\"\n",
        "    \n",
        "\n",
        "\n",
        "class QuantizedConv(nn.Module):\n",
        "  def __init__(self,bias ,weights,stride,padding,groups ,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.stride, self.padding, self.groups = stride, padding,groups\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "    self.bias = bias\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "    self.q_bias = torch.round(bias / (input_scale*weight_scale))\n",
        "    self.q_weight = weights - weight_zero_point\n",
        "    self.DeQ_scale = input_scale*weight_scale*8192\n",
        "  def forward(self, x):\n",
        "    return quantized_conv(x, self.bias, self.weights, self.stride, self.padding, self.groups, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedConv(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "  def __init__(self, weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point, bitwidth=8, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.weights = weights\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.weight_scale, self.weight_zero_point = weight_scale, weight_zero_point\n",
        "    self.output_scale, self.output_zero_point = output_scale, output_zero_point\n",
        "\n",
        "    self.bitwidth = bitwidth\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "\n",
        "  def forward(self, x):\n",
        "    return quantized_linear(x, self.weights, self.input_scale, self.weight_scale, self.output_scale, self.input_zero_point, self.weight_zero_point, self.output_zero_point, device)\n",
        "  def __repr__(self):\n",
        "    return f\"QuantizedLinear(in_channels={self.weights.size(1)}, out_channels={self.weights.size(0)})\"\n",
        "\n",
        "#Transform input data to correct integer range\n",
        "class Preprocess(nn.Module):\n",
        "  def __init__(self, input_scale, input_zero_point, activation_bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.input_scale, self.input_zero_point = input_scale, input_zero_point\n",
        "    self.activation_bitwidth = activation_bitwidth\n",
        "  def forward(self, x):\n",
        "    x = x / self.input_scale + self.input_zero_point\n",
        "    x = x.round() \n",
        "    return x\n",
        "  \n",
        "class Quantizer(nn.Module):\n",
        "  def __init__(self,scale,zero_point,bitwidth=8):\n",
        "    super().__init__()\n",
        "    self.scale = scale\n",
        "    self.zero = zero_point\n",
        "    self.store_scale = scale *64\n",
        "\n",
        "  def forward(self,x):\n",
        "    return do_requant(x,self.scale,self.zero)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUpiPDiu7RCH"
      },
      "source": [
        "# Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cBdXFnr5dZqT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Conv.0', 'Conv.1', 'Conv.2', 'Conv.3', 'Conv.4.avg_pool', 'Conv.4.fc.0', 'Conv.4.fc.1', 'Conv.4.fc.2', 'Conv.4.fc.3', 'Conv.5', 'Conv.6', 'backbone.0', 'backbone.1', 'backbone.2', 'backbone.3', 'backbone.4'])\n"
          ]
        }
      ],
      "source": [
        "# add hook to record the min max value of the activation\n",
        "input_activation = {}\n",
        "output_activation = {}\n",
        "\n",
        "#Define a hook to record the feature map of each layer\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        input_activation[module_name] = x.detach()\n",
        "        output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (nn.Linear, nn.ReLU,nn.Conv2d,h_swish,nn.AdaptiveAvgPool2d)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "\n",
        "\n",
        "    return all_hooks\n",
        "\n",
        "hooks = add_range_recoder_hook(FP32_model)\n",
        "sample_data = iter(train_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
        "FP32_model(sample_data) #Forward to use hook\n",
        "# print(output_activation['Conv.1'])\n",
        "# print(\"==\")\n",
        "# print(input_activation['Conv.2.avg_pool'])\n",
        "print(output_activation.keys())\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVf8vpiVTsDa"
      },
      "source": [
        "# Quantize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SVh-SRj8eOrs"
      },
      "outputs": [],
      "source": [
        "#copy original model\n",
        "quantized_model = copy.deepcopy(FP32_model)\n",
        "\n",
        "#Record each layer in original model\n",
        "quantized_backbone = []\n",
        "quantized_Conv = []\n",
        "i = 0\n",
        "\n",
        "#Record input scale and zero point\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[\"Conv.0\"])\n",
        "preprocess = Preprocess(input_scale, input_zero_point)\n",
        "quantized_Conv.append(preprocess)\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['Conv.0'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['Conv.0'])\n",
        "quantized_weights12, weight_scale, weight_zero_point = linear_quantize(FP32_model.Conv[0].weight.data)\n",
        "Conv_bias = FP32_model.Conv[0].bias.data\n",
        "quantizedConv1 = QuantizedConv(Conv_bias,quantized_weights12, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "h_swish1 = h_swish()\n",
        "#quantized_model.Conv[0] = quantizedConv1 \n",
        "\n",
        "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['Conv.1'])\n",
        "req1 = Quantizer(req_scale,output_zero_point)\n",
        "\n",
        "quantized_Conv.append(quantizedConv1)\n",
        "quantized_Conv.append(h_swish1)\n",
        "quantized_Conv.append(req1)\n",
        "#####################\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['Conv.2'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['Conv.2'])\n",
        "quantized_weights12, weight_scale, weight_zero_point = linear_quantize(FP32_model.Conv[2].weight.data)\n",
        "Conv_bias = FP32_model.Conv[2].bias.data\n",
        "quantizedConv2 = QuantizedConv(Conv_bias,quantized_weights12, 3,1,8,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "h_swish2 = h_swish()\n",
        "#quantized_model.Conv[0] = quantizedConv1 \n",
        "\n",
        "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['Conv.3'])\n",
        "req2 = Quantizer(req_scale,output_zero_point)\n",
        "\n",
        "quantized_Conv.append(quantizedConv2)\n",
        "quantized_Conv.append(h_swish2)\n",
        "quantized_Conv.append(req2)\n",
        "###############\n",
        "input_scale1, input_zero_point1 = get_scale_and_zero_point(input_activation['Conv.4.fc.0'])\n",
        "output_scale1, output_zero_point1 = get_scale_and_zero_point(output_activation['Conv.4.fc.1'])\n",
        "quantized_weights1, weight_scale1, weight_zero_point1 = linear_quantize(FP32_model.Conv[4].fc[0].weight.data)\n",
        "\n",
        "input_scale2, input_zero_point2 = get_scale_and_zero_point(input_activation['Conv.4.fc.2'])\n",
        "output_scale2, output_zero_point2 = get_scale_and_zero_point(output_activation['Conv.4.fc.3'])\n",
        "quantized_weights2, weight_scale2, weight_zero_point2 = linear_quantize(FP32_model.Conv[4].fc[2].weight.data)\n",
        "\n",
        "SE_in_scale, SE_in_zero_point = get_scale_and_zero_point(output_activation['Conv.3'])\n",
        "\n",
        "SE_out_scale, SE_out_zero_point = get_scale_and_zero_point(input_activation['Conv.5'])\n",
        "\n",
        "SE_out_pool_scale, SE_out_pool_zero_point = get_scale_and_zero_point(output_activation['Conv.4.avg_pool'])\n",
        "\n",
        "quantizedSE_linear1 =Q_SELayer(quantized_weights1,input_scale1,weight_scale1,output_scale1,input_zero_point1,weight_zero_point1,output_zero_point1, \n",
        "                               quantized_weights2,input_scale2,weight_scale2,output_scale2,input_zero_point2,weight_zero_point2,output_zero_point2,\n",
        "                               SE_in_scale, SE_in_zero_point,\n",
        "                               SE_out_scale, SE_out_zero_point,\n",
        "                               SE_out_pool_scale, SE_out_pool_zero_point)\n",
        "##################\n",
        "quantized_Conv.append(quantizedSE_linear1)\n",
        "\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['Conv.5'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['Conv.5'])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(FP32_model.Conv[5].weight.data)\n",
        "Conv_bias = FP32_model.Conv[5].bias.data\n",
        "quantizedConv3 = QuantizedConv(Conv_bias,quantized_weights, 1,0,1,input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "#quantized_model.Conv[2] = quantizedConv2 \n",
        "\n",
        "h_swish3 = h_swish()\n",
        "#quantized_model.Conv[0] = quantizedConv1 \n",
        "\n",
        "req_scale , output_zero_point = get_scale_and_zero_point(output_activation['Conv.6'])\n",
        "req3 = Quantizer(req_scale,output_zero_point)\n",
        "\n",
        "quantized_Conv.append(quantizedConv3)\n",
        "quantized_Conv.append(h_swish3)\n",
        "quantized_Conv.append(req3)\n",
        "\n",
        "\n",
        "################# below is linear\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['backbone.0'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['backbone.1'])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(FP32_model.backbone[0].weight.data)\n",
        "quantizedLinear1 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "quantized_backbone.append(quantizedLinear1)\n",
        "\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation['backbone.2'])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation['backbone.3'])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(FP32_model.backbone[2].weight.data)\n",
        "quantizedLinear2 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "quantized_backbone.append(quantizedLinear2)\n",
        "\n",
        "# #Record Linear + ReLU of the model (except the last Linear)\n",
        "# while i < len(quantized_model.backbone) - 1:\n",
        "#   if isinstance(quantized_model.backbone[i], nn.Linear) and isinstance(quantized_model.backbone[i+1], nn.ReLU):\n",
        "#     linear = quantized_model.backbone[i]\n",
        "#     linear_name = f\"backbone.{i}\"\n",
        "#     relu = quantized_model.backbone[i + 1]\n",
        "#     relu_name = f\"backbone.{i + 1}\"\n",
        "\n",
        "#     #Use the calibration data to calculate scale and zero point of each layer\n",
        "#     input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "#     output_scale, output_zero_point = get_scale_and_zero_point(output_activation[relu_name])\n",
        "#     quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "\n",
        "#     quantizedLinear = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "\n",
        "#     quantized_backbone.append(quantizedLinear)\n",
        "#     i += 2\n",
        "\n",
        "#Record the last Linear layer\n",
        "linear = quantized_model.backbone[4]\n",
        "linear_name = f\"backbone.4\"\n",
        "input_scale, input_zero_point = get_scale_and_zero_point(input_activation[linear_name])\n",
        "output_scale, output_zero_point = get_scale_and_zero_point(output_activation[linear_name])\n",
        "quantized_weights, weight_scale, weight_zero_point = linear_quantize(linear.weight.data)\n",
        "quantizedLinear3 = QuantizedLinear(quantized_weights, input_scale, weight_scale, output_scale, input_zero_point, weight_zero_point, output_zero_point)\n",
        "quantized_backbone.append(quantizedLinear3)\n",
        "\n",
        "quantized_model.Conv = nn.Sequential(*quantized_Conv)\n",
        "quantized_model.backbone = nn.Sequential(*quantized_backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRB96PKbfNX4",
        "outputId": "58ce882a-3521-4b40-ff98-44dcea373546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (Conv): Sequential(\n",
            "    (0): Preprocess()\n",
            "    (1): QuantizedConv(in_channels=1, out_channels=8)\n",
            "    (2): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): Quantizer()\n",
            "    (4): QuantizedConv(in_channels=1, out_channels=8)\n",
            "    (5): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Quantizer()\n",
            "    (7): Quantized_SE(in_channels=8, out_channels=8)\n",
            "    (8): QuantizedConv(in_channels=8, out_channels=1)\n",
            "    (9): h_swish(\n",
            "      (sigmoid): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (10): Quantizer()\n",
            "  )\n",
            "  (backbone): Sequential(\n",
            "    (0): QuantizedLinear(in_channels=100, out_channels=120)\n",
            "    (1): QuantizedLinear(in_channels=120, out_channels=84)\n",
            "    (2): QuantizedLinear(in_channels=84, out_channels=10)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(quantized_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 8, 28, 28])\n",
            "dict_keys(['Conv.0', 'Conv.1', 'Conv.2', 'Conv.3', 'Conv.4', 'Conv.5', 'Conv.6', 'Conv.7.avg_pool', 'Conv.7.fc.0', 'Conv.7.fc.1', 'Conv.7', 'Conv.8', 'Conv.9', 'Conv.10', 'backbone.0', 'backbone.1', 'backbone.2'])\n"
          ]
        }
      ],
      "source": [
        "# add hook to record the min max value of the activation\n",
        "q_input_activation = {}\n",
        "q_output_activation = {}\n",
        "\n",
        "#Define a hook to record the feature map of each layer\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        q_input_activation[module_name] = x.detach()\n",
        "        q_output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (QuantizedConv,  QuantizedLinear,h_swish,Quantizer,Preprocess,Q_SELayer,nn.AdaptiveAvgPool2d)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "\n",
        "\n",
        "    return all_hooks\n",
        "\n",
        "\n",
        "q_test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
        "hooks = add_range_recoder_hook(quantized_model)\n",
        "sample_data = iter(test_loader).__next__()[0].to(device) #Use a batch of training data to calibrate\n",
        "quantized_model(sample_data) #Forward to use hook\n",
        "print(q_input_activation[\"Conv.4\"].shape)\n",
        "print(q_output_activation.keys())\n",
        "#print(q_intput_activation.keys())\n",
        "#print(quantized_model.Conv[4].weights2)\n",
        "#print(q_output_activation[\"Conv.4\"])\n",
        "\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00\n",
            "byte0: 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 81 81 81 81 81 9c 81 81 81 81 81 81 d9 81 de 81 81 81 81 02 1e 29 81 81 81 83 03 20 18 81 81 83 81 eb 1d 1e 81 82 81 81 df 18 20 81 81 82 e3 ef 16 1f 81 82 81 f6 09 1b 14 84 81 b6 e8 10 1b 0b 81 c2 01 f7 06 1c 2d 81 f0 f4 10 1b 23 1a c7 e3 e3 f8 1d 44 46 91 3c 08 13 50 75 77 81 c4 48 50 89 32 23 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 b2 81 81 81 81 f9 6d 77 c9 81 81 81 7c 6e 66 7b 81 81 81 64 66 68 60 81 81 81 67 67 64 6b 81 81 81 7f 64 6e 5c 81 81 81 7f 6d 56 7f 81 81 87 7f 7f b0 7f 81 81 a0 7f d5 a0 7f 81 81 b6 7a 96 6c 73 81 81 d1 70 a8 58 7f 81 81 e6 74 50 7f 7f 81 81 ff 78 70 67 7e 84 81 14 36 66 6b 50 a4 81 24 d7 6b 6a fe b8 81 36 c6 6a 69 c7 ba 81 46 ef 6a 6a c6 ce 81 48 12 6a 6a b0 e3 81 4c 2e 6b 6b 94 ef 81 44 56 6b 6c b8 f9 81 40 68 6b 6c 07 0a 81 3e 6e 6b 6c 27 0f 81 38 77 69 69 60 18 81 32 6e 6e 6d 30 1e 81 3c 5a 5e 60 1b 3c 81 03 81 81 81 81 08 81 cd 81 81 81 81 fc 81 b2 81 83 83 81 c4 81 81 82 02 13 81 81 81 81 83 7f 7f 81 81 81 81 81 5a 5a 81 81 81 81 81 67 65 81 81 81 81 81 65 63 81 81 81 81 81 6a 64 81 81 81 81 81 6b 63 81 81 81 81 81 67 67 81 81 81 81 81 73 69 81 81 81 81 81 7f 6c 81 81 81 81 81 7f 7e 81 81 81 81 81 7f 78 81 81 81 81 81 7b 7b 81 81 81 81 81 58 7d 81 81 81 81 81 2d 79 81 81 81 81 81 0c 7f 81 81 81 81 81 f2 7f 81 81 81 81 81 18 7f 81 81 81 81 81 46 7f 81 81 81 81 81 78 7c 81 81 81 81 81 62 6f 81 81 81 81 81 6d 6a 81 81 81 81 81 74 52 81 81 81 81 82 75 46 81 81 81 81 83 71 4a 81 81 81 81 85 6c 1f 81 81 81 81 82 7f 22 81 81 81 81 81 20 c5 81 81 81 81 81 e4 d7 81 81 81 81 81 4e 58 81 81 81 81 81 4e 65 81 81 81 81 81 54 66 81 81 81 81 81 52 4e 81 81 81 81 81 38 3e 81 81 81 81 81 ea 26 81 81 81 81 81 c8 09 81 81 81 81 81 d6 0b 81 81 81 81 81 f0 14 81 81 81 81 81 e5 1d 81 81 81 81 81 e4 23 81 81 81 81 81 ea 24 81 81 81 81 81 f4 19 81 81 81 81 81 f6 20 81 81 81 81 81 fa 29 81 81 81 81 81 fe 2e 81 81 81 81 81 2c 48 81 81 81 81 81 75 5a 81 81 81 81 81 52 68 81 81 81 81 81 54 6c 81 81 81 81 81 50 69 81 81 81 81 81 48 6a 81 81 81 81 81 4a 68 81 81 81 81 81 46 65 81 81 81 81 81 44 58 81 81 81 81 81 40 60 81 81 81 81 81 e6 13 81 81 81 81 81 c4 d4 81 81 81 82 8e 23 2d 9f 81 81 82 0c 23 0c 2c 81 81 81 ed e5 0b fd 81 81 81 f2 d2 f0 05 81 81 81 e6 e2 f4 13 81 81 81 eb f0 f1 17 81 81 81 f1 eb d4 0d 87 81 98 e9 e8 d5 14 b4 81 ba e8 ee eb 1d d6 81 d7 05 ec f5 36 fc 81 f1 13 ec e8 28 2b 81 fd da ed ec f8 34 81 04 b0 eb e8 d2 42 81 09 96 e7 e7 ae 4e 81 0a 9a df e2 94 4c 81 0e 8d e1 df a0 4e 81 02 81 e6 e7 95 4c 81 ff 96 ed f0 b2 25 81 07 ca f0 f3 e2 30 81 0f d5 f1 e7 01 25 81 81 f8 e1 eb 38 81 81 81 0a eb e9 42 81 81 82 05 0d eb 28 81 81 81 f7 ee ec 22 82 81 81 15 d4 e9 29 83 81 81 ff 24 1d 23 81 81 81 81 e0 eb 81 81 81 81 81 36 3c 81 81 81 81 81 56 56 81 81 81 81 81 54 5a 81 81 81 81 81 52 54 b4 81 81 81 81 4e 52 ea 81 81 81 81 54 58 04 81 81 81 9a 52 5c fd 81 81 81 b2 4a 66 f5 81 81 81 d8 48 75 ee 81 81 81 00 58 7e e8 81 81 81 24 72 7f e5 81 81 81 32 77 7d e2 81 81 81 2d 60 6a eb 81 81 81 1b 18 52 ed 81 81 81 0e c7 32 09 81 81 81 08 9d 07 f8 81 81 81 db 85 bc fc 81 81 81 84 c7 95 0a 81 81 81 81 24 81 02 81 81 81 81 5a 81 0b 81 81 81 81 4a 81 0d 81 81 81 81 5a 81 fc 81 81 81 81 60 81 03 81 81 81 82 5a 81 05 81 81 81 84 52 81 17 81 81 81 82 4e 81 06 81 81 81 81 6e 81 1d 81 81 81 81 0a 81 be 81 81 81 82 81 ae 81 81 81 81 81 75 6f 81 81 81 81 81 7e 3c 82 81 81 81 81 36 a6 81 81 81 82 c9 81 81 9a 81 81 81 c7 b8 9e b4 81 81 81 d3 a0 b2 9a 81 81 81 04 95 b6 9c 81 81 81 1b 8d a2 a2 81 81 81 0a 8b 9e ae 81 81 81 13 8a a0 9e 81 81 81 2a 90 9e 9d 81 81 81 38 8e 99 a8 81 81 81 5e 91 9b ae 81 81 81 7f 8c 9b ca 81 81 89 7f 87 9a e6 81 81 8c 7f 93 9c 0b 81 81 8c 7e 8e 9a 32 81 81 91 7f 91 97 36 81 81 97 7e 9d 9c 4c 81 81 98 5a 99 9c 56 81 81 99 3e a2 9b 5a 81 81 99 1e 9f 98 63 81 81 a0 09 a8 9d 72 81 81 a4 d5 ba aa 5a 81 81 a4 81 81 81 65 81 81 c3 81 81 81 f8 81 81 b6 81 81 81 a4 81 81 81 83 df e3 81 81 81 81 81 7f 7b 82 81 81 81 81 b6 e0 81 81 81 81 04 be b0 be 81 81 81 f0 1b ff 02 81 81 81 a6 e2 dc 92 81 81 81 32 6a 4c fc 81 81 81 d4 05 03 fc 81 81 81 ba 00 fc 96 81 81 81 de 20 1e d5 81 81 81 eb 2c 32 f7 81 81 ae ef 32 1f f1 81 81 b8 c0 ea d6 50 81 81 8f 13 44 40 4e 81 81 b4 16 60 42 25 81 81 d9 c4 e4 d4 32 91 81 0c c0 23 04 14 a6 81 d3 ae 24 06 08 90 81 03 c8 42 1d e2 a0 81 4e 95 0c ed c7 d6 81 c3 a4 f4 d1 cd 96 81 e5 e8 75 52 e8 a6 81 f0 94 0e f3 81 ae 81 fd cf e5 e5 9a b6 81 d0 dc f3 1c a8 94 81 b2 0b f5 25 a8 83 81 85 ac e2 2f 81 82 81 81 81 9e d5 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 b6 17 81 81 81 82 84 52 4a b8 87 81 82 81 d9 18 65 81 81 81 81 81 81 4c d7 ef 9d ae cb 10 d9 13 07 91 fb 03 ba 81 8f 90 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 f4 81 81 81 81 82 a2 f1 81 de 81 81 81 fe f4 f6 48 81 81 be d3 08 5e 40 81 81 3e f0 01 e2 ea a6 fd e2 ec 2d 3a f1 0f 32 e9 3c 61 13 52 ba 38 28 4c d6 81 98 90 86 0f 81 81 9a 8d 81 c5 a2 a8 ba d7 ac 81 a4 b8 be ac bc ba 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 3e 3c 81 81 81 81 81 50 2e 81 81 81 81 1f 48 40 ff 81 81 84 17 6f 74 18 83 81 81 07 71 2e 01 81 81 81 13 32 07 05 81 81 81 1a 40 27 05 81 81 81 0a 58 44 2e 81 81 81 0d 50 2a 3c 81 81 81 34 44 1a 75 81 81 81 13 30 1f da 81 81 81 b8 3c 20 c8 81 81 81 c1 44 13 ea 8b 81 87 b6 28 1b de a2 81 aa 9e 27 2e e2 b0 81 ba 86 36 34 cd be 81 d1 8b 32 2f ca c8 81 e5 b4 3c 24 ca d0 81 ee ca 38 2c ca d6 81 ee f5 36 23 e2 e0 81 ec ea 4c 28 e4 e4 81 ee 81 ec 02 81 e2 81 ed 81 81 81 81 e4 81 e6 81 81 81 81 ea 81 fa 81 82 81 81 e5 81 fd 81 81 81 81 d8 81 fb 81 81 81 81 cc 81 9a 81 81 81 81 bc 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 9b 92 84 81 81 81 82 cb dc fa 81 81 81 81 81 e7 22 81 81 81 81 c8 25 2a 82 84 85 81 ce 19 01 81 84 81 e1 ba 2d 9f 82 81 85 aa ff 11 81 81 84 1d 81 e9 27 8d 81 81 19 aa 44 2c de 82 8f 32 a2 60 fc 38 81 90 62 f1 62 e7 c8 ba 28 52 32 62 0d 52 8c ec 04 19 c6 2a 1c 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 17 82 81 81 81 8a 81 01 81 81 81 81 81 81 fb 8e 82 81 8c 9e a2 3a 07 81 86 b8 f7 3c f3 f7 ca 81 c8 48 03 ed ed 07 c6 bc 07 ff f5 ef e9 ff 0b ff 0d 0d df d7 f7 2e 1b 11 1d 0d 01 1f 25 48 3c 38 2c 29 40 92 0f 23 23 2b 1b ed 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 b0 d0 81 81 81 81 81 5a 50 81 81 81 81 81 4c 46 81 81 81 81 81 5c 4e 81 81 81 81 81 63 56 81 81 81 81 81 60 5c 81 81 81 81 81 5c 4a 81 81 81 81 81 52 4a 81 81 81 81 81 4c 48 81 81 81 81 81 4a 46 81 81 81 81 81 4c 46 81 81 81 81 81 48 46 81 81 81 81 81 4c 48 81 81 81 81 81 50 4a 81 81 81 81 81 54 4e 81 81 81 81 81 5a 50 81 81 81 81 81 5e 52 81 81 81 81 81 62 56 81 81 81 81 81 64 56 81 81 81 81 81 66 54 81 81 81 81 81 67 52 81 81 81 81 81 67 4c 81 81 81 81 81 6a 4a 81 81 81 81 81 61 42 81 81 81 81 81 70 66 81 81 81 81 81 09 b2 81 81 81 81 81 e6 d2 81 81 81 82 86 78 7f 85 81 81 83 81 5e 69 81 81 81 81 cb 5e 65 83 81 81 83 7c 6a 64 67 81 81 83 5e 70 6e 68 82 81 81 62 6b 70 63 81 81 81 66 67 67 67 81 81 81 69 69 6d 66 81 81 81 6c 64 69 67 81 81 81 68 66 63 66 81 81 81 6a 68 66 66 81 81 81 6d 67 66 68 8f 81 81 6d 67 6a 6b b6 81 9c 6d 66 6e 70 d7 81 c3 6f 67 66 74 f0 81 e5 70 67 69 72 05 81 fb 72 67 68 70 17 81 1d 76 66 6a 5e 2c 81 32 73 69 6a 28 3e 81 3e 48 69 6a 28 42 81 4c 21 69 6a 16 44 81 4e 0f 69 6b 04 44 81 46 14 69 6b fe 36 81 42 1f 6b 6d f0 3a 81 36 1a 68 6a fe 26 81 81 3c 6d 67 2c 81 81 81 38 67 64 04 81 81 81 81 81 81 81 81 81 81 81 73 71 81 81 81 81 81 48 40 81 81 81 81 81 40 4a 81 81 81 81 81 40 44 81 81 81 81 81 2f 36 81 81 81 81 81 30 2f 81 81 81 81 81 29 3e 81 81 81 81 81 03 1d 81 81 81 81 81 fc f3 81 81 81 81 81 ea ec 81 81 81 81 81 e3 e6 81 81 81 81 81 ee e3 81 81 81 81 81 f3 ea 81 81 81 81 81 fc ef 81 81 81 81 81 04 ef 81 81 81 81 81 fc f2 81 81 81 81 81 fc ee 81 81 81 81 81 04 e2 81 81 81 81 81 23 04 81 81 81 81 81 2f 32 81 81 81 81 81 38 42 81 81 81 81 81 38 48 81 81 81 81 81 3a 4c 81 81 81 81 81 3a 4e 81 81 81 81 81 2f 4a 81 81 81 81 81 46 56 81 81 81 81 81 e6 04 81 81 81 81 81 e8 3c 81 81 81 81 b0 f5 44 dc 81 81 81 f8 24 1b 24 81 81 81 ec f5 ef fb 81 81 81 fe e8 dc f8 81 81 8a 05 d6 df f5 90 81 b2 02 d2 e2 f5 bc 81 d2 0e e2 cc e5 c9 81 d6 24 cf e2 df dc 81 dc 4c d6 df ef ef 81 f2 50 d9 c3 cf fb 81 05 18 d2 c6 c6 f8 81 08 14 cc c3 c6 f2 81 0b fb cf bc c6 f8 81 0e fb df cc d6 ef 81 0e fe e8 d2 dc e8 81 0b 14 e2 dc dc ef 81 08 30 e2 d9 e5 e2 81 05 3a e2 d9 e8 e2 81 02 3c e8 d9 ec e2 81 fb 2e fe ec f8 e2 81 fb 0e f2 dc f8 e5 81 f5 05 df c3 d9 e5 81 ef dc 81 81 81 e8 81 ec bc 90 90 d2 e5 81 f8 87 81 81 50 ef 81 fb 81 b8 d9 81 e8 81 05 81 a6 d6 81 f2 81 81 81 d1 df 81 81 81 81 81 62 7f 81 81 81 81 81 19 44 81 81 81 81 b8 a8 36 df 81 81 81 25 81 7f 7f 81 81 d0 38 97 17 db 94 81 e0 0b f2 00 96 e3 81 c7 26 34 7f 7f 06 81 ea 46 7f 5a 2d 02 81 27 52 4a 68 81 96 81 7f 76 d0 5c d9 81 81 7f 6c bc fe 32 ba 81 60 12 17 48 70 19 81 1f fa 22 6c 19 7f 81 1c 0d db 7f a8 6c 81 44 e5 cc 2b ac 40 81 46 d8 d8 ef 08 2a 81 7f e7 e2 a6 08 e3 81 7c ed 05 81 f5 a6 81 62 fe 3e 8f 17 d4 81 50 0d 3a b6 62 0d 81 46 3c 2c fe 58 29 81 5e 4a 10 15 62 3e 81 46 32 0a dd 6b 60 81 3a 2f 2d cc 40 5e 81 0e 2a 20 ee 7f 74 81 36 89 f8 0b a2 4a 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 81 85 c2 81 81 81 81 81 81 29 32 34 2b 13 81 81 28 3a 3e 42 3e 81 a0 2c 30 36 3e 3a 81 c1 34 34 36 38 3a 81 a0 3a 38 32 38 34 81 81 32 34 30 3a 32 81 81 34 34 30 38 34 81 86 38 34 38 38 32 82 db 3c 34 38 30 34 81 b2 2d 26 30 2d 24 82 81 44 36 38 38 3c 81 81 32 3c 3a 38 3a 81 94 0e 38 36 36 34 81 7f 60 4c 32 30 32 81 a2 44 42 52 4e 48 b2 81 e6 ff 92 94 93 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 9c 10 81 81 81 81 3a 6d 4a d8 81 81 81 3e 48 3a 42 81 81 81 3c 40 40 32 81 81 b0 42 3c 42 42 81 81 42 3e 36 34 3c 81 81 5c 3e 52 63 3c 81 81 6b 3e 22 50 3a b0 81 73 65 22 30 36 f4 81 ec 54 5e 54 3a 81 81 81 3a 46 54 52 81 81 83 48 3e 54 44 81 81 83 46 42 64 44 81 81 82 46 44 4a 38 81 81 83 46 46 40 38 81 81 83 46 4a 46 38 81 81 83 48 4a 46 38 81 81 83 4a 4a 46 3a 81 81 83 4c 4c 46 40 81 81 83 4a 4a 4a 4a 81 81 84 48 4c 4a 4c 81 81 83 46 4a 4a 46 81 81 83 46 48 48 42 81 81 83 42 46 46 44 81 81 83 44 44 46 4c 81 81 83 44 46 46 48 81 81 84 4c 46 46 5e 81 81 83 34 4a 4e 15 81 81 81 99 4e 3a 81 81 81 81 72 54 0c 54 81 81 81 50 56 6a 54 81 81 06 4e 3a 61 5e 9e 81 56 52 44 60 48 3a 81 54 54 44 42 68 48 81 4e 54 4a 42 5c 52 81 3e 40 3e 3c 40 3e 81 30 40 3a 44 48 34 81 2e 4a 34 40 40 42 81 40 3c 48 4c 4e 27 81 34 4a 46 2e 4e 10 81 28 4c 48 30 44 19 81 32 32 22 30 40 1f 81 3a 2e 32 30 44 1e 81 34 32 25 24 54 1a 81 46 28 36 28 48 12 81 2d 1b 36 29 3c 10 81 23 28 34 2c 4c 07 81 30 24 11 1d 24 0c 81 28 0f 08 0a 20 fc 81 24 1a 14 1a 1d ff 81 1b 0d 0c 1f 29 0b 81 16 25 1d 1d 42 0a 81 10 20 0b 1d 36 12 81 10 1f 1b 0d 44 0b 81 22 38 28 36 54 12 81 0a 23 11 1a 28 03 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 89 81 81 81 81 81 de ca 81 81 81 81 81 46 26 81 16 82 83 81 18 90 f0 21 81 81 ba f0 e8 3e fd 81 94 60 07 46 d3 11 82 36 1a 3a 24 5e 2f ba 29 06 3a 1b 12 64 d3 1e 2c 1e 15 aa f0 89 25 02 d8 48 f4 c7 81 5c 78 30 7b 16 36 82 81 16 ea a2 d9 24 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 82 81 82 83 81 81 81 83 3a 84 81 81 81 81 81 40 81 d2 81 81 81 5a 4a 81 2c 83 82 81 52 40 48 2a 84 82 23 4e 5c 3c 58 81 81 56 4e 5c 5e 1d 81 28 2b 48 6b 61 60 81 3a 16 32 70 66 68 be 4e 30 30 48 40 40 ee 44 46 44 46 4a 4a 81 77 75 6f 68 6c 64 81 b8 d0 b2 9c 99 82 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 32 84 ce 81 81 81 81 30 48 4e 81 81 81 81 52 50 5a 81 81 81 1e 9b 30 cd 81 81 82 67 81 81 81 81 88 81 5c 85 81 76 81 81 a2 73 81 b6 30 81 c3 3e 44 81 6e 60 81 0a d4 3e 79 6d 52 81 1b fa 52 4e 50 54 f1 52 4e 60 5c 4e 56 ac 4e 50 52 25 58 50 81 2a 78 78 81 6e 7c 81 81 b2 81 81 e0 aa 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 30 2d 81 81 81 81 81 21 3a 81 81 81 81 81 1a 38 81 81 81 81 81 19 2b 81 81 81 81 81 21 2d 81 81 81 81 81 28 28 81 81 81 81 81 2e 1d 81 81 81 81 81 56 20 81 81 81 81 81 75 1c 81 81 81 81 81 71 19 81 81 81 81 81 7b 26 81 81 81 81 81 7c 29 81 81 81 81 81 7f 2c 81 81 81 81 81 77 30 81 81 81 81 81 71 2f 81 81 81 81 81 62 30 81 81 81 81 81 4a 32 81 81 81 81 81 32 38 81 81 81 81 81 20 3c 81 81 81 81 81 17 3c 81 81 81 81 81 1f 3e 81 81 81 81 81 14 3e 81 81 81 81 81 0b 3e 81 81 81 81 81 fb 3a 81 81 81 81 81 e5 3c 81 81 81 81 81 cb 40 81 81 81 81 81 be 4a 81 81 81 81 81 9b 17 81 81 81 81 81 81 81 81 81 81 81 ac 34 34 a0 81 81 81 ba 54 56 ca 81 81 81 a4 82 89 a4 81 81 81 ac b6 b8 a8 81 81 81 b2 b6 b4 aa 81 81 81 b2 a8 b8 ae 81 81 88 b6 ae a6 ae 81 81 94 b4 b4 b0 b6 81 81 a2 be b0 b4 dc 81 81 b4 be a4 b6 03 81 81 be ae ae be 16 81 81 c3 9e b2 be 1d 81 81 c7 8f b2 c7 22 82 81 ca 84 b4 d3 28 8b 81 bc 86 ba df 16 8d 81 a6 89 ba ec ee 88 81 a8 99 a6 ff b4 8f 81 be a6 aa 14 b4 96 81 ba 90 b2 ff b4 a2 81 b0 9d b4 14 a2 aa 81 b6 be b4 1d 92 ae 81 ac ba c9 24 84 b4 81 c0 ef c2 1d d7 b8 81 c9 c9 2d c9 b0 c2 81 a4 81 c9 81 81 c7 81 97 81 9b 81 81 90 81 81 81 81 81 81 81 81 81 81 eb d9 82 81 81 81 81 7f 7f 81 81 81 82 81 46 42 81 81 81 81 42 5c 5a 3c 81 81 82 44 52 58 44 82 81 81 3c 36 38 38 81 81 81 42 40 44 3e 81 81 81 46 3e 3e 46 81 81 81 4c 34 32 4a 81 81 81 4a 38 36 6c 81 81 81 6c 3a 3a 7f 81 81 81 68 38 3e 56 81 81 81 38 3a 40 06 81 81 81 df 42 40 be 93 81 81 81 42 44 81 d8 81 81 81 3c 3c 81 15 81 8f 81 40 36 81 2d 81 94 81 42 40 81 30 81 ac 81 3c 3c d2 34 81 c3 b6 3e 3c 28 17 81 c3 11 46 42 46 f9 81 c5 32 44 3e 4c 18 81 d3 3c 3e 3e 46 19 81 b4 3c 40 3e 3a fd 81 81 52 3c 3a 4e e2 81 81 14 4e 4c 40 dd 81 81 81 d0 de 81 e8 81 81 81 81 81 81 81 81 81 81 8a e4 81 81 81 81 7f 7f 72 81 81 81 81 61 69 6a 81 81 81 81 58 6c 6c 81 81 81 81 7f 69 65 81 81 81 81 7f 6a 69 81 81 81 81 7f 6a 69 81 81 81 81 7f 68 69 99 81 81 81 78 67 67 ac 81 81 81 77 67 68 c4 81 81 81 74 69 67 cc 81 81 81 7f 6b 65 b8 81 81 81 7f 6c 65 83 81 81 81 7f 6d 65 81 81 81 81 7f 6c 64 81 81 81 81 7f 6d 65 83 81 81 81 7f 6e 66 95 81 81 81 7f 6f 65 a0 81 81 81 7f 70 66 be 81 81 81 7f 71 67 d8 81 81 81 7f 72 67 e9 81 81 81 7f 72 6a f6 81 81 81 7f 72 6f 01 81 81 81 7f 71 71 13 81 81 81 7f 71 74 28 81 81 81 7b 6b 6b 4e 81 81 81 7f 7f 7f ec 81 81 81 c2 1a 1d 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 9d 81 81 81 81 81 de 4e a6 81 81 81 81 5a 05 5e 81 81 81 81 08 c8 f7 81 81 81 81 36 07 dc 81 81 81 81 25 fc fb 82 81 81 81 38 27 3a 82 82 84 38 2f 29 40 82 85 81 0c f8 1e 2b 81 81 81 1c 06 22 1c 81 f4 2f 2f 36 1f 34 81 0a 17 02 2b 12 23 a8 d6 16 02 21 17 22 1a 3a 05 25 1a 25 3c c8 36 e1 2c 32 4c 48 81 44 19 21 46 3e 3a 81 48 44 3c 52 4a 4e 81 81 5e 6f 81 29 d4 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 b4 a8 81 81 81 81 83 60 46 81 81 81 81 81 4c 40 81 81 81 81 ba 46 48 85 81 81 81 36 42 44 e5 81 81 81 40 44 40 2b 81 81 81 46 44 3e 48 81 81 81 48 46 42 4c 81 81 81 48 44 3e 4c 81 81 81 46 44 40 50 81 81 81 46 40 3e 50 81 81 81 46 40 3e 4c 81 81 81 46 40 3e 48 81 81 81 48 40 3e 48 81 81 81 48 44 3e 4e 81 81 81 48 46 3e 4c 81 81 81 46 46 46 4c 81 81 81 48 46 46 4e 81 81 81 4e 46 46 50 81 81 81 46 44 46 5a 81 81 81 f5 42 46 e3 81 81 81 3c 40 46 e2 81 81 81 58 3e 40 44 81 81 81 52 40 40 58 81 81 81 52 3c 3e 50 81 81 81 54 3c 4a 4e 81 81 81 5a 5a 52 52 81 81 81 96 fe 96 94 81 81 81 81 81 81 81 81 81 81 81 81 da 82 81 81 81 81 81 34 83 81 81 81 81 11 81 81 81 81 81 81 48 81 81 81 81 81 81 d7 81 b4 81 81 81 81 81 81 dc 81 81 86 84 81 83 10 82 81 81 81 81 81 1c 81 81 b0 81 81 81 58 81 81 5c 3e 12 3c 2f 1b 81 3e 2f 40 38 46 52 81 2a 38 36 3e 06 4c 81 44 34 36 3e e7 42 81 63 3e 30 4a 20 44 81 6e 54 4a 56 fe 56 81 7a 5c 46 4a 38 52 81 7f 58 48 4e ff 5e 81 7f 58 4c 58 34 62 81 7f 58 54 4c 62 54 81 7f 58 5c 48 50 5c 81 7f 5c 58 52 56 5e 81 6c 5a 5c 5e 5e 60 81 64 5c 62 63 64 67 81 77 61 5a 5c 5e 62 81 71 63 6c 67 65 69 81 73 7f 56 58 52 38 81 b8 d9 81 81 81 81 81 81 81 81 be 81 81 81 81 81 83 2e 81 81 81 81 81 ea 87 83 81 81 81 81 d8 81 87 81 81 81 81 99 81 81 81 81 81 82 81 88 81 81 81 81 85 81 83 81 81 81 81 81 81 81 c3 81 81 81 81 84 81 08 81 81 81 81 83 81 22 81 81 81 81 81 81 01 81 81 81 b0 81 81 c7 81 81 81 f4 81 81 91 81 81 81 ff 81 81 81 81 81 81 10 83 84 81 81 81 81 ba 81 81 81 81 81 81 81 81 81 83 81 81 81 34 34 38 3a 81 81 81 5a 48 60 48 81 81 81 56 5a ae 50 81 81 81 5a 76 81 54 81 81 81 67 6f a8 58 81 81 81 62 61 7f 60 81 81 81 56 54 5a 5c 81 81 81 60 58 5a 58 81 81 81 3c 54 5c 56 81 81 81 58 72 7d 46 81 81 81 02 11 81 81 81\n",
            "=======\n",
            "byte1: 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 a6 81 81 81 82 d5 81 f8 81 81 81 82 10 81 eb 81 81 81 85 f9 27 0d 81 81 81 81 01 28 11 81 81 82 81 0a 26 1f 81 81 81 da 1a 2a 2a 81 81 81 09 23 20 2a 81 83 81 e4 1d 1d 1d 81 81 dd f4 1b 18 17 81 cd fc 00 09 22 22 c5 f3 00 ff 10 23 23 2a e5 e7 f9 28 46 3e ff 38 f9 3a 7f 7c 5c 81 eb 52 3e 81 50 1f 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 a6 81 81 81 81 52 7f 6e 81 81 81 81 73 74 68 42 81 81 e7 63 66 68 7c 81 81 69 7f 63 66 65 81 81 78 16 7f 68 6a 81 81 7f 90 40 7f 6a 81 81 7f 32 81 c2 6c 81 81 7f 4c 81 81 6c 81 81 7f 67 81 b6 6e 81 81 7f 7f 1e df 6d 81 81 7f 73 7f 66 70 81 81 7f 69 68 7b 73 81 81 7f 73 6a 64 7f 81 81 7f 7f 6c 68 7f 81 81 7e 7f 6b 6b 7f 81 81 77 7f 6a 6a 7f 81 81 76 7f 6b 6d 7f 81 81 73 7f 6c 6f 63 81 81 70 7a 6c 6d 4c 81 81 73 70 6c 6b 2c 81 81 7b 69 6c 6b fe 81 81 7c 68 69 6a 01 81 81 73 71 6d 70 b4 81 81 70 7f 5e 5e a0 81 81 75 81 81 81 a2 81 81 7f 81 81 81 a8 81 81 4e 85 83 83 84 81 81 81 81 1a 0e 81 81 81 81 81 7f 75 81 81 81 81 81 64 5c 81 81 81 81 81 69 5e 81 81 81 81 98 69 5e 81 81 81 81 c4 67 60 81 81 81 81 f3 6e 61 81 81 81 81 28 7a 60 81 81 81 81 50 61 62 81 81 81 81 60 22 60 81 81 81 81 75 c9 5a 81 81 81 81 7f 88 5c 81 81 81 81 7f 81 5e 81 81 81 81 7f 81 5c 81 81 81 81 7f 81 5e 81 81 81 81 7f 81 63 81 81 81 81 63 81 66 81 81 81 81 4c 81 63 81 81 81 81 18 81 6a 81 81 81 81 da 81 6d 81 81 81 81 a0 81 6e 81 81 81 81 81 a2 6e 81 81 81 81 81 c5 71 81 81 81 81 81 01 71 81 81 81 81 81 54 70 81 81 81 81 81 52 77 81 81 81 81 81 77 7f 81 81 81 81 81 c8 30 81 81 81 81 96 e4 dd 81 81 81 81 eb 6f 50 81 81 81 81 00 56 4a 81 81 81 81 13 64 56 81 81 81 81 f4 5a 60 81 81 81 81 f3 5a 4e 81 81 81 81 f1 3e 0c 81 81 81 81 c8 14 ed 81 81 81 81 d2 14 e5 81 81 81 81 c7 18 eb 81 81 81 81 be 0c e2 81 81 81 81 ba 00 d7 81 81 81 81 a6 f8 cf 81 81 81 81 8e fd c1 81 81 81 81 84 0f ba 81 81 81 81 81 fa be 81 81 81 81 81 ed cd 81 81 81 81 81 c5 b2 81 81 81 81 81 89 1d 81 81 81 81 82 81 54 81 81 81 81 81 81 62 81 81 81 81 81 81 67 81 81 81 81 81 81 69 81 81 81 81 81 81 6b 81 81 81 81 81 81 6f 81 81 81 81 81 81 6d 81 81 81 81 81 81 78 81 81 81 81 81 81 13 81 81 81 82 81 ca be 81 81 81 82 d0 22 36 81 81 81 81 0f 1b 02 22 81 81 81 e8 34 d6 1b 81 81 9e ed 13 13 12 82 81 c2 ed cb 17 19 81 81 ed e3 c9 f4 21 81 81 08 d4 f4 ee 42 81 81 08 cf ed f3 52 81 81 fd e1 f0 f2 44 81 81 f5 e7 e3 f1 3a 81 81 f3 e5 e1 f9 3c 81 81 f7 e6 e8 00 44 81 81 f4 ed f8 fc 4e 81 81 f7 ee 02 f7 58 8d 81 f6 f2 f9 f4 56 a4 81 ee fd f6 f0 40 ba 81 e7 f9 f5 f3 13 c7 81 f0 05 eb f5 f1 d5 81 07 fc e7 ee e3 08 81 2a ec ee e1 b4 f0 81 8d e7 f5 eb 9b 81 81 81 e6 07 f0 cb 81 81 84 e6 ed f8 08 81 81 81 ec e2 00 20 81 81 84 f1 fe f4 32 81 81 83 38 2c 2f be 81 81 81 81 ee e6 81 81 81 81 81 3e 30 81 81 81 81 8f 60 4c 81 81 81 81 97 5a 4c 81 81 81 81 ea 5c 50 81 81 81 81 4c 58 48 81 81 81 81 74 5a 42 81 81 81 81 77 54 48 81 81 81 81 6c 5c 4e 81 81 81 81 68 75 48 81 81 81 81 5e 61 46 81 81 81 81 4e 13 4a 81 81 81 81 46 a2 4c 81 81 81 81 3a 81 4c 81 81 81 81 32 81 4a 81 81 81 81 2c 81 4c 81 81 81 81 34 81 52 81 81 81 81 3e 81 74 81 81 81 81 3c 81 58 81 81 81 81 36 81 6e 81 81 81 81 42 81 76 81 81 81 81 08 81 58 81 81 81 81 9c a2 4a 81 81 81 81 81 d9 44 81 81 81 81 81 28 38 81 81 81 81 81 44 2a 81 81 81 81 81 5c 1c 81 81 81 81 81 6a 29 81 81 81 81 81 05 d2 81 81 81 81 81 a4 81 81 81 81 81 82 7e 0b 81 81 81 81 d9 0b a0 81 81 81 81 d5 52 81 81 81 81 81 dc 70 a8 83 81 81 81 b6 c1 9d 95 81 81 81 b2 d5 c0 9e 81 81 8d a6 db be a8 81 81 9d ba cd 94 ac 81 81 ac c2 d5 9d 9e 81 81 b6 d6 d5 99 9e 81 81 be e7 d1 9b 9c 81 81 c4 cf cc 9d 8e 81 81 cb c7 cc 91 88 81 81 ca ba cb 87 81 81 81 cc c4 d0 81 81 81 81 cc ba cd 85 81 81 81 cb b2 ca 86 81 81 81 cc a4 c4 8f 81 81 81 cf 96 cb 99 81 81 81 ce 95 c7 97 81 81 81 cc 96 c9 a0 84 81 81 d2 93 c8 9b 8e 81 81 cf b4 cc a2 91 81 81 c7 b4 c1 a8 ae 81 81 c6 81 81 81 f0 81 81 d5 81 81 81 cf 81 81 c8 81 81 81 9f 81 81 81 84 18 81 81 81 81 81 81 46 84 81 81 81 81 a8 38 c2 81 81 81 81 1d 11 b6 95 81 81 81 dc e8 b8 f6 81 81 ae a2 81 81 c2 81 81 e1 1e f0 c6 d3 81 81 23 c8 ae b2 d3 81 81 30 c3 89 be e9 81 81 21 e7 a8 99 23 81 81 ee fd b8 b4 ef 81 81 46 ee be a6 a6 81 81 5e 9a 86 81 1a 81 81 d8 04 c8 c5 d0 81 81 fe 07 f3 ca b8 81 81 f6 99 8a 81 ea 81 81 fa db b2 96 1b 81 81 9b ec b6 9a 22 81 81 ae 08 d2 b0 0d 81 81 04 c6 a6 82 6f 81 81 8a cb 8c 81 0c 81 81 ba 3a 05 eb 21 81 81 d0 c3 ac 91 13 81 81 ec d2 ac 81 06 81 81 98 d7 f1 a0 c8 81 81 db f1 f3 ac c1 81 81 1f e5 05 aa d7 81 81 cd 81 e7 81 a4 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 c0 ba 81 81 81 82 81 5c d9 81 81 81 84 b2 a6 3c 44 81 81 81 7e 81 81 52 e9 f0 c3 0f d3 28 b4 05 09 cb e3 fe 9a 81 8b 91 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 be 81 81 81 81 81 81 36 81 f6 81 81 81 6c 24 81 4c 81 82 87 f7 fb a6 48 81 81 3e ef 09 56 08 81 a4 c2 f4 09 09 db fe 02 f8 f3 36 14 2b 3c 24 ec 4c 61 48 52 fd 3c 48 3e e9 81 8d 9c 8e c8 81 81 98 98 9e b4 81 a8 b6 a8 b6 81 ae ca bc ae aa ae 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 e2 cd 83 81 81 81 c1 73 23 81 81 81 81 44 60 2b 97 81 81 81 0e 65 12 30 81 81 81 03 7f 0f 1a 81 81 92 16 48 12 02 81 81 b2 17 30 1a 0e 81 81 ce 18 36 1c 17 81 81 e9 05 30 14 23 81 81 f1 14 40 0f 4a 81 81 ff 1e 44 18 5e 81 81 1e 30 56 12 54 81 81 38 0b 48 03 61 81 81 3c 16 3c 0e 50 81 81 36 13 50 1b 42 81 81 30 2e 50 1c 40 81 81 34 36 38 0b 3a 81 81 26 42 44 02 40 81 81 1f 34 50 1a 44 81 81 24 32 3e 13 22 81 81 1f 0a 48 22 27 81 81 1b a2 16 fd f3 81 81 1b 81 81 81 de 81 81 1a 84 81 81 d1 81 81 1e 84 81 81 c4 81 81 1a 84 81 82 ba 81 81 26 86 81 81 b6 81 81 cc 82 81 81 9e 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 87 81 81 81 81 81 cd 36 94 81 81 81 81 d3 1a 90 81 81 81 81 32 24 dc 82 81 8a fa 9a 06 30 81 82 81 1a 81 f1 b4 82 81 ec 9f 81 22 81 81 81 2a a2 81 2a 81 82 81 b4 95 81 4a 81 85 b2 3c 82 c3 42 84 81 01 8f 81 40 42 84 22 24 a8 f1 52 32 20 be f6 2c 22 ae 19 11 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 8a 84 81 23 81 81 81 2c 81 81 07 81 82 81 48 8c 03 eb e5 81 84 48 f1 42 e1 0b 96 81 5a 36 dd eb ed ff b0 7f 0f 0b fb f5 e9 f5 65 e9 11 09 df dd 0f 44 19 13 17 09 0d 27 7b 52 3e 34 27 36 44 07 09 27 2c 25 17 c6 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 ae 81 81 81 81 81 81 c8 81 81 81 81 81 81 3e 81 81 81 81 81 2c 25 81 81 81 81 81 54 10 81 81 81 81 81 4e 05 81 81 81 81 81 56 ea 81 81 81 81 82 56 eb 81 81 81 81 81 58 0a 81 81 81 81 81 4c 32 81 81 81 81 81 4c 4a 81 81 81 81 81 50 64 81 81 81 81 81 50 4c 81 81 81 81 81 54 50 81 81 81 81 81 56 54 81 81 81 81 81 56 52 81 81 81 81 84 56 4e 81 81 81 81 a2 54 48 81 81 81 81 cb 54 44 81 81 81 81 e6 54 44 81 81 81 81 f3 56 42 81 81 81 81 0d 56 44 81 81 81 81 1c 54 42 81 81 81 81 28 54 42 81 81 81 81 30 56 46 81 81 81 81 6a 4c 44 81 81 81 81 03 7a 5a 81 81 81 81 81 e3 da 81 81 81 81 81 f6 81 81 81 81 81 81 76 38 81 81 81 81 81 5a 67 84 81 81 86 3e 3e 66 81 81 81 81 71 56 66 d5 81 81 81 63 76 66 72 81 81 81 65 6b 65 70 81 81 82 65 64 67 62 81 81 d9 68 6a 65 63 81 81 13 67 68 69 67 81 81 44 65 67 63 68 81 81 66 65 64 66 6b 81 81 7d 6a 68 69 6b 81 81 7f 6c 6a 66 6c 81 81 7f 6c 6b 68 6a 81 81 7f 6d 68 69 6a 81 81 7f 6d 68 6a 69 81 81 7f 6c 69 68 6d 81 81 7f 6c 6b 69 72 81 81 7b 6d 69 67 72 81 81 75 7d 69 69 61 81 81 6c 7f 6a 68 3c 81 81 68 7f 69 68 03 81 81 6b 7f 6b 69 c0 81 81 6b 7d 6b 6a b8 81 81 77 77 67 68 c7 81 81 95 77 6e 6b 81 81 81 81 6f 68 66 81 81 81 81 81 81 82 81 81 81 81 f7 66 63 81 81 81 81 f3 56 0a 81 81 81 81 04 5a 14 81 81 81 81 03 4c 36 81 81 81 81 0a 48 36 81 81 81 81 ff 3a 2e 81 81 81 81 ec 32 3a 81 81 81 81 e8 36 28 81 81 81 81 ce 2d f0 81 81 81 81 be 3c e2 81 81 81 81 b8 3e e7 81 81 81 81 b2 38 ec 81 81 81 81 9f 32 ea 81 81 81 81 9a 22 e5 81 81 81 81 8b 14 de 81 81 81 81 88 0d e1 81 81 81 81 86 0a e6 81 81 81 81 81 09 e0 81 81 81 81 81 09 fb 81 81 81 81 81 08 2b 81 81 81 81 81 0c 3e 81 81 81 81 81 0e 44 81 81 81 81 81 19 42 81 81 81 81 81 15 44 81 81 81 81 81 18 3e 81 81 81 81 81 30 4a 81 81 81 81 81 87 06 81 81 81 81 90 ec 30 81 81 81 81 4a 34 40 84 81 81 81 14 f8 27 1b 81 81 d6 0b f8 ec 02 81 81 11 11 df e5 ec 81 81 0b fb c6 e5 f5 81 81 11 ec cc e5 05 81 81 f8 f2 d9 c9 08 81 81 fb f5 d2 cf 2e 81 81 f8 ef d6 d9 21 81 81 e5 e8 cf be fb 81 81 cf 18 c6 cf f2 81 81 c3 21 be d2 e8 81 81 c6 21 c9 d6 cc 93 81 c3 2b d6 d9 b8 a2 81 bc 14 df d6 b8 a6 81 bc 14 df d9 ac a6 81 be 11 df d9 9d b0 81 c9 05 dc d9 97 b6 81 c6 02 e2 d9 b0 bc 81 c3 05 02 e5 be c3 81 cc f8 f2 dc b2 c6 81 d2 ec dc be ac c9 81 d6 ac 9d 81 84 cc 81 dc be c6 97 a0 cc 81 ef 1e b6 81 81 cf 81 f2 b2 d9 f8 81 d6 81 e2 81 a2 d9 81 d2 81 81 81 9b d8 81 81 81 82 81 5e 36 86 81 81 81 8c 54 ee 81 81 81 81 12 f9 5a b0 81 81 11 12 f8 66 72 81 81 32 fa 9d e6 1a 81 81 44 1b 81 90 0c 81 81 44 3a 86 ee 7f 81 81 4c 62 81 6a f9 81 81 4e 6c 81 7e f7 81 81 30 56 ae 14 26 81 81 fb 2e ea 11 58 81 81 ba 42 de 3a 65 81 81 8c 3c a0 17 50 a4 81 81 e7 91 14 44 ec 81 81 dd 81 fb f3 fd 81 b4 ef 81 c9 9e ce 81 eb 40 c1 c9 81 ae 81 4c 64 08 c1 81 93 81 7d 67 58 cc 81 86 81 58 7b 78 f2 81 92 81 3c 58 68 ea 81 96 81 ff 1d 30 c5 81 c0 81 ea fc 06 df 81 d5 81 fc e3 ea 38 81 01 81 fc 2c d5 5a 81 d7 81 30 06 14 4a 81 c8 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 be 81 81 81 81 81 83 1d 32 34 38 16 81 24 2b 3a 3e 44 36 82 ff 34 32 38 3e 2f 83 e8 32 38 36 36 34 85 00 36 36 36 34 24 86 05 36 32 32 3a 23 85 09 36 34 34 36 2c 85 f3 30 34 38 36 28 83 e6 36 32 34 32 29 81 58 2d 25 2f 2d 1b 81 0c 38 36 36 38 38 81 f5 4c 36 3a 38 34 84 f1 15 3a 38 34 34 81 25 30 3e 30 30 32 9a 2c 4a 56 50 4e 2e e9 cc 07 93 93 94 98 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 b4 82 34 81 81 81 81 58 5c 40 81 81 81 b2 3a 3c 3c 52 81 81 48 3e 48 40 3e 81 81 42 3e 40 3e 38 81 81 64 40 3c 36 3c 81 81 6e 34 60 4e 44 81 81 61 5e 2a 42 44 81 81 5a 34 2e f8 40 81 81 61 2f 64 17 58 81 81 81 62 3e 60 f4 81 81 81 42 40 67 81 81 81 81 44 42 74 81 81 81 81 44 46 68 81 81 81 81 46 48 42 81 81 81 81 46 4a 42 81 81 81 81 46 4a 46 81 81 81 81 48 4a 46 81 81 81 81 48 4a 46 81 81 81 81 46 4c 46 81 81 81 81 46 4c 48 81 81 81 81 46 4c 48 81 81 81 81 4a 4a 48 81 81 81 81 4c 46 46 81 81 81 81 4a 44 46 81 81 81 81 42 44 44 81 81 81 81 4e 42 4a 81 81 81 81 29 42 44 81 81 81 81 2e 34 69 81 81 81 81 58 f1 64 1f 81 81 10 46 6a 60 6d 81 81 58 3a 34 6e 5c 81 84 3a 4a 2e 32 58 81 88 3a 56 60 52 5a 88 81 36 4c 52 44 63 fa 81 32 44 3e 3e 3c 36 81 38 3e 48 44 44 3a 81 46 3c 36 42 44 52 81 34 3a 3e 36 4a 38 81 4e 48 4c 34 48 46 81 32 4c 40 3e 36 3e 81 14 12 1e 1b 02 46 86 25 17 1e 27 0c 3c 9a 0f 29 27 1d 0d 2b a4 0f 1f 30 20 11 3e a6 fa 27 2d 25 0d 2b 9e f1 30 32 28 17 23 99 f0 1a 0f 0b 0b 1b 9a ee 0d 10 0b 04 25 81 06 19 19 15 27 20 81 ff 16 0b 16 06 24 81 ff 06 19 10 02 20 81 0c 0c 1a 15 ec 22 81 f9 19 15 0f ba 20 81 04 1e 2e 22 9f 24 81 d3 29 0b 07 88 ff 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 1b 81 81 82 81 81 81 5e f9 81 23 85 84 81 23 46 e3 22 81 81 fb 19 23 54 08 ae cb 01 17 38 fe 25 2f 2d 0d 26 3e 54 f8 36 32 56 f2 5c e0 99 11 3e 7f a2 0e a6 ed 48 15 d1 db 34 e8 e6 81 54 5c 77 61 4a 5c 81 a2 1f fd ac 0c f7 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 83 81 c4 82 81 81 81 81 81 3c 81 f3 81 81 82 b8 2b 81 71 81 81 81 5a 28 8f 46 81 83 81 48 4e 6a 40 83 81 40 44 56 09 4e 81 df 4e 5c 60 75 4a 91 6a 3e 58 54 5a 64 1f 30 30 5e 6a 64 5c 26 56 1f 52 46 3e 48 52 44 40 42 48 48 4a fb 6e 74 6c 6a 6c 68 81 c0 c9 a4 9b 9b 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 84 81 dc 81 f4 81 81 82 81 4e 52 6b 81 81 81 2d 60 67 70 81 81 81 ba c8 28 0d 81 81 81 81 81 81 ec 81 86 81 81 85 81 66 81 81 2a 76 81 62 54 81 b2 24 6a 81 67 52 dc 2e a8 46 fe 58 4a 29 10 fa 62 4c 50 4e 50 56 5a 5a 5c 50 44 21 50 56 58 f5 50 40 81 54 73 20 81 71 4e 81 81 b2 81 81 a8 a4 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 50 2b 81 81 81 81 90 2d 71 81 81 81 81 ae 1c 5c 81 81 81 81 c1 23 5e 81 81 81 81 c2 21 4a 81 81 81 81 d1 1f 46 81 81 81 81 d9 50 4c 81 81 81 81 d6 22 4c 81 81 81 81 d3 e4 50 81 81 81 81 d1 d9 52 81 81 81 81 cb be 56 81 81 81 81 cc 91 56 81 81 81 81 c8 81 40 81 81 81 81 cc 81 40 81 81 81 81 d3 81 40 81 81 81 81 d1 81 40 81 81 81 81 d2 81 40 81 81 81 81 d9 81 40 81 81 81 81 de 81 44 81 81 81 81 de 81 44 81 81 81 81 dd 81 44 81 81 81 81 df 81 44 81 81 81 81 e7 81 44 81 81 81 81 e4 81 44 81 81 81 81 e3 81 40 81 81 81 81 d8 81 48 81 81 81 81 da 81 50 81 81 81 81 bc 81 2f 81 81 81 81 81 81 81 81 81 81 81 ae ff 77 81 81 81 81 b8 62 f5 b0 81 81 81 ac ae 8b c5 81 81 90 ae 9e be b6 81 81 a8 b4 a4 b2 b2 81 81 be b2 be b0 b6 81 81 c9 b2 a8 a6 c3 81 81 d8 b6 9e b2 c5 81 81 de b4 b4 b6 ce 81 81 d3 b6 ec c0 d7 81 81 c3 ba c0 c2 e7 81 81 a6 c2 b6 c3 01 81 81 99 c7 da c3 12 81 81 a6 c5 c5 c7 22 81 81 ca c3 b4 c2 2b 81 81 f8 b6 aa b4 36 81 81 e5 b6 ef b4 46 81 81 aa b4 c3 d5 d8 81 81 90 ae b4 a4 c9 81 81 8f ba de a4 d5 81 81 8f bc fd a2 f5 81 81 8d c7 a6 96 08 81 81 c0 16 e8 ce dc 81 81 ec c5 fd c7 8f 81 81 f1 81 81 81 81 81 81 b4 81 84 81 82 81 81 81 81 81 81 81 81 81 81 82 0c 81 81 81 81 81 81 6c a6 81 81 81 81 c6 68 6e 81 81 81 83 56 77 44 d9 81 81 81 3a 46 48 52 81 81 81 42 3e 3e 46 81 81 81 3e 48 44 46 81 81 81 44 42 46 44 81 81 81 40 3a 32 4a 81 81 a0 46 44 36 4e 81 81 be 4a 44 42 52 81 81 ef 48 46 40 50 81 81 15 40 4a 3c 63 81 81 38 38 4e 3e 70 81 81 5e 30 52 3c 52 81 81 48 25 4a 4a 0a 81 81 52 3a 46 44 ce 81 81 52 5a 4e 36 b6 81 81 52 74 4e 42 b6 81 81 56 6d 50 3a b6 81 81 74 58 4c 3a f5 81 81 75 4a 4c 3c 0a 81 81 75 44 4a 3a 21 81 81 79 40 4a 3e 42 81 81 48 44 40 3a 50 81 81 2e 70 50 4c f9 81 81 52 81 d1 cb aa 81 81 94 81 81 81 81 81 81 81 cb 81 2b 81 81 81 81 73 7f 6d 81 81 81 81 68 56 5e 81 81 81 81 6d 66 62 81 81 81 81 6e 6d 66 81 81 81 81 6f 69 66 81 81 81 81 6c 6b 66 81 81 81 81 6c 69 65 81 81 81 81 6c 66 62 81 81 81 81 6e 67 64 81 81 81 81 6c 68 66 81 81 81 81 6a 69 64 81 81 81 81 6f 69 65 81 81 81 81 72 68 66 81 81 81 81 71 68 68 81 81 81 81 71 69 69 81 81 81 81 70 69 69 81 81 81 81 71 69 69 81 81 81 81 70 6a 6a 81 81 81 81 70 6b 6b 81 81 81 81 71 6b 6a 81 81 81 81 71 6b 68 81 81 81 81 6c 6b 68 81 81 81 81 6c 6b 68 81 81 81 81 70 6d 69 81 81 81 81 6f 67 68 81 81 81 81 7b 7f 7b 81 81 81 81 e5 25 04 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 78 07 8e 81 81 81 83 2f 64 32 81 81 81 84 13 cc fb 81 81 81 81 1d f9 0a 81 81 81 81 32 ea 42 81 81 81 fe 27 29 60 81 81 85 23 12 30 54 81 83 81 05 03 24 48 81 81 cc 2c 28 29 4c 81 e5 34 1f 27 1e 48 3e 27 03 25 30 04 46 63 ca 11 23 2b 06 4c 56 0d f9 1b 1e 30 46 5a 1f d9 25 38 46 48 fa 42 06 2f 4c 42 3c 81 52 38 46 3a 4e 54 81 ba 3c 38 81 e8 ae 83 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 85 15 81 82 81 81 81 81 5c 81 82 81 81 81 c6 4c 4e 83 81 81 81 50 3c 46 81 81 81 81 48 4c 3e 81 81 81 81 4a 4a 46 81 81 81 81 50 46 48 81 81 81 81 4e 46 40 81 81 81 81 4e 46 3a 8e 81 81 81 46 42 3c 9e 81 81 81 32 44 3c ae 81 81 81 2d 44 3e bc 81 81 81 1b 46 3e d1 81 81 81 f4 46 40 dd 81 81 81 f1 46 44 ed 81 81 81 0f 44 46 f7 81 81 81 38 46 44 fa 81 81 81 50 46 46 fb 81 81 81 48 48 4a f8 81 81 81 48 4a 4e 1e 81 81 81 52 4e 52 8d 81 81 81 50 58 58 81 81 81 81 52 5e 60 81 81 81 81 58 68 66 8e 81 81 81 5c 70 6b c9 81 81 81 5a 6a 6e 17 81 81 81 60 7f 7a 40 81 81 81 b8 d7 81 81 81 81 81 81 87 81 81 81 81 81 81 81 81 82 81 81 81 81 11 52 81 81 81 81 81 38 ae 85 81 81 81 84 81 81 86 81 81 81 8c 81 81 81 81 81 81 81 81 83 81 81 81 85 81 81 88 81 82 81 81 81 81 81 81 81 82 82 81 81 81 81 81 81 4c 1c 42 4a ff 13 81 52 3c 3e 3c 1b 3c 81 54 3c 3a 32 3c 3c 81 5c 2b 3c 38 4e 46 d9 61 21 36 44 54 52 10 5e 46 4c 5a 14 5c 36 5a 56 4c 5a 15 5c 3c 58 52 54 56 fb 5c 3c 54 58 50 58 5c 5e 3a 50 56 4e 60 58 52 3a 50 58 54 4c 54 56 29 4a 5a 5a 4c 56 5c c6 48 58 5a 60 5e 5c 81 4c 5e 62 63 65 61 81 67 65 5e 5a 5e 60 81 62 62 68 67 64 66 81 67 7f 56 5a 54 2d 81 cf ca 81 81 81 81 81 81 81 81 81 81 81 81 81 81 3e c7 81 81 81 81 81 b6 29 81 81 81 81 84 81 e9 81 81 81 81 84 81 81 83 81 81 81 81 84 81 87 81 81 81 81 83 81 81 81 81 81 81 81 87 81 81 81 81 a0 81 88 81 81 81 81 ce 81 83 a0 81 81 81 e8 81 81 e5 81 81 81 c5 81 81 29 81 81 82 b4 81 81 2f 81 81 81 81 81 81 03 81 81 86 81 84 83 ba 81 81 89 81 81 81 81 81 81 83 81 81 81 84 81 81 81 3e 3c 32 3c 81 81 81 4a 56 50 48 81 81 81 4e 19 36 4c 81 81 b8 4e f1 1b 4e 81 81 b8 4e 1e 3c 5a 81 81 81 52 76 73 62 81 81 81 50 52 58 5e 81 81 81 4e 58 5a 61 81 81 84 5a 58 5c 74 81 81 84 6c 66 7a 25 81 81 83 11 10 81 81 81\n",
            "=======\n",
            "byte2: 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 83 8c 81 81 81 81 81 81 ef 81 81 81 81 81 81 14 08 81 81 81 81 8c 21 32 81 81 82 81 f4 29 28 8c 81 81 84 0c 16 2d b0 81 83 81 ef 08 28 f8 83 81 9b f0 07 21 32 81 81 f6 02 26 1b 26 98 d6 f0 fe 21 11 3e df f0 08 00 15 11 44 02 df ed 02 3e 46 38 2c 2c ff 44 52 7b 4e 81 25 54 17 81 3c 18 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 8e 81 81 81 81 81 81 62 7f 6b 81 81 81 0a 6a 76 6f 81 81 81 7f 64 65 67 34 81 81 71 7f 6e 63 71 81 81 71 b0 72 7f 7a 81 81 70 81 81 8e 7c 81 81 6e 81 03 ac 79 81 81 6c 81 a6 9a 79 81 81 6c 83 5e 75 75 81 81 6d 81 81 95 6f 81 81 6b 06 4a 88 6e 81 81 69 7f 73 7f 6e 81 81 69 63 66 6b 6c 81 81 75 69 6a 72 6c 81 81 7f 64 6a 71 6b 81 81 7f 60 6a 70 6c 81 81 7f 60 6b 69 6f 81 81 7f 63 6b 65 76 81 81 7f 65 6b 62 7b 81 81 7f 68 6a 65 7c 81 81 7f 6c 6b 68 7f 81 81 7f 6a 68 69 7f 81 81 7f 6e 6c 6d 7f 81 81 7a 5c 60 5a 7f 81 81 72 81 81 81 72 81 81 79 81 81 81 79 81 81 46 82 83 83 2f 81 81 81 c4 f6 30 81 81 81 81 44 71 7f 81 81 81 81 73 67 7f 81 81 81 81 7f 62 7f 81 81 81 81 7f 65 71 81 81 81 81 7f 62 6a 81 81 81 81 79 75 71 81 81 81 81 77 48 7b 81 81 81 81 72 81 7f 81 81 81 81 6e 81 7f 81 81 81 81 6b 81 7f 81 81 81 81 69 81 7f 81 81 81 81 69 81 7f 81 81 81 81 68 81 7f 81 81 81 81 64 81 7f 81 81 81 81 66 81 7f 81 81 81 81 6d 81 7f 81 81 81 81 6f 81 7f 81 81 81 81 75 81 7f 81 81 81 81 78 81 7f 81 81 81 81 70 81 7f 81 81 81 81 62 81 7f 81 81 81 81 69 81 6d 81 81 81 81 19 81 6e 81 81 81 81 ca 81 6d 81 81 81 81 81 81 6e 81 81 81 81 81 ba 7c 81 81 81 81 81 81 25 81 81 81 81 fc d5 c7 81 81 81 81 78 54 0c 81 81 81 81 58 56 26 81 81 81 81 44 58 25 81 81 81 81 42 52 26 81 81 81 81 15 62 04 81 81 81 81 f8 50 e9 81 81 81 81 0b de d8 81 81 81 81 00 e2 d4 81 81 81 81 0a b2 d7 81 81 81 81 08 81 d0 81 81 81 81 03 81 cc 81 81 81 81 02 81 b2 81 81 81 81 08 81 b4 81 81 81 81 fe 81 a0 81 81 81 81 f3 81 a8 81 81 81 81 f7 81 b6 81 81 81 81 0a 81 b4 81 81 81 81 48 81 ef 81 81 81 81 4c 81 22 81 81 81 81 52 81 26 81 81 81 81 4c 81 0e 81 81 81 81 42 81 09 81 81 81 81 5a 81 00 81 81 81 81 44 81 ea 81 81 81 81 fb 81 cb 81 81 81 81 f3 81 ae 81 81 81 81 95 81 82 81 81 81 82 81 cd 81 81 81 81 81 01 2e 38 81 81 81 81 e8 26 17 9b 81 81 eb eb 1d 1e 1e 81 81 07 e2 38 04 2a 81 81 04 e5 4e ed 23 81 81 fc e5 09 ee 23 81 81 fb de dc fe 21 81 81 05 e1 ed 16 21 81 81 04 e6 ed 1d 28 81 81 ff e1 de 16 23 81 81 f1 e5 e5 0e 14 81 81 e6 e5 ed 0c 12 81 81 e2 e1 e7 0d 13 81 81 e3 e1 e5 0e 15 81 81 e6 dd ea 0f 15 81 81 f6 db f2 14 13 81 81 14 db eb 13 20 81 81 23 da e7 0e 2a 81 81 3a d8 eb 07 21 81 81 46 e2 f1 04 7e 81 81 ae ee fb 02 95 81 81 81 f3 08 fc 81 81 81 81 e9 ed f2 81 81 81 81 e9 fc f9 81 81 81 81 ea fc e5 88 81 81 81 48 17 46 81 81 81 81 9f f0 c8 81 81 81 81 40 1e 34 81 81 81 81 6b 61 54 81 81 81 81 6a 48 34 81 81 81 81 7c 42 42 81 81 81 81 64 5e 3e 81 81 81 81 40 52 30 81 81 81 81 2b 6f 34 81 81 81 81 2b 3c 36 81 81 81 81 20 81 30 81 81 81 81 2b 81 38 81 81 81 81 30 81 38 81 81 81 81 30 81 34 81 81 81 81 2f 81 23 81 81 81 81 2f 81 0e 81 81 81 81 30 83 fa 81 81 81 81 40 86 ec 81 81 81 81 46 84 f7 81 81 81 81 44 85 f5 81 81 81 81 48 81 14 81 81 81 81 50 81 46 81 81 81 81 61 81 58 81 81 81 81 5c 81 58 81 81 81 81 4c 81 5c 81 81 81 81 06 81 5e 81 81 81 81 bc 81 64 81 81 81 81 81 b0 6d 81 81 81 81 81 f5 76 81 81 81 81 81 b6 12 81 81 81 81 81 b8 81 81 81 81 81 7a 6e 81 81 81 81 81 fd 3c 85 81 81 81 81 e3 73 81 81 81 81 87 06 5a ae 81 81 81 b0 ac 90 ba 81 81 81 c2 ba c6 ae 81 81 81 cf ba cf a4 81 81 81 d6 b0 c8 96 88 81 81 d0 aa c4 93 96 81 81 ca ac c0 8f 8e 81 81 c7 9d bc 81 8f 81 81 bc ac be 81 96 81 81 b2 a6 c2 81 9a 81 81 a4 ac c0 82 9c 81 81 9e ae c0 9c 9e 81 81 90 a8 c2 81 9d 81 81 86 b0 c2 81 9f 81 81 85 b0 c6 81 9d 81 81 81 bc c9 81 99 81 81 81 ba ca 81 95 81 81 81 c1 c9 81 92 81 81 81 ba ce 81 8e 81 81 81 d3 d1 81 8d 81 81 84 cc c3 89 87 81 81 9e 81 81 81 81 81 81 ae 81 81 81 99 81 81 a0 82 81 81 9c 81 81 81 81 1c 81 81 81 81 81 81 44 81 81 81 81 81 c0 46 fe 81 81 81 81 e3 27 f0 81 81 81 0a b0 eb c7 a6 81 81 58 81 a2 8b 18 81 81 a0 f6 10 ec cd 81 81 c0 9c d1 dd e9 81 81 dd 8c bc cc 0d 81 81 ee aa e5 b2 19 81 81 8f c4 f2 da df 81 81 95 b8 f7 d6 be 81 81 f5 83 bc 90 48 81 81 92 e8 eb e2 d9 81 81 d5 f1 1b 00 de 81 81 da 89 ba 91 f4 81 81 ed b4 e1 cf 00 81 81 c7 ba d7 d4 d3 81 81 f7 d4 f8 f2 b6 81 81 72 94 d4 c3 34 81 81 10 8c a8 a6 d0 81 81 13 fd 22 1b e7 81 81 28 87 d8 b6 f3 81 81 4c 8b ce a0 11 81 81 13 9a fc be f2 81 81 30 a4 01 c8 09 81 81 65 a0 00 ce 36 81 81 06 81 cc 8b d5 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 de a0 83 82 81 81 c3 56 26 81 81 81 82 32 96 30 cf 81 81 81 93 81 a2 46 ea f5 b8 9a d8 1a 98 01 19 e1 fd f3 81 81 8f 91 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 81 81 81 8c 81 81 81 13 9e 85 4a 81 81 81 14 30 81 5a 81 81 9f c9 3e a0 2b 81 81 24 ff f4 4a 3e 81 b4 cd f7 fb 2c 5e e8 f0 f6 09 3c f0 4c 36 17 0d 56 5a 7f 4e 1a 2f 54 44 3a 8e 87 83 b0 9a 81 9c 95 a6 c6 89 87 b0 9c ac c3 81 ae d1 b0 b4 ae a4 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 ee 81 81 81 81 81 2b 70 1b 81 81 81 81 16 73 1c 81 81 81 ea 03 5c 01 98 81 81 17 14 f3 09 f2 81 81 1e 0a 48 0f 0d 81 81 1f fe 0e 03 16 81 81 27 27 1f 20 17 81 81 20 36 09 1c 0f 81 81 2c 24 13 11 05 81 81 36 01 e9 0d 0e 81 81 30 1a ff 12 1b 81 81 24 0f 1f 22 13 81 81 22 26 2b 03 27 81 81 26 2b fd 05 1a 81 81 2c 23 e5 13 23 81 81 27 18 27 13 22 81 81 2c 1e 27 0f 1b 81 81 30 13 0e 0a 1e 81 81 34 11 14 06 1f 81 81 28 22 16 14 1e 81 81 27 d5 05 16 20 81 81 2a 81 83 81 1e 81 81 2a 81 81 81 1c 81 81 27 81 81 81 23 81 81 23 81 81 81 28 81 81 36 81 81 81 2b 81 81 dd 81 81 81 12 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 8c 81 81 81 81 81 a2 5c eb 81 81 81 81 81 19 2a 81 81 81 81 c3 0a 42 81 81 81 87 1c 64 60 81 81 82 81 ef 81 1c dc 82 81 1d f9 81 14 e9 81 d1 cb 0d 81 2c cb 82 01 84 3e 81 25 9b 85 20 94 7f 81 ce a8 81 46 a2 c0 81 e6 d9 2d 3a e0 d9 4a 3c 3e d3 fe 2d 12 bc 1a 17 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 84 cc 9e 84 81 81 c0 81 36 f5 81 82 81 0d 82 1f fb a0 81 81 19 eb 05 e5 1d 81 81 15 3c db d9 ed f5 88 f9 ff 0b 03 03 f1 2c 13 ed 11 f3 eb e1 2b e7 0f 19 1d 09 1d 50 4a 44 3e 32 1b 3e 58 f5 13 25 2b 17 07 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 84 81 82 81 81 81 81 89 82 81 81 81 81 81 81 81 81 81 81 81 81 86 cb 81 81 81 81 81 81 60 81 81 81 81 81 81 50 81 81 81 81 81 81 56 81 81 81 81 81 81 56 81 81 81 81 81 81 5c 81 81 81 81 81 81 5a 81 81 81 81 81 a6 4a 81 81 81 81 81 d8 4a 81 81 81 81 81 0e 48 a4 81 81 81 81 3a 48 b8 81 81 81 81 5a 44 dd 81 81 81 81 76 42 0a 81 81 81 81 7f 44 22 81 81 81 81 7f 46 40 81 81 81 81 7f 44 5a 81 81 81 81 7e 46 66 81 81 81 81 7b 48 6b 81 81 81 81 75 48 72 81 81 81 81 6f 4a 72 81 81 81 81 6a 4a 76 81 81 81 81 60 4e 56 81 81 81 81 5c 46 5e 81 81 81 81 7b 68 68 81 81 81 81 d6 b2 81 81 81 81 81 81 fa 81 81 81 81 81 84 77 81 81 81 81 83 18 61 81 81 81 81 81 61 58 66 81 81 81 81 67 58 6d 81 81 81 c2 67 4c 65 a4 81 81 29 68 67 67 34 81 81 64 69 65 68 71 81 81 75 5a 60 65 72 81 81 73 63 50 67 6e 81 81 71 6b 68 69 68 81 81 6d 66 62 67 67 81 81 6a 68 6a 64 66 81 81 67 69 56 65 67 81 81 65 69 58 66 68 81 81 65 69 6b 6a 6a 81 81 65 67 64 69 6b 81 81 65 67 65 6b 69 81 81 64 67 4e 66 69 81 81 69 67 68 63 6c 81 81 73 63 67 61 74 81 81 7f 64 6b 60 7f 81 81 7f 62 5c 60 7f 81 81 7f 67 5e 66 7b 81 81 73 69 6b 65 7f 81 81 7f 66 69 62 7f 81 81 9e 67 6b 63 81 81 81 81 62 60 65 81 81 81 81 81 93 86 81 81 81 81 6c 6e 68 81 81 81 81 28 5e 2b 81 81 81 81 19 3e fe 81 81 81 81 2c 09 27 81 81 81 81 3a 30 28 81 81 81 81 46 3a 38 81 81 81 81 4e 27 46 81 81 81 81 32 5a 38 81 81 81 81 1c 0f 12 81 81 81 81 13 d2 05 81 81 81 81 0e b8 05 81 81 81 81 10 98 08 81 81 81 81 18 81 05 81 81 81 81 22 81 fe 81 81 81 81 23 81 00 81 81 81 81 27 81 08 81 81 81 81 14 81 01 81 81 81 81 10 81 06 81 81 81 81 0a 81 15 81 81 81 81 20 81 1a 81 81 81 81 29 81 10 81 81 81 81 29 81 15 81 81 81 81 1e 81 10 81 81 81 81 1b 81 0c 81 81 81 81 11 81 15 81 81 81 81 3c 81 36 81 81 81 81 a6 81 b4 81 81 81 81 e5 f2 27 81 81 81 81 65 7f 21 81 81 81 cf 36 f2 0e ef 81 81 08 02 f8 0b 11 81 81 fb 05 d9 ec f2 81 81 fe e8 c6 df ef 81 81 fb d9 d6 e5 f2 81 81 ef d9 d6 dc f5 81 81 cc df df cc ef 81 81 b2 ec df d9 e5 81 81 c9 f5 be c9 e8 81 81 e5 e8 c6 c6 ef 81 81 e5 e8 cc c6 f5 81 81 e5 e2 cf c3 f5 81 81 ec e5 df c9 02 81 81 e5 e5 dc d2 05 81 81 e2 e2 d9 d6 fe 81 81 dc e5 d9 d6 fe 81 81 d6 e5 d9 d6 e2 81 81 cf ef dc df dc 81 81 d2 ec ec e5 d9 81 81 cf ec e5 df b8 81 81 cc cf d2 bc a2 81 81 bc 81 81 81 9a 81 81 ac 81 93 87 84 81 81 b0 9d 90 81 9a 81 81 9a 21 c6 0e a0 81 81 81 81 c6 b0 90 81 81 81 81 d1 81 81 81 81 84 44 5e 32 83 81 81 81 79 6e 2a 81 81 81 9a d8 7f 9c 81 81 81 17 d9 50 a8 32 81 81 c5 e7 70 85 1e 81 81 3c 26 58 9b 68 81 81 5a 7e 29 f1 40 81 81 52 60 1a 7f 9b 81 81 08 61 20 3e 18 81 81 a2 60 56 a2 2f 81 81 81 69 63 c8 18 81 81 b6 4a 4e 07 16 81 81 21 fb 2c 2b f6 81 81 7f c4 27 40 1f 81 81 7f d6 32 4e 40 81 81 4e 15 27 30 16 81 81 56 46 17 01 32 81 81 6e 60 26 d9 36 81 81 58 6f 1e be 4e 81 81 1d 60 48 cc 3e 81 81 ba 32 69 e0 0d 81 81 85 f1 54 28 e6 81 81 82 c9 52 50 b6 81 81 81 c2 54 5e 9b 81 81 9e 13 5e 56 81 81 81 9b 38 71 07 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 c2 9a 81 81 81 81 81 20 34 34 32 3a e4 81 36 2f 3a 3c 30 30 82 34 32 36 36 3a 30 81 44 36 3a 34 3c 38 81 44 32 36 38 32 52 81 40 30 34 32 36 58 81 40 34 36 32 38 60 81 46 30 32 38 38 52 81 46 34 30 30 30 4e 84 36 28 25 2d 2d 46 81 42 38 34 38 3e 60 81 4e 38 36 3a 38 5c 81 4a 42 3a 3a 34 3c 81 34 1f 3a 32 30 3a e2 4a 44 58 4e 4c 63 a0 e9 2f 8a 94 95 a2 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 40 82 a0 81 81 81 86 3e 5a 5e 81 81 81 30 3a 38 38 90 81 81 42 3e 46 3c 32 81 81 3e 3e 40 3c 3e 81 81 44 32 3a 3c 40 81 81 4e 40 67 38 3a 81 81 4a 56 40 6f 38 81 81 46 2c 30 34 3e 81 81 7f fc 70 5c 52 81 81 e4 6b 3a 66 81 81 81 81 4c 44 63 81 81 81 81 44 46 63 81 81 81 81 48 44 63 81 81 81 81 48 44 44 81 81 81 81 48 46 3a 81 81 81 81 46 46 3e 81 81 81 81 48 48 40 81 81 81 81 4a 48 42 81 81 81 81 4c 4c 44 81 81 81 81 4e 4a 44 81 81 81 81 4c 4a 44 81 81 81 81 4a 4a 44 81 81 81 8c 4a 48 48 81 81 81 8c 44 46 46 81 81 81 8b 42 44 42 81 81 81 8c 4a 40 4c 81 81 81 81 42 44 3c 81 81 81 81 5e 6b 69 81 81 81 97 7c 81 78 92 81 82 66 72 3a 6b 5c 81 81 5c 58 48 36 50 82 81 50 42 3a 48 36 81 81 56 52 48 5e 3a 81 81 4a 52 44 40 34 81 b0 6b 38 3c 38 30 81 03 4e 34 3e 48 40 81 1f 2d 4e 36 36 42 8e 1e 3a 44 3c 48 32 b4 44 3a 40 48 44 4a c9 52 44 4e 36 48 1f d8 46 f3 1d 2b 17 ec f9 40 d2 2d 27 2c db f0 32 c1 34 23 32 d6 02 36 d7 1b 25 32 e0 0d 2e c9 22 2b 30 c2 fc 2e c1 36 27 38 ac f8 25 bc 17 12 0b a4 fd 36 b8 10 12 22 a8 02 22 a8 12 0d 19 a0 ef 2e a0 0c 1b 23 90 f5 27 81 20 16 17 91 f6 0a 81 17 0d 1b 81 c5 c1 81 1d 15 0f 81 9b 94 81 1e 23 1b 81 8b 83 81 17 0d 1d 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 81 81 81 81 81 30 81 81 81 81 81 81 2a 8f 81 81 85 81 b6 3a 40 32 c2 81 81 7f 30 0e 19 0e b6 df fb 19 38 21 01 34 d6 16 24 10 32 81 4c 1a 04 46 52 44 9d 2a 09 0d 2c f6 cc 0a 32 1b c6 ff fd 01 28 e1 77 32 7f 7f 7f 5a 81 04 28 a4 cd 05 90 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 83 8a 83 81 81 81 82 81 04 82 86 81 81 82 5e 2c 81 dd 81 81 81 56 52 c4 3a 81 82 ef 54 46 71 71 84 81 7b 3c 50 26 75 81 4a 3e 4a 60 64 7b d8 5c 4a 5a 64 3a 7f 40 00 44 61 5e 5c 7b 1d 4e 36 52 40 46 6a 38 4e 4a 40 48 4a 67 48 6c 70 6b 6a 6c 52 81 c8 be a0 98 9c 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 ac 81 81 81 81 81 63 50 4c c0 81 81 84 13 58 64 cf 81 81 81 81 f4 20 fc 81 81 81 81 81 81 46 84 81 02 81 82 c4 56 81 81 42 90 81 5e 52 92 ef 10 7f 81 56 67 50 32 f0 54 87 60 62 4e d6 2f 63 50 50 6c 40 4c 65 50 60 4c 60 46 4e 56 75 9b 50 54 a2 66 70 94 81 7e 7f 81 81 a2 81 81 a0 ac 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 10 3e 81 81 81 81 81 2b 46 8c 81 81 81 81 42 2b b4 81 81 81 81 38 1d ef 81 81 81 81 2b 20 f7 81 81 81 81 2c 3e 02 81 81 81 81 27 69 02 81 81 81 81 29 22 ff 81 81 81 81 36 e6 01 81 81 81 81 36 aa f3 81 81 81 81 36 81 e9 81 81 81 81 3e 81 df 81 81 81 81 3e 81 dd 81 81 81 81 38 81 de 81 81 81 81 40 81 dd 81 81 81 81 30 81 df 81 81 81 81 2e 81 de 81 81 81 81 2f 81 df 81 81 81 81 2d 81 de 81 81 81 81 2d 81 da 81 81 81 81 2d 81 de 81 81 81 81 2c 81 d8 81 81 81 81 2f 81 cb 81 81 81 81 32 81 cc 81 81 81 81 34 81 c7 81 81 81 81 3a 81 b8 81 81 81 81 44 81 c2 81 81 81 81 1f 81 9e 81 81 81 81 d1 81 89 81 81 81 81 f5 ff ff 81 81 81 90 86 38 86 81 81 81 be b6 f8 b2 8d 81 81 ce b2 ac b2 a0 81 81 c7 b2 a4 ae b6 81 81 c9 b0 dc b0 be 81 81 d7 aa ba ac ba 81 81 ec b4 b4 b2 ba 81 81 f5 be bc ae be 81 81 0b c0 fc a2 be 81 81 29 c3 d3 a0 b6 81 81 38 c7 01 a8 a0 81 81 40 d3 1b a8 97 81 81 34 e1 42 ae 8f 81 81 16 e5 29 b0 8b 81 81 21 e5 d5 b4 88 81 81 38 08 d5 b4 b0 81 81 66 32 d0 be e5 81 81 74 fd c7 b6 0b 81 81 4a 18 bc c5 2d 81 81 52 06 aa d8 21 81 81 26 f5 d1 dc 2b 81 81 4e ff e8 ff 1f 81 81 88 ba 4c d7 12 81 81 86 81 99 81 28 81 81 89 81 81 81 b6 81 81 81 81 a2 81 81 81 81 81 81 f5 81 81 81 81 81 81 65 81 81 81 81 81 44 72 23 81 81 81 81 4e 5c 52 81 81 81 a2 36 48 38 ac 81 81 10 3e 3c 42 27 81 81 5c 3e 4c 42 46 81 81 61 42 38 46 44 81 81 42 38 38 38 4e 81 81 54 38 3e 34 56 81 81 5a 4c 32 46 5a 81 81 5c 52 3e 4e 5a 81 81 58 52 46 48 5a 81 81 54 56 38 3e 58 81 81 52 5a 36 40 62 81 81 58 56 42 44 6e 81 81 64 52 4c 46 7f 81 81 69 4a 32 34 7f 81 81 61 3e 3e 30 7f 81 81 5c 34 4c 36 7f 81 81 58 38 42 38 7f 81 81 5a 3e 3a 3a 76 81 81 56 3e 40 3e 66 81 81 58 42 3e 3e 66 81 81 5a 3c 42 3a 62 81 81 5a 4c 4c 4a 64 81 81 6b aa d7 a0 6a 81 81 ae 81 81 81 88 81 81 81 10 8c d8 81 81 81 81 77 77 70 81 81 81 81 67 e5 62 81 81 81 81 69 4e 65 81 81 81 81 6b 72 63 81 81 81 81 6b 67 60 81 81 81 81 6c 6c 60 81 81 81 81 6b 6a 60 81 81 81 81 68 68 5c 81 81 81 81 6c 68 5c 81 81 81 81 6f 68 5e 81 81 81 81 71 68 5c 81 81 81 81 6e 68 5e 81 81 81 81 6d 68 61 81 81 81 81 6c 67 63 81 81 81 81 6c 67 67 81 81 81 81 6d 67 67 81 81 81 81 6e 67 65 81 81 81 81 6e 66 63 81 81 81 81 6d 65 63 81 81 81 81 6e 66 64 81 81 81 81 6e 66 66 81 81 81 81 6e 67 67 81 81 81 81 70 68 65 81 81 81 81 75 6a 67 81 81 81 81 6f 61 64 81 81 81 81 7f 7f 76 81 81 81 81 02 23 ea 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 b6 81 81 81 81 81 84 dd ea 81 81 81 81 81 02 7f c5 81 81 81 81 11 db ce 81 81 81 81 f1 ee c9 81 81 82 e9 17 df b2 81 81 81 38 1c 2f ba 81 81 81 fd 21 28 df 87 85 c7 32 1f 36 13 81 81 36 2b 1e 12 3c c1 e6 2f 04 27 ff 62 56 32 22 29 29 0d 5c 4c f1 26 24 28 24 50 50 0a f0 21 26 30 5c 40 f8 e9 2c 32 42 5e 58 30 12 36 52 3a 64 8d 58 38 54 44 48 4e 81 0a 48 17 81 17 81 83 81 9b 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 82 ec 83 81 81 81 81 81 5e 81 81 81 81 81 3c 6e 06 81 81 81 85 48 30 48 81 81 81 81 40 2c 3a 81 81 81 81 46 34 3e 81 81 81 81 48 2b 3a 81 81 81 81 50 34 3e 81 81 81 81 56 40 4a 81 81 81 81 4e 40 3c 81 81 81 81 2d 40 21 81 81 81 81 27 40 1a 81 81 81 81 1c 42 ed 81 81 81 81 13 44 d7 81 81 81 81 23 46 c5 81 81 81 81 3e 46 e9 81 81 81 81 48 48 26 81 81 81 81 46 4c 3a 81 81 81 81 44 4c 3a 81 81 81 81 40 4e 3c 81 81 81 81 3e 50 3a 81 81 81 81 3c 4c 3a 81 81 81 81 3e 4c 40 81 81 81 81 3c 46 48 81 81 81 81 3a 46 50 81 81 81 81 44 3e 4e 81 81 81 94 58 4e 60 81 81 81 81 b4 f7 d2 81 81 81 81 81 81 89 81 81 81 81 81 9a 81 81 81 81 81 82 56 d4 81 81 81 81 88 81 4e 81 81 81 81 81 81 c1 81 81 81 81 81 83 81 85 81 81 81 81 81 81 8a 81 81 85 81 82 81 84 81 81 81 81 81 81 81 81 81 85 a0 81 81 81 81 81 3e 2d 3e 1f 44 36 8d 42 52 36 3a 4a 08 f3 40 32 34 4a 46 f1 74 4a 36 4a 38 3c 2a 1e 56 40 20 4a 3c 3a aa 58 38 4e 68 5c 54 81 58 4e 4e 42 3a 67 81 5a 54 48 60 56 72 81 5a 56 52 63 60 7f 81 5a 56 4e 56 54 7f 81 58 5a 48 4a 58 7f 81 5c 58 5a 50 5e 7f 00 5c 58 5a 5e 60 7f ee 56 60 64 65 64 7f 81 5e 65 5e 5c 60 6b 81 67 62 67 65 66 75 81 7f 7f 58 54 48 3a 81 d9 be 81 81 81 81 81 81 81 ef 83 81 81 81 81 83 bc 81 81 81 81 81 83 81 81 81 81 81 81 81 88 e9 81 81 81 81 81 82 1c 81 81 81 81 91 81 e5 81 81 81 81 cc 81 81 83 81 81 81 d9 81 81 86 81 81 81 c0 81 81 87 81 81 81 97 81 81 81 81 81 81 81 81 87 81 81 81 85 81 81 87 81 81 81 88 81 81 83 ae 81 81 81 81 81 81 db 81 81 81 89 84 83 0a 81 81 81 81 81 81 a4 81 81 81 81 81 81 98 81 81 81 3a 3c 34 2c 81 81 81 4a 5e 4a 4e 81 81 81 4e b0 56 4a 81 81 cb 54 81 69 5e 81 81 02 56 be 69 5c 81 81 d8 58 7f 64 60 81 81 c0 4e 56 5a 52 81 81 81 50 58 5c 32 81 81 81 4e 5c 5a 03 81 81 81 60 7f 54 8a 81 81 81 0a a2 81 81 81\n",
            "=======\n",
            "byte3: 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 84 88 81 81 81 81 81 81 81 81 81 81 81 81 81 97 81 81 81 81 b6 30 1b 81 81 81 81 0a 30 16 81 81 81 84 f3 1a 10 81 81 81 81 db 04 10 81 81 85 81 ee 11 11 81 83 81 ed f6 1b 0e 81 81 96 f0 07 2b 10 ac b6 f7 f2 0c 0d 1c be db f3 09 18 fe 0b ba e9 e2 eb 16 3e 3c a4 3c 1a 00 52 32 7b b2 8d 3a 52 d3 81 30 8c 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 c4 81 81 81 81 81 89 77 7f 2d 81 81 81 6f 65 70 78 81 81 81 67 69 64 63 81 81 81 65 6e 76 6a 95 81 81 67 7c 58 64 b6 81 81 68 7f 81 fc f3 81 81 6f 7f f5 a6 23 81 81 75 7f 9d 8e 48 81 81 7a 7f 7f 46 60 81 81 7a 81 81 ff 7a 81 81 7d 0b 56 2f 7f 81 81 7f 7f 73 7f 7f 81 81 79 60 66 5e 7f 81 81 5e 69 6a 67 7f 81 81 48 68 6a 6d 76 81 81 3c 69 6a 7d 78 81 81 16 6b 6b 7f 79 81 81 f0 6b 6b 7f 79 81 81 ee 6b 6d 7f 75 81 81 f3 6b 6d 7f 74 81 81 1b 6a 6e 7f 73 81 81 2c 6a 6a 70 72 81 81 ba 6e 6d 7e 6f 81 81 be 5e 60 7f 6c 81 81 99 81 81 81 74 81 81 97 81 81 81 7f 81 81 8c 83 83 84 3e 81 81 81 32 02 81 81 81 81 81 7f 7b cd 81 81 81 81 6d 5e f5 81 81 81 81 6b 61 12 81 81 81 81 66 61 28 81 81 81 81 61 63 3c 81 81 81 81 5a 63 30 81 81 81 81 63 78 14 81 81 81 81 61 7f 02 81 81 81 81 60 7f 08 81 81 81 81 5e 78 12 81 81 81 81 62 40 ff 81 81 81 81 67 20 f0 81 81 81 81 6c 04 f0 81 81 81 81 73 ba f0 81 81 81 81 74 8b e5 81 81 81 81 72 87 da 81 81 81 81 75 81 e4 81 81 81 81 6e 81 dd 81 81 81 81 69 81 db 81 81 81 81 65 81 dd 81 81 81 81 69 81 e0 81 81 81 81 6c 81 e3 81 81 81 81 72 81 d2 81 81 81 81 76 81 cb 81 81 81 81 65 81 e4 81 81 81 81 48 81 f8 81 81 81 81 e3 81 ba 81 81 81 81 ed d4 87 81 81 81 81 66 2f de 81 81 81 81 56 42 e4 81 81 81 81 65 52 b6 81 81 81 81 63 54 8f 81 81 81 81 36 64 8b 81 81 81 81 eb 7b 91 81 81 81 81 cf 76 88 81 81 81 81 e7 7d 90 81 81 81 81 e5 7c 8b 81 81 81 81 d9 7c 86 81 81 81 81 d8 74 83 81 81 81 81 da 67 81 81 81 81 81 db 6b 81 81 81 81 81 cf 60 81 81 81 81 81 d1 3c 81 81 81 81 81 e3 12 81 81 81 81 81 d2 e0 81 81 81 81 81 2d b4 81 81 81 81 81 52 b0 81 81 81 81 81 5a b6 81 81 81 81 81 60 b0 81 81 81 81 81 67 aa 81 81 81 81 81 6a 9f 81 81 81 81 81 70 8e 81 81 81 81 81 77 81 81 81 81 81 81 7f 81 81 81 81 81 81 2f 81 81 81 81 83 81 ba cd 81 81 81 82 81 4a 40 ed 81 81 81 e7 f4 1a 0b 81 81 82 fd de 14 f3 81 81 83 e6 f5 25 fb de 81 81 ea e9 2f f8 0e 81 81 f1 e6 05 0f 44 81 81 f8 e2 ee 26 30 81 81 16 e2 f4 1f 32 81 81 21 ee f3 24 2f 81 81 21 e9 fc 21 23 81 81 28 e6 ff 1f 15 81 81 46 e1 fb 17 0f 81 81 46 e1 f9 17 08 81 85 42 eb f8 1a 0c 81 99 3c f1 00 17 04 81 a8 1f f5 09 24 07 81 a8 f8 f8 00 34 02 81 ba e6 fc ff 32 0b 81 ed d5 fb 01 2f 14 81 e2 a6 fb ff 2d 29 81 81 83 fc 05 18 89 81 81 8e e5 02 18 81 81 81 ca e7 04 0f 82 81 81 f1 0c 0b 09 81 81 81 0f e6 10 ff 81 81 81 8a 28 28 36 81 81 81 81 c9 f0 8d 81 81 81 81 3c 26 36 81 81 81 81 56 5a 3e 81 81 81 81 5a 4c 2b 81 81 81 81 4e 44 4c 81 81 81 81 38 54 63 81 81 81 81 42 58 61 81 81 81 81 40 67 5a 81 81 81 81 42 67 56 81 81 81 81 42 50 52 81 81 81 81 3e 18 52 81 81 81 81 3a b6 4e 81 81 81 81 3e 81 48 81 81 81 81 48 81 48 81 81 81 81 54 81 34 81 81 81 81 76 81 12 81 81 81 81 78 81 0f 81 81 81 81 54 81 26 81 81 81 81 61 81 28 81 81 81 81 5e 81 34 81 81 81 81 56 81 4c 81 81 81 81 4e 81 5e 81 81 81 81 4c 81 60 81 81 81 81 56 81 66 81 81 81 81 5a 81 68 81 81 81 81 5a 81 56 81 81 81 81 40 81 60 81 81 81 81 22 81 6d 81 81 81 81 a6 81 02 81 81 81 81 92 b4 81 81 81 81 84 7f 74 81 81 81 81 81 2b 7f 81 81 81 81 81 81 7f 81 81 81 81 b8 ac 81 95 81 81 81 ee c8 95 ba 81 81 81 ca b4 9a 9c 81 81 81 b6 b4 95 92 81 81 81 9e ac 96 94 81 81 81 8e aa 8e aa 81 81 81 8b aa 91 a2 81 81 81 83 b0 8b ce 81 81 81 84 ae 8a 22 81 81 81 85 ae 88 72 81 81 81 8c b6 85 5a 81 81 81 9a b2 89 74 81 81 81 b6 b2 87 5c 84 81 81 e8 ae 83 52 88 81 81 fa b4 83 3e 89 81 81 03 ba 82 30 8b 81 81 1c b6 83 07 8d 81 81 3a bc 85 fb 91 81 81 42 b8 85 f0 94 81 81 62 ca 89 e7 94 81 81 6a cc 94 b4 92 81 81 70 81 81 81 92 81 81 f4 81 81 81 9b 81 81 b4 81 81 81 9c 81 81 81 81 0b 82 82 81 81 81 ae 40 81 81 81 81 81 aa 36 87 81 81 81 b4 d0 1d fd 81 81 81 c7 d1 f2 ff 81 81 81 81 91 ac ac 81 81 81 e3 1d 26 0f 81 81 81 b2 ba d6 f8 95 81 81 a8 ac b2 d5 a8 81 81 f3 cd d3 e3 ae 81 81 c9 de e8 ed 9d 81 81 cb cf e8 eb de 81 81 3e 88 a6 a0 16 81 81 e0 e6 04 ef ae 81 81 f6 ee 22 12 dd 81 81 4c 8b b2 a2 ef 81 81 72 c0 f1 c9 14 81 81 36 c2 f6 c9 e0 81 81 1c e1 14 df d6 81 81 2c b0 e9 b2 4e 81 81 ed a0 c3 ae d2 81 81 c6 17 4a 2d ed 81 81 92 b6 f0 c9 fe 81 81 88 ce c3 ce 1d 81 81 83 06 db ee e4 81 81 81 27 e4 0b d7 81 81 81 32 ee fa e9 81 81 81 d1 ac 81 b8 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 94 b2 92 81 81 81 81 2f 34 69 81 81 81 82 0b e0 52 81 81 81 81 81 81 1c f0 ec b2 b0 ba db f6 e8 03 c0 fd fd dc 81 81 8f 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 83 81 81 81 81 81 81 81 30 81 81 8d 81 83 81 ff 81 81 b4 82 81 6c e9 6e dc 9a 81 be d1 ff 30 5e a8 81 16 e5 fa db 2d a2 f0 d6 e8 21 46 df cd 36 ff 22 5e 52 7c a8 2b 36 50 16 81 98 93 83 c6 87 81 c9 8c b0 ca ba 9b ba c2 aa b0 81 ca c3 a8 ac ba 82 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 12 ea 81 81 81 81 81 24 65 93 81 81 81 a8 40 3a 23 81 81 81 22 48 3c 0f 81 81 81 0a fa 70 05 81 81 81 02 0e 50 0d 84 81 81 0a 1c 26 02 93 81 81 22 36 58 23 ba 81 81 2b 46 2f fa e6 81 81 4e 32 3c 09 0a 81 81 5c fa 1e e8 1a 81 81 58 18 4c f5 1c 81 81 68 36 67 05 1e 81 81 5e 18 36 11 0f 81 81 6c 0e 23 0d 22 81 81 40 1b 22 16 17 81 81 28 1e 50 0d 1c 81 81 0b 06 46 07 1c 81 81 f7 16 38 14 1b 81 81 fb 26 34 1a 17 81 81 24 24 36 14 0e 81 81 12 c0 1a b8 0e 81 81 d6 81 8a 81 0f 81 81 d1 82 81 82 11 81 81 cd 81 81 84 0e 81 81 b4 81 82 86 13 81 81 b6 81 81 86 0d 81 81 84 81 81 84 0a 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 fe 81 81 81 81 81 32 b4 04 82 81 81 81 81 e0 22 82 81 81 81 e6 30 07 85 82 81 87 14 5a 1c 81 81 81 81 fa 17 0d 81 82 81 0a 24 82 d3 81 81 ec 0a 2c 81 32 9f 84 09 c0 40 81 fe c8 81 f3 ce 4a 81 81 d0 81 28 e3 3e 90 81 aa 2a 3c 2c 4c 52 75 ae e4 06 20 db 38 25 b2 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 eb 81 81 81 81 81 84 05 81 84 81 81 81 81 01 df 84 82 81 ca 84 ed 29 81 84 81 25 0f 1f e1 f5 81 81 34 2e d9 eb fb e5 81 f5 f7 ff fd ef fb d0 21 07 0f ed d4 e9 42 13 11 1f 17 ff 2b 60 42 30 3c 2c 1f 44 df 09 25 21 25 19 ff 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 a4 84 81 81 81 81 81 c1 81 81 81 81 81 81 25 81 81 81 81 81 81 61 48 81 81 81 81 81 4a 58 81 81 81 81 81 40 48 81 81 81 81 81 16 4a 82 81 81 81 81 0c 34 82 81 81 81 81 30 40 81 81 81 81 81 70 40 81 81 81 81 81 5c 40 81 81 81 81 81 61 48 81 81 81 81 81 5e 48 81 81 81 81 81 5c 4c 81 81 81 81 81 56 4c 81 81 81 81 81 50 4a 81 81 81 81 81 4e 48 81 81 81 81 81 44 4a 81 81 81 81 81 42 4a 81 81 81 81 81 42 4a 81 81 81 81 81 46 4a 81 81 81 81 81 4a 48 86 81 81 81 81 4a 46 98 81 81 81 81 4a 44 a8 81 81 81 81 4c 44 b0 81 81 81 81 4c 3e d7 81 81 81 81 63 6c c6 81 81 81 81 00 bc 81 81 81 81 81 81 ee 81 81 81 81 82 7f 75 81 81 81 81 81 78 67 81 81 81 81 81 65 64 f3 81 81 81 28 65 6c 76 85 81 81 77 65 60 63 81 81 81 74 69 60 67 81 81 81 6a 6d 6b 68 ae 81 81 68 64 68 66 21 81 81 6c 69 60 68 5c 81 81 6d 63 66 63 54 81 81 6e 66 67 66 5e 81 81 6f 67 69 69 7f 81 81 70 66 60 69 7f 81 81 71 66 64 69 7f 81 81 73 66 68 68 7f 81 81 6f 68 67 65 7f 81 81 6d 68 68 62 7e 81 81 6b 68 62 68 7b 81 81 6c 69 6a 72 75 81 81 54 6a 6a 7d 72 81 81 25 69 6b 7f 6b 81 81 e4 69 67 7f 69 81 81 95 6b 6c 7f 68 81 81 81 6c 6b 7f 6a 81 81 81 6b 68 76 63 81 81 81 6d 6a 77 81 81 81 81 62 63 6a 81 81 81 81 81 89 81 81 81 81 81 54 7f 29 81 81 81 81 44 5c f7 81 81 81 81 36 4c ec 81 81 81 81 38 1e 01 81 81 81 81 3a 24 06 81 81 81 81 38 24 f0 81 81 81 81 28 42 d4 81 81 81 81 02 46 c4 81 81 81 81 fa 4c ac 81 81 81 81 e8 50 99 81 81 81 81 e1 50 90 81 81 81 81 d8 4e 8d 81 81 81 81 cf 4a 8c 81 81 81 81 c6 3c 81 81 81 81 81 cf 36 81 81 81 81 81 d1 36 81 81 81 81 81 c9 38 81 81 81 81 81 c1 34 81 81 81 81 81 d6 32 81 81 81 81 81 24 36 81 81 81 81 81 34 2f 81 81 81 81 81 36 30 81 81 81 81 81 3c 2a 81 81 81 81 81 38 1f 81 81 81 81 81 32 23 81 81 81 81 81 46 36 81 81 81 81 81 ed ac 81 81 81 81 81 05 18 b2 81 81 81 81 50 1e 40 81 81 81 0e 40 e5 08 81 81 81 f8 ec fb 1b c6 81 81 f2 fb d6 fe 0b 81 81 08 d9 d6 df 0b 81 81 14 d6 dc d6 08 81 81 27 d9 d2 d6 f2 81 81 24 cc d9 d9 f8 81 81 18 d9 df cf ef 81 81 0b e2 cc e5 e5 81 81 cc e5 cf e8 e5 81 81 a2 e5 bc e2 e2 81 81 97 e2 b2 d9 ef 81 81 8d ec d2 d9 f5 81 8d 81 ef df d9 f8 81 a0 81 ec d9 d6 f8 81 a0 81 ec d9 d6 05 81 a0 81 ec d9 d6 0e 81 a8 81 f2 d6 dc 0e 81 ac 81 f5 e8 e5 0e 81 b6 81 ef dc d9 08 81 c3 81 d9 c9 b2 fe 81 d9 81 81 81 81 05 81 d9 81 93 8d 81 08 81 ef 81 81 81 fe 1e 81 e8 81 bc be 0b fe 81 cc 81 9d c9 81 fb 81 81 81 81 d7 81 81 81 81 83 26 75 b0 81 81 81 81 3c 4a d0 81 81 83 c4 be 46 ba 81 81 81 dc a6 38 c3 91 81 81 50 81 65 a0 46 81 81 36 a4 56 bc 7b 81 81 2b d4 1d 46 1d 81 81 48 19 00 7f 50 81 81 5a 6a 75 d5 7f 81 81 5a 58 7f b4 52 81 81 70 04 36 fb 11 81 81 7c e0 6c 66 69 81 8d 67 f2 70 78 46 81 b2 26 e8 36 6f da 81 c8 d5 e7 08 5e db 81 aa 88 09 e6 32 e1 81 be 81 3e 96 f5 ec 81 ba 81 4e 8b a8 38 81 b4 81 32 8c a0 40 81 bc 81 2b 91 fd 52 81 ba 81 10 ae 4a 5c 81 a6 81 d3 f3 4c 70 81 98 81 c3 1f 54 71 81 ba 81 03 fd 69 65 81 e1 81 30 cb 7e 5c 81 f3 81 18 e7 08 58 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 eb 81 81 81 81 81 81 fc 88 81 81 81 81 83 1f 3e 34 2f 1e 81 81 27 34 3c 3c 3a 81 81 23 30 36 36 2e be 8e 2d 34 38 32 32 cb c4 30 32 34 34 30 cf dd 32 2f 32 32 34 d1 dd 30 32 32 32 34 ce cc 2c 32 36 38 30 c7 8e 2d 34 34 32 2b c3 81 30 26 2a 2d 23 c0 81 3a 36 36 38 38 bc 81 11 34 38 3a 38 c9 81 44 44 38 36 36 aa 99 46 3e 36 30 32 a4 e8 40 32 56 4e 4c ae 81 f7 1a 93 94 96 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 f4 a0 81 81 81 81 f2 65 72 28 81 81 81 42 3c 42 3c 81 81 81 36 3c 44 38 81 81 81 3e 3c 3e 3a aa 81 81 3e 32 38 40 ff 81 81 3e 5c 5c 3e 3e 81 c3 3c 3e 30 40 32 81 04 3c 26 1f 6f 44 81 81 3c 60 5a 56 c8 81 81 5c 62 44 38 81 81 82 5c 38 3c 5a 84 81 81 5e 46 38 58 81 81 81 5a 46 42 52 82 81 81 5a 46 4a 46 81 81 81 58 48 4a 46 82 81 81 54 4a 48 48 82 81 81 56 4a 4a 46 82 81 81 58 4a 4a 44 82 81 81 5a 4a 4a 44 82 81 81 5a 4c 4a 44 82 81 81 5e 4a 48 44 82 81 81 60 48 48 44 82 81 81 40 48 46 46 82 81 81 42 44 46 46 82 81 81 40 42 46 46 82 81 81 46 46 44 4e 82 81 81 30 4a 4a 2e 82 81 81 81 60 30 16 81 81 82 0c 65 b4 4a 81 81 81 65 60 7f 48 b0 81 81 4e 4a 40 46 42 81 9b 4e 4e 36 44 50 81 ff 58 46 54 63 40 81 38 3e 56 40 3e 32 81 40 44 3c 3c 36 3a 81 36 3a 3c 3e 44 2e 81 3e 48 42 2e 40 36 81 3a 3a 4a 3c 3a 3a 81 2e 40 3e 40 4e 24 81 2b 58 4e 3e 44 32 81 2c 58 29 25 23 3e 81 27 52 23 30 28 3a 81 20 4c 1f 32 25 52 81 25 58 24 20 23 48 81 1a 58 24 2c 29 25 81 11 5c 28 2c 36 2c 81 0c 48 0a 14 0a 3a 81 0b 32 12 17 19 4a 81 10 3a 1b 06 0c 29 81 0b 42 0b 08 19 38 81 12 4c 1b 28 02 48 81 24 34 14 1b 0f 44 81 23 1a 24 11 16 28 81 20 eb 19 32 1e 20 81 f8 ac 15 19 19 ec 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 c5 81 84 81 81 81 81 6a 81 81 81 81 81 81 d8 81 9e 81 82 81 1f df 32 30 81 81 81 48 4c f2 0a 81 81 f6 0e 26 25 07 81 23 14 1a 2b 2d 36 81 15 2e 30 ff 60 34 94 17 ec 40 0b cf ed ba 36 2c d4 48 ec d7 ed 4a 79 05 7d 7f 40 b0 81 0a df 9a f6 0b 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 81 81 81 83 c2 81 83 81 81 81 81 4e 81 81 81 81 81 cb 48 e2 05 81 82 81 7d 58 46 42 99 84 b0 44 60 52 38 c4 81 48 3c 60 46 34 e1 e6 32 2c 54 66 2f d1 15 f8 38 68 5a 6b ae 30 3a 30 4a 42 40 91 38 48 46 44 48 4a 83 66 6f 73 6b 6a 68 81 8a d1 ba 9e 99 90 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 2b 8d 9e 81 81 81 84 52 50 52 81 81 81 81 09 5c 62 81 81 81 81 81 21 13 81 81 81 0f 81 81 81 81 87 81 36 84 81 20 93 81 81 28 81 81 1a ba bc 15 2f 19 ac 6c d3 16 11 34 69 62 58 99 38 eb 46 58 5c 4e b2 4a 4c 5e 56 52 54 ba 4c 52 52 54 34 50 9f f4 6b 6d 81 4c 7f c0 81 95 95 81 cc be 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 1d 22 81 81 81 81 81 1d 32 81 81 81 81 81 21 30 81 81 81 81 81 0e 2e 81 81 81 81 81 10 2f 81 81 81 81 81 15 36 81 81 81 81 81 15 26 81 81 81 81 81 1c 2e 81 81 81 81 81 2d 4c 81 81 81 81 81 28 58 81 81 81 81 81 23 5a 81 81 81 81 81 27 50 81 81 81 81 81 29 42 81 81 81 81 81 29 3c 81 81 81 81 81 34 38 81 81 81 81 81 38 2b 81 81 81 81 81 3e 13 81 81 81 81 81 44 03 81 81 81 81 81 44 fe 81 81 81 81 81 42 fc 81 81 81 81 81 44 fc 81 81 81 81 81 46 fb 81 81 81 81 81 48 f1 81 81 81 81 81 4c ea 81 81 81 81 81 4c ed 81 81 81 81 81 4c e4 81 81 81 81 81 4a dd 81 81 81 81 81 30 a2 81 81 81 81 81 94 81 81 82 81 81 82 7f 01 b8 81 81 81 c5 df 58 b8 81 81 81 ac 92 b8 a8 81 81 82 ae ba b4 ae 81 81 81 ac b4 be ac 81 81 81 b0 b4 a4 ae 82 81 81 bc a8 bc a8 99 81 81 c2 ac ba aa a2 81 81 b6 b2 ac b8 aa 81 81 c9 b2 a4 c3 b4 81 81 c2 b0 a8 c7 be 81 88 da ae a2 a6 c2 81 90 0b aa 99 94 cc 81 94 22 a8 99 88 d0 81 a0 52 a6 9b 86 de 81 a6 ff b2 a4 90 ee 81 9b d3 b6 a8 99 ec 81 9b f3 c9 b4 c3 dc 81 99 e1 ca be a8 ce 81 a2 c5 d1 d1 ac b4 81 aa b8 ee e1 c7 a2 81 a4 92 ea e7 cc a0 81 c3 f1 d1 ef ea b2 81 c7 d3 08 06 d5 b8 81 c2 81 9b 94 81 bc 81 96 81 90 9e 82 9d 81 81 81 84 aa 82 81 81 81 81 81 21 83 81 81 81 81 c2 67 81 81 81 81 81 73 64 ba 81 81 81 e4 40 6f 54 81 81 81 52 4c 56 3a 81 81 81 4e 3a 36 42 81 81 81 4a 42 40 42 81 81 81 4c 42 3a 46 8d 81 81 52 32 34 40 a0 81 81 54 32 32 58 bc 81 81 5c 40 3c 5c fb 81 81 60 3a 4a 58 2e 81 81 66 38 34 65 56 81 81 71 38 42 6e 6d 81 81 7f 36 3e 6d 52 81 81 71 3a 3a 67 4e 81 81 58 3e 46 72 4e 81 81 4c 3a 3e 7d 4c 81 81 4c 42 3e 6d 52 81 81 40 3c 42 52 5a 81 81 40 3c 44 40 6a 81 81 42 3e 42 40 66 81 81 4c 3e 44 40 63 81 81 54 42 44 40 6a 81 81 50 3e 42 3e 60 81 81 36 4c 4e 67 5c 81 81 40 c7 d7 81 5e 81 81 8e 81 81 81 98 81 81 81 d3 a6 81 81 81 81 83 76 7f 7f 81 81 81 81 66 4a 54 81 81 81 81 6a 61 4c 81 81 81 81 6b 6b 72 81 81 81 81 6b 67 7b 81 81 81 81 6a 6b 7f 81 81 81 9e 68 6a 7f 81 81 81 b8 67 6b 7f 81 81 81 d2 68 69 7f 81 81 81 c7 69 67 7f 81 81 81 99 6b 66 7f 81 81 81 81 6b 65 7f 81 81 81 81 6a 66 7e 81 81 81 81 6a 65 7a 81 81 81 8d 6c 67 7f 81 81 81 9e 6d 69 7f 81 81 81 ac 6e 68 7f 81 81 81 be 6e 68 7f 81 81 81 cc 6f 69 7f 81 81 81 d8 70 6b 7f 81 81 81 e6 71 6d 7f 81 81 81 f1 72 6d 7f 81 81 81 f9 74 6c 7f 81 81 81 0d 75 6e 6d 81 81 81 38 69 69 6c 81 81 81 25 7f 7f 7f 81 81 81 81 0b 21 90 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 2f 81 81 81 81 81 81 27 c4 81 81 81 81 81 f0 6f 81 81 81 81 84 dd dd 81 81 81 81 25 ed e7 81 81 81 82 36 f7 f0 81 81 81 81 17 2c 36 81 81 81 b8 2d 29 32 81 86 81 56 3a 19 36 81 81 81 1d 11 1c fa 81 fd 0c 36 13 1f 2d 81 0c 23 17 36 1e 27 81 16 19 05 1b 28 21 81 42 0e 11 1f 21 30 ea 3e e4 10 2f 30 44 ee 50 2e 29 42 48 3a 12 19 4c 3c 4a 44 44 9d 81 3e 42 81 9d da 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 81 25 85 81 81 81 82 95 6a 81 81 81 81 83 56 4e 97 81 81 81 81 46 44 3a 81 81 81 81 40 3c 48 81 81 81 81 48 40 46 81 81 81 9d 48 44 48 81 81 81 be 44 44 4e 81 81 81 de 44 44 52 81 81 81 f9 46 42 50 81 81 81 07 4c 42 56 81 81 81 15 4c 42 62 81 81 81 22 4c 40 54 81 81 81 29 4e 40 44 81 81 81 2f 4e 3e 21 81 81 81 36 4c 3e 14 81 81 81 3a 4c 3e 22 81 81 81 30 50 3e 5a 81 81 81 38 50 3e 50 81 81 81 1a 54 3a 44 81 81 81 81 5e 3a 4e 81 81 81 81 63 3e 52 81 81 81 88 6a 40 48 81 81 81 c8 70 44 46 81 81 81 09 70 46 48 81 81 81 3a 78 3e 48 81 81 81 5a 72 52 54 81 81 81 81 87 21 a6 81 81 81 81 81 81 81 81 81 81 81 83 e0 83 81 81 81 81 81 2f 81 81 81 81 81 81 81 be 81 81 81 81 c2 81 2e 81 81 81 81 0d 81 21 81 81 81 81 24 81 f5 81 81 82 84 25 83 ed 81 81 81 81 76 81 f8 81 81 81 81 7f 81 12 81 81 c6 4e 48 3a 28 32 be d3 3a 52 32 32 46 95 8d 3e 34 32 40 44 81 88 44 3c 4a 52 44 8b 81 4e 46 21 60 42 8d 81 5a 3c 54 f7 52 99 81 5c 42 64 fd 61 9e 81 5a 4c 54 13 5c b4 9d 5a 50 68 5a 5a cf ca 5a 56 5c 50 56 e5 ec 5a 5c 48 50 5c 00 fe 58 5a 56 54 61 1b 02 5a 5c 5c 5e 61 1c 6e 5a 62 63 65 67 26 38 5e 60 5e 5c 62 1f 25 67 68 67 65 69 4c 6a 7f 79 58 52 3a e7 85 df 81 81 81 81 81 81 81 84 0b 81 81 81 81 81 81 b8 86 81 81 81 81 81 81 81 81 81 81 81 aa 8d 81 81 81 81 81 d2 83 81 81 81 81 81 ee 81 ed 81 81 81 81 a4 81 17 81 81 81 81 81 81 02 81 81 81 83 81 81 c6 81 81 81 85 81 81 81 84 81 81 81 81 81 81 87 81 81 81 86 81 81 81 81 81 81 85 81 81 81 81 81 81 82 81 84 81 81 81 c3 83 84 88 81 81 81 d3 81 81 81 81 81 81 95 81 81 81 97 81 81 e5 36 3a 38 d1 81 81 4a 48 5e 4a 07 81 81 36 4c da 4e d9 81 81 2c 48 93 4e c1 81 81 d6 52 ec 54 81 81 81 e1 5c 7f 64 81 81 81 f5 4e 58 5c 81 81 81 eb 54 5a 5a 81 81 81 81 4c 5c 58 81 81 81 81 63 7b 52 81 81 81 81 0e 81 81 85 81\n",
            "=======\n",
            "===\n",
            "byte0: c7 b2 aa 0d ea 3b 4b b6 c8 05 02 0f 63 79 06 d2 f1 df\n",
            "=======\n",
            "byte1: c8 a5 ec c9 cc 37 17 da ee 02 f6 7f 7a 78 80 e2 bf db\n",
            "=======\n",
            "byte2: cd bb 0e c6 2f 1c 48 db b0 29 e2 41 44 af e3 f9 df ba\n",
            "=======\n",
            "byte3: cf d9 d9 cd 49 46 f1 a6 d3 e3 e2 71 67 0d ed a7 b3 cf\n",
            "=======\n",
            "-57\n",
            "0.33603994332107845\n",
            "req_scale (shift 6):\n",
            "output zero\n",
            "01010110\n"
          ]
        }
      ],
      "source": [
        "def signed_dec2hex_matrix(input):\n",
        "    '''Convert a matrix which data is signed decimal to 8 bits hex with 2's complement'''\n",
        "    temp = []\n",
        "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
        "    for i in input:\n",
        "        test =bin8(i)\n",
        "        test = int(test,base=2)\n",
        "        hex_test = hex(test)[2:].zfill(2)\n",
        "        temp.append(hex_test)\n",
        "\n",
        "    return temp\n",
        "\n",
        "def signed_dec2hex(input):\n",
        "    '''Convert a number which data is signed decimal to 8 bits hex with 2's complement'''\n",
        "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
        "    test =bin8(input)\n",
        "    test = int(test,base=2)\n",
        "    hex_test = hex(test)[2:].zfill(2)\n",
        "\n",
        "    return hex_test\n",
        "\n",
        "\n",
        "def golden_gen(golden_layer_decimal):\n",
        "    '''Convert a layer output which is signed decimal in GPU to 8 bits hex with 2's complement in CPU and\n",
        "     make it 4 element in a line, example of use : golden_gen(q_output_activation[\"Conv.3\"]) '''\n",
        "    golden = []\n",
        "    i=0\n",
        "    golden_in_numpy = golden_layer_decimal.cpu().numpy()\n",
        "    test = golden_in_numpy.flatten()\n",
        "    test =test.astype('int32')\n",
        "    golden.append([])\n",
        "    for j, data in enumerate(test):\n",
        "        if(j%4==0 ):\n",
        "            golden.append([])\n",
        "            i = i+1\n",
        "            golden[i].append(signed_dec2hex(data))\n",
        "        if(j%4!=0):\n",
        "            golden[i].append(signed_dec2hex(data))\n",
        "    golden.pop(0)\n",
        "    for indice,data in enumerate(golden):\n",
        "        print(*data,sep='')\n",
        "\n",
        "def input_or_weight_gen(layer_decimal):\n",
        "    '''Convert a layer output which is signed decimal in GPU to 8 bits hex with 2's complement in CPU and\n",
        "     make each byte display in different place, example of use : input_or_weight_gen(quantized_model.Conv[1].weights)'''\n",
        "    byte0 = []\n",
        "    byte1 = []\n",
        "    byte2 = []\n",
        "    byte3 = []\n",
        "\n",
        "    data_in_numpy = layer_decimal.cpu().numpy()\n",
        "    data_test = data_in_numpy.flatten()\n",
        "    data_test = data_test.astype('int32')\n",
        "    data_test = signed_dec2hex_matrix(data_test)\n",
        "    for indice,data in enumerate(data_test):\n",
        "        if(indice%4 == 0):\n",
        "            byte0.append(data)\n",
        "        elif(indice%4 == 1):\n",
        "            byte1.append(data)\n",
        "        elif(indice%4 == 2):\n",
        "            byte2.append(data)\n",
        "        else:\n",
        "            byte3.append(data)\n",
        "    print(\"byte0:\",*byte0)\n",
        "    print(\"=======\")\n",
        "    print(\"byte1:\",*byte1)\n",
        "    print(\"=======\")\n",
        "    print(\"byte2:\",*byte2)\n",
        "    print(\"=======\")\n",
        "    print(\"byte3:\",*byte3)\n",
        "    print(\"=======\")\n",
        "    return byte0,byte1,byte2,byte3\n",
        "\n",
        "def DecToBin_machine(num,accuracy):\n",
        "    integer = int(num)\n",
        "    flo = num - integer\n",
        "    integercom = '{:1b}'.format(integer)\n",
        "    tem = flo\n",
        "    flo_list = []\n",
        "    for i in range(accuracy):\n",
        "        tem *= 2\n",
        "        flo_list += str(int(tem))\n",
        "        tem -= int(tem)\n",
        "    flocom = flo_list\n",
        "    binary_value =  ''.join(flocom)\n",
        "    return binary_value\n",
        "\n",
        "def customize_dw_input_gen(layer_decimal):\n",
        "    '''Convert a layer output which is signed decimal in GPU to 8 bits hex with 2's complement in CPU and\n",
        "     make each byte display in different place, example of use : input_or_weight_gen(quantized_model.Conv[1].weights)'''\n",
        "    batch,channel,rows, cols = layer_decimal.shape\n",
        "    patches = []\n",
        "    for d in range(channel):\n",
        "        for i in range(0, rows, 3):\n",
        "            for j in range(0, cols, 3):\n",
        "                patch = layer_decimal[0,d,i:i+3, j:j+3]\n",
        "                patches.append(patch)\n",
        "    temp_data_in_np = []\n",
        "    for i , data in enumerate(patches):\n",
        "        data =data.cpu()\n",
        "        temp = np.array(data)\n",
        "        temp_data_in_np.append(temp)\n",
        "    temp_np = np.array(temp_data_in_np)\n",
        "    temp_np = temp_np.flatten()\n",
        "\n",
        "    byte0 = []\n",
        "    byte1 = []\n",
        "    byte2 = []\n",
        "    byte3 = []\n",
        "\n",
        "    # data_in_numpy = layer_decimal.cpu().numpy()\n",
        "    # data_test = data_in_numpy.flatten()\n",
        "    data_test =temp_np.astype('int32')\n",
        "    data_test = signed_dec2hex_matrix(data_test)\n",
        "    for indice,data in enumerate(data_test):\n",
        "        if(indice%3 == 0):\n",
        "            byte0.append(data)\n",
        "            byte3.append('00')\n",
        "        elif(indice%3 == 1):\n",
        "            byte1.append(data)\n",
        "        elif(indice%3 == 2):\n",
        "            byte2.append(data)\n",
        "        # else:\n",
        "        #     byte3.append(data)\n",
        "    print(\"byte0:\",*byte0)\n",
        "    print(\"=======\")\n",
        "    print(\"byte1:\",*byte1)\n",
        "    print(\"=======\")\n",
        "    print(\"byte2:\",*byte2)\n",
        "    print(\"=======\")\n",
        "    print(\"byte3:\",*byte3)\n",
        "    print(\"=======\")\n",
        "    return byte0,byte1,byte2,byte3\n",
        "#golden_gen(q_output_activation[\"Conv.3\"])\n",
        "torch.save(quantized_model,\"dw_customize_Toymodel.pt\")\n",
        "print(signed_dec2hex(quantized_model.Conv[1].input_zero_point))\n",
        "input_or_weight_gen(q_input_activation[\"Conv.1\"])\n",
        "print(\"===\")\n",
        "input_or_weight_gen(quantized_model.Conv[4].weights)\n",
        "\n",
        "print(quantized_model.Conv[4].input_zero_point)\n",
        "\n",
        "DeS = quantized_model.Conv[3].store_scale\n",
        "print(DeS)\n",
        "#print(DeS *8192) # 2**13\n",
        "#print(\"deq_scale (shift 13):\",DecToBin_machine(quantized_model.Conv[1].,8))\n",
        "print(\"req_scale (shift 6):\",)\n",
        "print(\"output zero\",)\n",
        "#binary_value = integercom + '.' + ''.join(flocom)\n",
        "result = DecToBin_machine(DeS,8)\n",
        "print(result)\n",
        "# 0.1100111101011100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "byte0: c7 b2 aa 0d ea 3b 4b b6 c8 05 02 0f 63 79 06 d2 f1 df\n",
            "=======\n",
            "byte1: c8 a5 ec c9 cc 37 17 da ee 02 f6 7f 7a 78 80 e2 bf db\n",
            "=======\n",
            "byte2: cd bb 0e c6 2f 1c 48 db b0 29 e2 41 44 af e3 f9 df ba\n",
            "=======\n",
            "byte3: cf d9 d9 cd 49 46 f1 a6 d3 e3 e2 71 67 0d ed a7 b3 cf\n",
            "=======\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(['c7',\n",
              "  'b2',\n",
              "  'aa',\n",
              "  '0d',\n",
              "  'ea',\n",
              "  '3b',\n",
              "  '4b',\n",
              "  'b6',\n",
              "  'c8',\n",
              "  '05',\n",
              "  '02',\n",
              "  '0f',\n",
              "  '63',\n",
              "  '79',\n",
              "  '06',\n",
              "  'd2',\n",
              "  'f1',\n",
              "  'df'],\n",
              " ['c8',\n",
              "  'a5',\n",
              "  'ec',\n",
              "  'c9',\n",
              "  'cc',\n",
              "  '37',\n",
              "  '17',\n",
              "  'da',\n",
              "  'ee',\n",
              "  '02',\n",
              "  'f6',\n",
              "  '7f',\n",
              "  '7a',\n",
              "  '78',\n",
              "  '80',\n",
              "  'e2',\n",
              "  'bf',\n",
              "  'db'],\n",
              " ['cd',\n",
              "  'bb',\n",
              "  '0e',\n",
              "  'c6',\n",
              "  '2f',\n",
              "  '1c',\n",
              "  '48',\n",
              "  'db',\n",
              "  'b0',\n",
              "  '29',\n",
              "  'e2',\n",
              "  '41',\n",
              "  '44',\n",
              "  'af',\n",
              "  'e3',\n",
              "  'f9',\n",
              "  'df',\n",
              "  'ba'],\n",
              " ['cf',\n",
              "  'd9',\n",
              "  'd9',\n",
              "  'cd',\n",
              "  '49',\n",
              "  '46',\n",
              "  'f1',\n",
              "  'a6',\n",
              "  'd3',\n",
              "  'e3',\n",
              "  'e2',\n",
              "  '71',\n",
              "  '67',\n",
              "  '0d',\n",
              "  'ed',\n",
              "  'a7',\n",
              "  'b3',\n",
              "  'cf'])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# p2d = (1,1,1,1)\n",
        "# test = golden_layer_decimal = torch.nn.functional.pad(q_input_activation[\"Conv.4\"],p2d,\"constant\",0)\n",
        "# customize_dw_input_gen(test)\n",
        "# input_or_weight_gen(quantized_model.Conv[4].weights)\n",
        "golden_gen(q_output_activation[\"Conv.4\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMccqTL6URaZ"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRMXzNeCgDuq",
        "outputId": "0fa65b9f-3a60-41e0-a935-a601a8db0484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 0.000792 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knTzO1mbheBe",
        "outputId": "411fc256-b7ea-4757-9f75-6acaae605324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.002536 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loop(test_loader, quantized_model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Conv.0', 'Conv.1', 'Conv.2', 'Conv.3', 'Conv.4.avg_pool', 'Conv.4.fc.0', 'Conv.4.fc.1', 'Conv.4.fc.2', 'Conv.4.fc.3', 'Conv.5', 'Conv.6', 'backbone.0', 'backbone.1', 'backbone.2', 'backbone.3', 'backbone.4'])\n",
            "dict_keys(['Conv.0', 'Conv.1', 'Conv.2', 'Conv.3', 'Conv.4', 'Conv.5', 'Conv.6', 'Conv.7.avg_pool', 'Conv.7.fc.0', 'Conv.7.fc.1', 'Conv.7', 'Conv.8', 'Conv.9', 'Conv.10', 'backbone.0', 'backbone.1', 'backbone.2'])\n"
          ]
        }
      ],
      "source": [
        "print(output_activation.keys())\n",
        "print(q_output_activation.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25600,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([ 508.,  136.,   93.,  118.,  113.,  118.,  124.,  253.,  225.,\n",
              "         199.,  209.,  214.,  167.,  627.,   99., 1107., 3046.,  159.,\n",
              "          89.,   88.,   70.,   89.,  131.,  442., 2157., 1326., 1398.,\n",
              "        2142., 2136.,  911.,  481., 1425., 1840.,  296.,  389.,  719.,\n",
              "         122.,   85.,   73.,  278.,  576.,   78.,   45.,   31.,   32.,\n",
              "          35.,   28.,   26.,  547.]),\n",
              " array([-3.10070729, -2.94793701, -2.79516673, -2.64239645, -2.48962617,\n",
              "        -2.33685613, -2.18408585, -2.03131557, -1.87854528, -1.725775  ,\n",
              "        -1.57300472, -1.42023456, -1.26746428, -1.114694  , -0.96192378,\n",
              "        -0.8091535 , -0.65638328, -0.503613  , -0.35084277, -0.19807251,\n",
              "        -0.04530226,  0.10746799,  0.26023823,  0.41300848,  0.56577873,\n",
              "         0.71854901,  0.87131923,  1.02408946,  1.17685974,  1.32963002,\n",
              "         1.4824003 ,  1.63517046,  1.78794074,  1.94071102,  2.0934813 ,\n",
              "         2.24625158,  2.39902186,  2.55179191,  2.70456219,  2.85733247,\n",
              "         3.01010275,  3.16287303,  3.31564331,  3.46841359,  3.62118387,\n",
              "         3.77395391,  3.9267242 ,  4.07949448,  4.232265  ,  4.38503504]),\n",
              " <BarContainer object of 49 artists>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#y = torch.flatten(FP32_model.Conv[0].weight)\n",
        "y = torch.flatten(input_activation['Conv.3'])\n",
        "y = y.cpu()\n",
        "y = torch.flatten(y)\n",
        "y = y.detach()\n",
        "y = y.numpy()\n",
        "print(y.shape)\n",
        "\n",
        "plt.hist(y, bins='auto',density=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25600,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([ 317.,  155.,   95.,   77.,   96.,  125.,  126.,  169.,  154.,\n",
              "         198.,  174.,  201.,  167.,  803.,  126.,  889., 2935.,  481.,\n",
              "          79.,   90.,   91.,   83.,  102.,  412.,  690., 2819.,  598.,\n",
              "        2196., 1801., 2039.,  497.,  501., 2085.,  964.,  318.,  402.,\n",
              "         389.,   89.,   87.,   85.,  301.,  591.,   62.,   68.,   54.,\n",
              "          45.,   39.,   27.,   27.,  681.]),\n",
              " array([-3.08687687, -2.93772531, -2.78857398, -2.63942242, -2.49027109,\n",
              "        -2.34111953, -2.1919682 , -2.04281664, -1.89366519, -1.74451375,\n",
              "        -1.59536231, -1.44621086, -1.29705942, -1.14790785, -0.99875647,\n",
              "        -0.84960496, -0.70045352, -0.55130208, -0.4021506 , -0.25299916,\n",
              "        -0.1038477 ,  0.04530377,  0.19445522,  0.34360668,  0.49275815,\n",
              "         0.6419096 ,  0.79106104,  0.94021249,  1.08936393,  1.23851538,\n",
              "         1.38766694,  1.53681839,  1.68596983,  1.83512127,  1.98427272,\n",
              "         2.13342428,  2.28257561,  2.43172717,  2.5808785 ,  2.73003006,\n",
              "         2.87918139,  3.02833295,  3.17748451,  3.32663584,  3.4757874 ,\n",
              "         3.62493873,  3.77409029,  3.92324162,  4.07239294,  4.22154474,\n",
              "         4.37069607]),\n",
              " <BarContainer object of 50 artists>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#x = torch.flatten(quantized_weights12)\n",
        "#x = torch.flatten(quantized_model.Conv[1].weights)\n",
        "#print(q_output_activation['Conv.3'])\n",
        "x = torch.flatten(q_input_activation['Conv.5'])\n",
        "x = x.cpu()\n",
        "x = torch.flatten(x)\n",
        "x = x.detach()\n",
        "x = x.numpy()\n",
        "print(x.shape)\n",
        "\n",
        "plt.hist(x, bins='auto',density=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
