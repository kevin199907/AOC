{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "\n",
    "no_cuda = False\n",
    "use_gpu = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "def bn_folding_model(model):\n",
    "\n",
    "    new_model = copy.deepcopy(model)\n",
    "\n",
    "    module_names = list(new_model._modules)\n",
    "\n",
    "    for k, name in enumerate(module_names):\n",
    "        if len(list(new_model._modules[name]._modules)) > 0:\n",
    "            new_model._modules[name] = bn_folding_model(new_model._modules[name])\n",
    "\n",
    "            \n",
    "        else:\n",
    "            if isinstance(new_model._modules[name], nn.BatchNorm2d):\n",
    "                if isinstance(new_model._modules[module_names[k-1]], nn.Conv2d):\n",
    "\n",
    "                    # Folded BN\n",
    "                    folded_conv = fold_conv_bn_eval(new_model._modules[module_names[k-1]], new_model._modules[name])\n",
    "\n",
    "                    # Replace old weight values\n",
    "                    new_model._modules.pop(name) # Remove the BN layer\n",
    "                    new_model._modules[module_names[k-1]] = folded_conv # Replace the Convolutional Layer by the folded version\n",
    "    return new_model\n",
    "\n",
    "\n",
    "\n",
    "def bn_folding(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n",
    "    if conv_b is None:\n",
    "        conv_b = bn_rm.new_zeros(bn_rm.shape)\n",
    "    bn_var_rsqrt = torch.rsqrt(bn_rv*bn_rv + bn_eps)\n",
    "    \n",
    "    w_fold = conv_w * (bn_w * bn_var_rsqrt).view(-1, 1, 1, 1)\n",
    "    b_fold = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n",
    "    \n",
    "    return torch.nn.Parameter(w_fold), torch.nn.Parameter(b_fold)\n",
    "\n",
    "\n",
    "def fold_conv_bn_eval(conv, bn):\n",
    "    assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n",
    "    fused_conv = copy.deepcopy(conv)\n",
    "\n",
    "    fused_conv.weight, fused_conv.bias = bn_folding(fused_conv.weight, fused_conv.bias,\n",
    "                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n",
    "\n",
    "    return fused_conv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    " \n",
    "\n",
    "def pw_conv(infp,outfp):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=infp, out_channels=outfp, kernel_size=1, stride=1,padding= 0, bias=False)\n",
    "    )\n",
    "\n",
    "def dw_conv(infp,outfp,kernel_size,stride,padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(infp,outfp,kernel_size,stride=stride,padding=padding,groups=infp)\n",
    "    )\n",
    "\n",
    "class MobileNetV3_block(nn.Module):\n",
    "    def __init__(self, infp, outfp, middle_feature ,kernel_size,stride,padding,bias=False):\n",
    "        super(MobileNetV3_block, self).__init__()\n",
    "        ############ My Design #########\n",
    "        self.pw1 = nn.Conv2d(in_channels=infp, out_channels=middle_feature, kernel_size=1, stride=1,padding= 0, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(middle_feature)\n",
    "        self.hs1 = h_swish()\n",
    "\n",
    "        self.dw1 = nn.Conv2d(middle_feature,middle_feature,kernel_size,stride=stride,padding=padding,groups=middle_feature, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(middle_feature)\n",
    "        self.hs2 = h_swish()\n",
    "        self.se1 = SELayer(middle_feature)\n",
    "        self.hs3 = h_swish()\n",
    "        self.pw2 = nn.Conv2d(in_channels=middle_feature, out_channels=outfp, kernel_size=1, stride=1,padding= 0, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d( outfp)\n",
    "        self.hs4 = h_swish()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.pw1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.hs1(out)\n",
    "        out = self.dw1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.hs2(out)\n",
    "        out = self.se1(out)\n",
    "        out = self.hs3(out)\n",
    "        out = self.pw2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.hs4(out)\n",
    "        return out\n",
    "class Our_MobileNetV3_have_bias(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Our_MobileNetV3_have_bias, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1,padding=1)\n",
    "        self.block1 = MobileNetV3_block(infp = 8, outfp=16 , middle_feature=8 ,kernel_size=3,stride=2,padding=1,bias=True)\n",
    "        self.block2 = MobileNetV3_block(infp = 16, outfp=32 , middle_feature=48 ,kernel_size=3,stride=2,padding=1,bias=True)\n",
    "        self.block3 = MobileNetV3_block(infp = 32, outfp=32 , middle_feature=64 ,kernel_size=3,stride=2,padding=1,bias=True)\n",
    "        self.block4 = MobileNetV3_block(infp = 32, outfp=48 , middle_feature=64 ,kernel_size=3,stride=2,padding=1,bias=True)\n",
    "        self.block5 = MobileNetV3_block(infp = 48, outfp=64 , middle_feature=96 ,kernel_size=3,stride=2,padding=1,bias=True)\n",
    "        self.block6 = MobileNetV3_block(infp = 64, outfp=64 , middle_feature=96 ,kernel_size=3,stride=2,padding=1,bias=True)\n",
    "        self.block7 = MobileNetV3_block(infp = 64, outfp=32 , middle_feature=48 ,kernel_size=3,stride=2,padding=1,bias=True)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear1 = nn.Linear(32,20)\n",
    "        self.swish = h_swish()\n",
    "        self.linear2 = nn.Linear(20,10)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = self.block5(out)\n",
    "        out = self.block6(out)\n",
    "        out = self.block7(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.linear2(out)\n",
    "        return out  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mobilenet_model.mobilenet_model import Our_MobileNetV3_have_bias\n",
    "from collections import OrderedDict\n",
    "from mobilenet_model.mobilenet_model import Our_MobileNetV3\n",
    "from mobilenet_model.mobilenet_model import BN_fold_Our_MobileNetV3\n",
    "\n",
    "#test = Our_MobileNetV3_have_bias()\n",
    "#trained_net = Our_MobileNetV3()\n",
    "trained_net =  Our_MobileNetV3()\n",
    "have_bias_model = BN_fold_Our_MobileNetV3()\n",
    "\n",
    "input_shape = torch.randn(1,1,28,28).cuda()\n",
    "trained_net_weight = torch.load(\"N26122246_minist_best.ckpt\")\n",
    "trained_net.load_state_dict(trained_net_weight,strict=False)\n",
    "\n",
    "origin_net = copy.deepcopy(trained_net)\n",
    "origin_net = origin_net.cuda()\n",
    "origin_net.eval()\n",
    "trained_net = trained_net.cuda()\n",
    "trained_net.eval()\n",
    "\n",
    "BN_fold_trained_net=  bn_folding_model(trained_net)\n",
    "\n",
    "# final_BN_fold_trained_net = BN_fold_Our_MobileNetV3()\n",
    "\n",
    "#########################################\n",
    "torch.save(BN_fold_trained_net.state_dict(),\"saved_test.ckpt\")\n",
    "have_bias_model_weight =torch.load(\"saved_test.ckpt\")\n",
    "have_bias_model.load_state_dict(have_bias_model_weight,strict=True)\n",
    "have_bias_model = have_bias_model.cuda()\n",
    "have_bias_model.eval()\n",
    "#########################################\n",
    "# print(trained_net)\n",
    "# print(\"-\"*10)\n",
    "# for name, param in trained_net.named_parameters():\n",
    "#     print(\"name:\", name)\n",
    "#     print(\"param:\", param)\n",
    "# for name, param in BN_fold_trained_net.named_parameters():\n",
    "#     print(\"name:\", name)\n",
    "#     print(\"param:\", param)\n",
    "# print(\"-\"*5)\n",
    "# for name, param in have_bias_model.named_parameters():\n",
    "#     print(\"name:\", name)\n",
    "#     print(\"param:\", param)\n",
    "\n",
    "\n",
    "print(\"pw_weight:\",trained_net.block1.pw1.weight)\n",
    "print(\"-\"*5)\n",
    "print(\"pw_bias:\",trained_net.block1.pw1.bias)\n",
    "print(\"-\"*5)\n",
    "print(\"bn_weight:\",trained_net.block1.bn1.weight)\n",
    "print(\"-\"*5)\n",
    "print(\"bn_bias:\",trained_net.block1.bn1.bias)\n",
    "print(\"-\"*5)\n",
    "print(\"bn_running_mean:\",trained_net.block1.bn1.running_mean)\n",
    "print(\"-\"*5)\n",
    "print(\"bn_running_var:\",trained_net.block1.bn1.running_var)\n",
    "print(\"-\"*5)\n",
    "print(\"bn_running_eps:\",trained_net.block1.bn1.eps)\n",
    "print(\"-\"*5)\n",
    "print(\"bn_fold_pw1_weight:\",BN_fold_trained_net.block1.pw1.weight)\n",
    "print(\"-\"*5)\n",
    "print(\"bn_fold_pw1_bias:\",have_bias_model.block1.pw1.weight)\n",
    "print(\"-\"*10)\n",
    "#print(final_BN_fold_trained_net)\n",
    "\n",
    "\n",
    "FP32_model = Our_MobileNetV3()\n",
    "#print(FP32_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO : You can modify the configuration for model training ###\n",
    "\n",
    "# For the classification task, we use cross-entropy as the measurement of performance.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
    "optimizer = optim.Adam(have_bias_model.parameters(), lr=0.001)\n",
    "\n",
    "# The number of batch size.\n",
    "batch_size = 512\n",
    "\n",
    "# If no improvement in 'patience' epochs, early stop.\n",
    "patience  = 10\n",
    "\n",
    "# The number of training epochs\n",
    "n_epoch = 100\n",
    "\n",
    "_exp_name = \"N26122246_minist\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select training_set and testing_set (dataset : FashionMNIST )\n",
    "train_data = datasets.FashionMNIST(\"data\",train= True, download=True,   transform = transforms.ToTensor())\n",
    "\n",
    "test_data = datasets.FashionMNIST(\"data\",  train= False, download=True, transform = transforms.ToTensor())\n",
    "\n",
    "# # Number of subprocesses to use for data loading\n",
    "# num_workers = 0\n",
    "\n",
    "# Percentage of training set to use as validation\n",
    "n_valid = 0.2\n",
    "\n",
    "# Get indices for training_set and validation_set\n",
    "n_train = len(train_data)\n",
    "indices = list(range(n_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(n_valid * n_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Define samplers for obtaining training and validation\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,  sampler = train_sampler , num_workers=2)\n",
    "\n",
    "# Validation data\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, sampler = valid_sampler, num_workers = 2)\n",
    "\n",
    "# Test data\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,  num_workers=2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        if use_gpu:\n",
    "            images, labels = images.cuda(),labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        #outputs = trained_net(images)\n",
    "        outputs = have_bias_model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
